========== FILE: requirements.txt ==========

numpy<2.0,>=1.20
pandas>=1.3.0
scikit-learn>=1.4.0
matplotlib>=3.0.0
seaborn>=0.13.2
scipy>=1.13.1
joblib>=1.4.2
category_encoders
# missingpy>=0.2.0 # Removed
imbalanced-learn>=0.12.0
# –î–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
gensim>=4.0.0  # –î–ª—è Word2Vec and FastText 
tensorflow>=2.12.0 # –î–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π Keras 
# XGBoost and LightGBM can be added if/when those models are integrated
# xgboost>=1.5.0 
# lightgbm>=3.3.0 


========== FILE: src\cli.py ==========

#!/usr/bin/env python3

import argparse
import sys
import os
import json
import numpy as np
from pathlib import Path
from datetime import datetime
import pandas as pd

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from ga_optimizer import GAConfig, run_genetic_algorithm
from pipeline_processor import decode_chromosome_full, process_data, train_model
from deployment.production_pipeline import ProductionPipeline
from deployment.model_serializer import UniversalModelSerializer
from modeling.model_trainer import ModelTrainer


class MLPipelineCLI:
    """–ö–æ–º–∞–Ω–¥–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è ML –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    
    def __init__(self):
        project_root = Path.cwd()
        self.models_dir = project_root / "models"
        self.results_dir = project_root / "results" 
        self.models_dir.mkdir(exist_ok=True)
        self.results_dir.mkdir(exist_ok=True)
        
    def run_chromosome(self, args):
        print(f"üß¨ –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ö—Ä–æ–º–æ—Å–æ–º—ã: {args.chromosome}")
        print(f"üìä –î–∞—Ç–∞—Å–µ—Ç: {args.dataset}")
        print(f"üéØ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {args.target}")
        
        try:
            chromosome = [int(x.strip()) for x in args.chromosome.split(',')]
            print(f"üîç –•—Ä–æ–º–æ—Å–æ–º–∞: {chromosome}")
            
            decoded_info = decode_chromosome_full(chromosome, verbose=True)
            params = decoded_info['pipeline_params']
            
            print(f"\n‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞–π–ø–ª–∞–π–Ω–∞:")
            for key, value in params.items():
                print(f"  {key}: {value}")
            
            print(f"\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
            train_data, test_data, research_path, preprocessor_states = process_data(
                args.dataset, None, args.target,
                imputation_method=params['imputation_method'],
                imputation_params=params['imputation_params'],
                outlier_method=params['outlier_method'],
                outlier_params=params['outlier_params'],
                encoding_method=params['encoding_method'],
                encoding_params=params['encoding_params'],
                resampling_method=params['resampling_method'],
                resampling_params=params['resampling_params'],
                scaling_method=params['scaling_method'],
                scaling_params=params['scaling_params'],
                save_processed_data=False,
                save_model_artifacts=False
            )
            
            print(f"üìä –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {train_data.shape}")
            print(f"üìä –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {test_data.shape}")
            
            print(f"\nüöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {params['model_type']}")
            trainer = ModelTrainer(
                model_type=params['model_type'],
                model_hyperparameters=params['model_params']
            )
            
            metrics, feature_importance, trainer_dropped_cols = trainer.train(
                train_data, test_data, args.target,
                output_path=None,
                plot_learning_curves=False,
                save_run_results=False
            )
            
            if not metrics:
                print("‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
                return False
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
            process_dropped_cols = preprocessor_states.get('dropped_columns', [])
            all_dropped_cols = list(set(process_dropped_cols + trainer_dropped_cols))
            preprocessor_states['dropped_columns'] = all_dropped_cols
            
            if all_dropped_cols:
                print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω—ã ID –∫–æ–ª–æ–Ω–∫–∏: {all_dropped_cols}")
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")  
            model_name = f"{Path(args.dataset).stem}_{params['model_type']}_{timestamp}"
            model_save_path = self.models_dir / model_name
            
            print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞...")
            
            pipeline_metadata = {
                'model_name': model_name,
                'dataset': args.dataset,
                'target_column': args.target,
                'features': list(train_data.columns[train_data.columns != args.target]),
                'chromosome': chromosome,
                'pipeline_config': params,
                'metrics': metrics,
                'model_type': params['model_type'],
                'created_at': timestamp,
                'source': 'cli_chromosome'
            }
            
            production_pipeline = ProductionPipeline(
                preprocessor_states=preprocessor_states,
                model=trainer.model,
                metadata=pipeline_metadata
            )
            
            production_pipeline.save(str(model_save_path))
            
            metadata_path = self.models_dir / f"{model_name}_metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(pipeline_metadata, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏:")
            print(f"   üìà AUPRC: {metrics.get('auprc', 0):.4f}")
            print(f"   üéØ ROC-AUC: {metrics.get('roc_auc', 0):.4f}")
            print(f"   üé≤ F1-Score: {metrics.get('f1_score', 0):.4f}")
            print(f"   ‚úÖ Accuracy: {metrics.get('accuracy', 0):.4f}")
            print(f"   üîç Precision: {metrics.get('precision', 0):.4f}")
            print(f"   üìû Recall: {metrics.get('recall', 0):.4f}")
            
            auprc = metrics.get('auprc', metrics.get('accuracy', 0))
            print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
            print(f"üìÅ –ü–∞–ø–∫–∞ –º–æ–¥–µ–ª–∏: {model_save_path}")
            print(f"üìÑ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {metadata_path}")
            print(f"üìà –û—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ (AUPRC): {auprc:.4f}")
            print(f"üß¨ –•—Ä–æ–º–æ—Å–æ–º–∞: {chromosome}")
            
            return True
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_ga(self, args):
        print(f"üß¨ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞")
        print(f"üìä –î–∞—Ç–∞—Å–µ—Ç: {args.train}")
        print(f"üéØ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {args.target}")
        print(f"üë• –ü–æ–ø—É–ª—è—Ü–∏—è: {args.population}")
        print(f"üîÑ –ü–æ–∫–æ–ª–µ–Ω–∏—è: {args.generations}")
        
        try:
            ga_config = GAConfig(
                train_path=args.train,
                test_path=None,
                target_column=args.target,
                population_size=args.population,
                num_generations=args.generations,
                elitism_percent=args.elitism,
                mutation_rate=args.mutation,
                tournament_size=args.tournament,
                generate_learning_curves=args.learning_curves
            )
            
            print(f"\n‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {ga_config}")
            
            print("\nüöÄ –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
            results = run_genetic_algorithm(ga_config)
            
            if not results or results['best_chromosome'] is None:
                print("‚ùå –ì–ê –Ω–µ –Ω–∞—à–µ–ª –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ")
                return False
            
            best_chromosome = results['best_chromosome']
            best_fitness = results['best_fitness']
            
            print(f"\nüèÜ –ù–∞–π–¥–µ–Ω–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ!")
            print(f"üìà –õ—É—á—à–∏–π —Ñ–∏—Ç–Ω–µ—Å: {best_fitness:.4f}")
            print(f"üß¨ –õ—É—á—à–∞—è —Ö—Ä–æ–º–æ—Å–æ–º–∞: {best_chromosome}")
            
            if args.auto_save:
                print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏...")
                
                decoded_info = decode_chromosome_full(best_chromosome, verbose=False)
                params = decoded_info['pipeline_params']
                
                train_data, test_data, research_path, preprocessor_states = process_data(
                    args.train, None, args.target,
                    imputation_method=params['imputation_method'],
                    imputation_params=params['imputation_params'],
                    outlier_method=params['outlier_method'],
                    outlier_params=params['outlier_params'],
                    encoding_method=params['encoding_method'],
                    encoding_params=params['encoding_params'],
                    resampling_method=params['resampling_method'],
                    resampling_params=params['resampling_params'],
                    scaling_method=params['scaling_method'],
                    scaling_params=params['scaling_params'],
                    save_processed_data=False,
                    save_model_artifacts=False
                )
                
                print(f"üöÄ –û–±—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏: {params['model_type']}")
                trainer = ModelTrainer(
                    model_type=params['model_type'],
                    model_hyperparameters=params['model_params']
                )
                
                metrics, feature_importance, trainer_dropped_cols = trainer.train(
                    train_data, test_data, args.target,
                    output_path=None,
                    plot_learning_curves=False,
                    save_run_results=False
                )
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
                process_dropped_cols = preprocessor_states.get('dropped_columns', [])
                all_dropped_cols = list(set(process_dropped_cols + trainer_dropped_cols))
                preprocessor_states['dropped_columns'] = all_dropped_cols
                
                if all_dropped_cols:
                    print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω—ã ID –∫–æ–ª–æ–Ω–∫–∏: {all_dropped_cols}")
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                model_name = f"{Path(args.train).stem}_GA_best_{timestamp}"
                model_save_path = self.models_dir / model_name
                
                print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏...")
                
                ga_metadata = {
                    'model_name': model_name,
                    'dataset': args.train,
                    'target_column': args.target,
                    'features': list(train_data.columns[train_data.columns != args.target]),
                    'chromosome': best_chromosome,
                    'pipeline_config': params,
                    'metrics': metrics,
                    'model_type': params['model_type'],
                    'ga_results': {
                        'best_fitness': best_fitness,
                        'fitness_history': results['fitness_history']
                    },
                    'created_at': timestamp,
                    'source': 'genetic_algorithm'
                }
                
                production_pipeline = ProductionPipeline(
                    preprocessor_states=preprocessor_states,
                    model=trainer.model,
                    metadata=ga_metadata
                )
                
                production_pipeline.save(str(model_save_path))
                
                metadata_path = self.models_dir / f"{model_name}_metadata.json"
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(ga_metadata, f, indent=2, ensure_ascii=False, default=str)
                
                print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏:")
                print(f"   üìà AUPRC: {metrics.get('auprc', 0):.4f}")
                print(f"   üéØ ROC-AUC: {metrics.get('roc_auc', 0):.4f}")
                print(f"   üé≤ F1-Score: {metrics.get('f1_score', 0):.4f}")
                print(f"   ‚úÖ Accuracy: {metrics.get('accuracy', 0):.4f}")
                print(f"   üîç Precision: {metrics.get('precision', 0):.4f}")
                print(f"   üìû Recall: {metrics.get('recall', 0):.4f}")
                
                print(f"\n‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
                print(f"üìÅ –ü–∞–ø–∫–∞ –º–æ–¥–µ–ª–∏: {model_save_path}")
                print(f"üìÑ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {metadata_path}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def predict(self, args):
        print(f"üîÆ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {args.model}")
        print(f"üìä –î–∞–Ω–Ω—ã–µ: {args.data}")
        
        try:
            metadata_path = None
            if args.model.endswith('_metadata.json'):
                metadata_path = Path(args.model)
            else:
                metadata_path = self.models_dir / f"{args.model}_metadata.json"
                if not metadata_path.exists():
                    metadata_path = self.models_dir / f"{args.model.replace('.pkl', '')}_metadata.json"
            
            if not metadata_path.exists():
                print(f"‚ùå –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {args.model}")
                print(f"üí° –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–æ–º–∞–Ω–¥–æ–π: list-models")
                return False
            
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {metadata_path}")
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            print(f"üìã –ú–æ–¥–µ–ª—å: {metadata.get('model_name', 'unknown')}")
            print(f"üéØ –¶–µ–ª—å: {metadata.get('target_column', 'unknown')}")
            print(f"ü§ñ –¢–∏–ø: {metadata.get('pipeline_config', {}).get('model_type', 'unknown')}")
            
            print(f"üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {args.data}")
            data = pd.read_csv(args.data)
            
            print(f"üìã –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {data.shape}")
            print(f"üìä –ö–æ–ª–æ–Ω–∫–∏: {list(data.columns)}")
            
            print(f"\nüîç –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö...")
            target_column = metadata.get('target_column')
            expected_features = metadata.get('features', [])
            
            print(f"üìä –û–∂–∏–¥–∞–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(expected_features)}): {expected_features[:5]}{'...' if len(expected_features) > 5 else ''}")
            print(f"üìä –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ ({len(data.columns)}): {list(data.columns)[:5]}{'...' if len(data.columns) > 5 else ''}")
            
            missing_features = set(expected_features) - set(data.columns)
            if missing_features:
                print(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {list(missing_features)[:5]}{'...' if len(missing_features) > 5 else ''}")
            
            extra_features = set(data.columns) - set(expected_features) - {target_column}
            if extra_features:
                print(f"‚ÑπÔ∏è –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(extra_features)[:5]}{'...' if len(extra_features) > 5 else ''}")
            
            print(f"\nüîÆ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
            
            model_name = metadata.get('model_name')
            model_folder = self.models_dir / model_name
            
            if not model_folder.exists():
                print(f"‚ùå –ü–∞–ø–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_folder}")
                return False
            
            pipeline_metadata_path = model_folder / 'pipeline_metadata.json'
            
            if pipeline_metadata_path.exists():
                print(f"üîß –û–±–Ω–∞—Ä—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω, –∑–∞–≥—Ä—É–∂–∞–µ–º ProductionPipeline...")
                try:
                    production_pipeline = ProductionPipeline.load(str(model_folder))
                    print(f"‚úÖ –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
                    
                    print(f"üîÑ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π...")
                    results = production_pipeline.predict(data)
                    
                    print(f"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
                    print(f"üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
                    print(f"üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(results)}")
                    
                    n_show = min(10, len(results))
                    for i in range(n_show):
                        pred = results.iloc[i]['prediction']
                        prob_cols = [col for col in results.columns if col.startswith('probability_class_')]
                        if prob_cols:
                            prob_str = ", ".join([f"{results.iloc[i][col]:.3f}" for col in prob_cols])
                            print(f"  [{i+1:2d}] –ö–ª–∞—Å—Å: {pred} (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: [{prob_str}])")
                        else:
                            print(f"  [{i+1:2d}] –ö–ª–∞—Å—Å: {pred}")
                    
                    if len(results) > n_show:
                        print(f"  ... –∏ –µ—â–µ {len(results) - n_show} –∑–∞–ø–∏—Å–µ–π")
                    
                    if args.output:
                        output_file = args.output
                    else:
                        project_root = Path.cwd()
                        results_dir = project_root / "results"
                        results_dir.mkdir(exist_ok=True)
                        
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        data_path = Path(args.data)
                        data_filename = data_path.stem
                        
                        output_file = results_dir / f"predictions_{data_filename}_{model_name}_{timestamp}.csv"
                    
                    results.to_csv(output_file, index=False)
                    
                    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")
                    print(f"üìä –ö–æ–ª–æ–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {list(results.columns)}")
                    
                    return True
                    
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞: {e}")
                    print(f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª–∏...")
            
            print(f"üîß –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å (–±–µ–∑ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏)...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏
            model_info = metadata.get('model_info')
            if not model_info:
                print("‚ùå –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
                return False
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            try:
                model, loaded_model_info = UniversalModelSerializer.load_model(str(model_folder))
                print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                print(f"üîß –¢–∏–ø: {loaded_model_info.get('model_type', 'unknown')}")
                print(f"üì¶ –§–æ—Ä–º–∞—Ç: {loaded_model_info.get('serialization_format', 'unknown')}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                return False
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            target_column = metadata.get('target_column')
            expected_features = metadata.get('features', [])
            
            # –£–¥–∞–ª—è–µ–º —Ü–µ–ª–µ–≤—É—é –∫–æ–ª–æ–Ω–∫—É –µ—Å–ª–∏ –æ–Ω–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç
            prediction_data = data.copy()
            if target_column in prediction_data.columns:
                prediction_data = prediction_data.drop(columns=[target_column])
                print(f"‚ÑπÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Ü–µ–ª–µ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ '{target_column}' –∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            missing_features = set(expected_features) - set(prediction_data.columns)
            if missing_features:
                print(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {list(missing_features)}")
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω—É–ª–µ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                for feature in missing_features:
                    prediction_data[feature] = 0
                    print(f"  + –î–æ–±–∞–≤–ª–µ–Ω '{feature}' = 0")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –æ–∂–∏–¥–∞–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
            prediction_data = prediction_data[expected_features]
            
            print(f"üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {prediction_data.shape}")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            try:
                predictions = model.predict(prediction_data)
                probabilities = model.predict_proba(prediction_data) if hasattr(model, 'predict_proba') else None
                
                # –î–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                model_type = loaded_model_info.get('model_type', 'unknown')
                if model_type == 'neural_network':
                    # –î–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π predictions –º–æ–∂–µ—Ç –±—ã—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
                    if predictions.ndim > 1 and predictions.shape[1] > 1:
                        probabilities = predictions
                        predictions = np.argmax(predictions, axis=1)
                    elif predictions.ndim == 1 or predictions.shape[1] == 1:
                        # –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
                        probabilities = np.column_stack([1 - predictions.ravel(), predictions.ravel()])
                        predictions = (predictions > 0.5).astype(int).ravel()
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
                if probabilities is not None:
                    n_classes = probabilities.shape[1]
                else:
                    n_classes = len(np.unique(predictions))
                
                print(f"üéØ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {n_classes}")
                print(f"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {e}")
                return False
            
            print(f"üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
            print(f"üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(predictions)}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            n_show = min(10, len(predictions))
            for i in range(n_show):
                prob_str = ", ".join([f"{probabilities[i][j]:.3f}" for j in range(n_classes)])
                print(f"  [{i+1:2d}] –ö–ª–∞—Å—Å: {predictions[i]} (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: [{prob_str}])")
            
            if len(predictions) > n_show:
                print(f"  ... –∏ –µ—â–µ {len(predictions) - n_show} –∑–∞–ø–∏—Å–µ–π")
            
            # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (–≤—Å–µ–≥–¥–∞)
            output_data = data.copy()
            output_data['prediction'] = predictions
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤
            for class_idx in range(n_classes):
                output_data[f'probability_class_{class_idx}'] = probabilities[:, class_idx]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            if args.output:
                output_file = args.output
            else:
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –≤ –ø–∞–ø–∫–µ results –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞  
                project_root = Path.cwd()  # —Ç–µ–∫—É—â–∞—è —Ä–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
                results_dir = project_root / "results"
                
                # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É results –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
                results_dir.mkdir(exist_ok=True)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                model_name = metadata.get('model_name', 'unknown')
                data_path = Path(args.data)
                data_filename = data_path.stem  # –ò–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
                
                output_file = results_dir / f"predictions_{data_filename}_{model_name}_{timestamp}.csv"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            output_data.to_csv(output_file, index=False)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å
            full_path = os.path.abspath(str(output_file))
            print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
            print(f"üìÑ –ü–æ–ª–Ω—ã–π –ø—É—Ç—å: {full_path}")
            print(f"üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {len(data)} –∑–∞–ø–∏—Å–µ–π + prediction + {n_classes} probability columns")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def list_models(self, args):
        """–ü—Ä–æ—Å–º–æ—Ç—Ä –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
        
        try:
            # –ò—â–µ–º —Ñ–∞–π–ª—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤–º–µ—Å—Ç–æ .pkl —Ñ–∞–π–ª–æ–≤
            metadata_files = list(self.models_dir.glob("*_metadata.json"))
            
            if not metadata_files:
                print("‚ùå –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return False
            
            for metadata_path in sorted(metadata_files):
                model_name = metadata_path.stem.replace('_metadata', '')
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                dataset = Path(metadata.get('dataset', 'unknown')).stem
                target = metadata.get('target_column', 'unknown')
                metrics = metadata.get('metrics', {})
                auprc = metrics.get('auprc', 'N/A')
                roc_auc = metrics.get('roc_auc', 'N/A')
                f1_score = metrics.get('f1_score', 'N/A')
                accuracy = metrics.get('accuracy', 'N/A')
                created = metadata.get('created_at', 'unknown')
                source = metadata.get('source', 'manual')
                
                print(f"\nü§ñ {model_name}")
                print(f"   üìä –î–∞—Ç–∞—Å–µ—Ç: {dataset}")
                print(f"   üéØ –¶–µ–ª—å: {target}")
                print(f"   üìà AUPRC: {auprc if auprc != 'N/A' else 'N/A'}")
                print(f"   üéØ ROC-AUC: {roc_auc if roc_auc != 'N/A' else 'N/A'}")
                print(f"   üé≤ F1-Score: {f1_score if f1_score != 'N/A' else 'N/A'}")
                print(f"   ‚úÖ Accuracy: {accuracy if accuracy != 'N/A' else 'N/A'}")
                print(f"   üìÖ –°–æ–∑–¥–∞–Ω: {created}")
                print(f"   üîß –ò—Å—Ç–æ—á–Ω–∏–∫: {source}")
                
                if args.verbose and 'chromosome' in metadata:
                    print(f"   üß¨ –•—Ä–æ–º–æ—Å–æ–º–∞: {metadata['chromosome']}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏ (–ø–∞–ø–∫–∞ –∏–ª–∏ pkl —Ñ–∞–π–ª)
                model_folder = self.models_dir / model_name
                pkl_path = self.models_dir / f"{model_name}.pkl"
                
                if model_folder.exists() and model_folder.is_dir():
                    # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç - –ø–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—å—é
                    try:
                        # –ò—â–µ–º —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –≤ –ø–∞–ø–∫–µ
                        model_files = list(model_folder.glob("*.joblib")) + list(model_folder.glob("*.pkl")) + list(model_folder.glob("keras_model/"))
                        if model_files:
                            total_size = sum(f.stat().st_size for f in model_files if f.is_file())
                            total_size += sum(sum(subf.stat().st_size for subf in f.rglob("*") if subf.is_file()) for f in model_files if f.is_dir())
                            print(f"   üíæ –ú–æ–¥–µ–ª—å: {total_size / 1024:.1f} KB (–ø–æ–ª–Ω–∞—è)")
                        else:
                            print(f"   üíæ –ú–æ–¥–µ–ª—å: –ø–∞–ø–∫–∞ –±–µ–∑ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏")
                    except:
                        print(f"   üíæ –ú–æ–¥–µ–ª—å: –ø–∞–ø–∫–∞ (–Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä)")
                elif pkl_path.exists():
                    # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç - pkl —Ñ–∞–π–ª
                    size = pkl_path.stat().st_size / 1024
                    print(f"   üíæ –ú–æ–¥–µ–ª—å: {size:.1f} KB (legacy)")
                else:
                    print(f"   üíæ –ú–æ–¥–µ–ª—å: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ")
            
            print(f"\nüìä –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {len(metadata_files)}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return False


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI"""
    parser = argparse.ArgumentParser(
        description="CLI –¥–ª—è production deployment —Å–∏—Å—Ç–µ–º—ã ML –ø–∞–π–ø–ª–∞–π–Ω–æ–≤",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  
  # –ó–∞–ø—É—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ö—Ä–æ–º–æ—Å–æ–º—ã
  python cli.py run-chromosome --chromosome "1,2,0,1,1,2,0,1,0,0,1,1,1,0,0,1,2,3,1,0" \\
                              --dataset "../datasets/diabetes.csv" \\
                              --target "Outcome"
  
  # –ó–∞–ø—É—Å–∫ –ì–ê —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
  python cli.py run-ga --dataset "../datasets/diabetes.csv" \\
                       --target "Outcome" \\
                       --population 10 \\
                       --generations 5 \\
                       --save-best
  
  # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
  python cli.py predict --model "diabetes_logistic_regression_20250608_120000" \\
                        --data "new_data.csv" \\
                        --output "predictions.csv"
  
  # –ü—Ä–æ—Å–º–æ—Ç—Ä –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
  python cli.py list-models --verbose
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã')
    
    # –ö–æ–º–∞–Ω–¥–∞ run-chromosome
    cmd_chromosome = subparsers.add_parser(
        'run-chromosome', 
        help='–ó–∞–ø—É—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ö—Ä–æ–º–æ—Å–æ–º—ã —Å —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π'
    )
    cmd_chromosome.add_argument('--chromosome', required=True, 
                              help='–•—Ä–æ–º–æ—Å–æ–º–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ "1,2,0,1,..." (20 –≥–µ–Ω–æ–≤)')
    cmd_chromosome.add_argument('--dataset', required=True,
                              help='–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É')
    cmd_chromosome.add_argument('--target', required=True,
                              help='–ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π')
    cmd_chromosome.add_argument('--learning-curves', action='store_true',
                              help='–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è')
    
    # –ö–æ–º–∞–Ω–¥–∞ run-ga
    cmd_ga = subparsers.add_parser(
        'run-ga',
        help='–ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞'
    )
    cmd_ga.add_argument('--train', '--dataset', required=True,
                       help='–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
    cmd_ga.add_argument('--target', required=True,
                       help='–ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π')
    cmd_ga.add_argument('--population', type=int, default=10,
                       help='–†–∞–∑–º–µ—Ä –ø–æ–ø—É–ª—è—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)')
    cmd_ga.add_argument('--generations', type=int, default=8,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–∫–æ–ª–µ–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 8)')
    cmd_ga.add_argument('--elitism', type=float, default=0.25,
                       help='–ü—Ä–æ—Ü–µ–Ω—Ç —ç–ª–∏—Ç–∏–∑–º–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.25)')
    cmd_ga.add_argument('--mutation', type=float, default=0.1,
                       help='–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º—É—Ç–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.1)')
    cmd_ga.add_argument('--tournament', type=int, default=3,
                       help='–†–∞–∑–º–µ—Ä —Ç—É—Ä–Ω–∏—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 3)')
    cmd_ga.add_argument('--learning-curves', action='store_true',
                       help='–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è')
    cmd_ga.add_argument('--auto-save', '--save-best', action='store_true',
                       help='–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å')
    
    # –ö–æ–º–∞–Ω–¥–∞ predict
    cmd_predict = subparsers.add_parser(
        'predict',
        help='–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º'
    )
    cmd_predict.add_argument('--model', required=True,
                           help='–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –∏–º—è –º–æ–¥–µ–ª–∏')
    cmd_predict.add_argument('--data', required=True,
                           help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è')
    cmd_predict.add_argument('--output',
                           help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    
    # –ö–æ–º–∞–Ω–¥–∞ list-models
    cmd_list = subparsers.add_parser(
        'list-models',
        help='–ü—Ä–æ—Å–º–æ—Ç—Ä –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π'
    )
    cmd_list.add_argument('--verbose', action='store_true',
                         help='–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
    
    cli = MLPipelineCLI()
    
    # –ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∫–æ–º–∞–Ω–¥
    if args.command == 'run-chromosome':
        success = cli.run_chromosome(args)
    elif args.command == 'run-ga':
        success = cli.run_ga(args)
    elif args.command == 'predict':
        success = cli.predict(args)
    elif args.command == 'list-models':
        success = cli.list_models(args)
    else:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: {args.command}")
        return 1
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main()) 


========== FILE: src\deployment\__init__.py ==========

"""
–ú–æ–¥—É–ª—å —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è (deployment) –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞

–°–æ–¥–µ—Ä–∂–∏—Ç –∫–ª–∞—Å—Å—ã –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è:
- –°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–Ω—ã—Ö –ø–∞–π–ø–ª–∞–π–Ω–æ–≤
- –ó–∞–≥—Ä—É–∑–∫–∏ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ
- CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏
"""

from .model_serializer import UniversalModelSerializer
from .production_pipeline import ProductionPipeline

__all__ = [
    'UniversalModelSerializer',
    'ProductionPipeline'
] 


========== FILE: src\deployment\model_serializer.py ==========

"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ç–æ—Ä –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ ML –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
"""

import os
import json
import pickle
import joblib
import warnings
from datetime import datetime
from typing import Dict, Any, Tuple, Optional

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ TensorFlow –¥–ª—è Keras –º–æ–¥–µ–ª–µ–π
try:
    import tensorflow as tf
    from tensorflow.keras.models import load_model, save_model
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è sklearn
try:
    from sklearn.base import BaseEstimator
    import sklearn
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False


class UniversalModelSerializer:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–æ–µ–∫—Ç–µ.
    
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏:
    - TensorFlow/Keras –º–æ–¥–µ–ª–∏: SavedModel —Ñ–æ—Ä–º–∞—Ç
    - Sklearn –º–æ–¥–µ–ª–∏: joblib (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è numpy)
    - –û—Å—Ç–∞–ª—å–Ω—ã–µ: pickle (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π fallback)
    """
    
    SUPPORTED_FORMATS = ['tensorflow_savedmodel', 'joblib', 'pickle']
    
    @staticmethod
    def save_model(model: Any, save_path: str, model_type: Optional[str] = None) -> Dict[str, Any]:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        
        Args:
            model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            save_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None, —Ç–æ –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
            
        Returns:
            Dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        os.makedirs(save_path, exist_ok=True)
        
        # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
        if model_type is None:
            model_type = UniversalModelSerializer._detect_model_type(model)
        
        print(f"[Serializer] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ç–∏–ø–∞: {model_type}")
        
        model_info = {
            'model_type': model_type,
            'serialization_format': None,
            'model_path': None,
            'created_at': datetime.now().isoformat(),
            'library_versions': UniversalModelSerializer._get_library_versions()
        }
        
        try:
            if model_type == 'neural_network' and TENSORFLOW_AVAILABLE:
                # Keras/TensorFlow –º–æ–¥–µ–ª—å
                model_path = os.path.join(save_path, 'keras_model')
                model.save(model_path)
                model_info['serialization_format'] = 'tensorflow_savedmodel'
                model_info['model_path'] = 'keras_model'
                print(f"[Serializer] –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ TensorFlow SavedModel: {model_path}")
                
            elif model_type in ['logistic_regression', 'random_forest', 'gradient_boosting'] and SKLEARN_AVAILABLE:
                # Sklearn –º–æ–¥–µ–ª–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º joblib –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                model_path = os.path.join(save_path, 'sklearn_model.joblib')
                joblib.dump(model, model_path, compress=3)  # –°–∂–∞—Ç–∏–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞
                model_info['serialization_format'] = 'joblib'
                model_info['model_path'] = 'sklearn_model.joblib'
                print(f"[Serializer] –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ joblib: {model_path}")
                
            else:
                # Fallback –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
                model_path = os.path.join(save_path, 'model.pkl')
                with open(model_path, 'wb') as f:
                    pickle.dump(model, f)
                model_info['serialization_format'] = 'pickle'
                model_info['model_path'] = 'model.pkl'
                print(f"[Serializer] –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ pickle: {model_path}")
                
        except Exception as e:
            print(f"[Serializer] –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
            print(f"[Serializer] –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback (pickle)")
            
            # Fallback –∫ pickle –ø—Ä–∏ –ª—é–±–æ–π –æ—à–∏–±–∫–µ
            model_path = os.path.join(save_path, 'model_fallback.pkl')
            with open(model_path, 'wb') as f:
                pickle.dump(model, f)
            model_info['serialization_format'] = 'pickle'
            model_info['model_path'] = 'model_fallback.pkl'
            model_info['fallback_used'] = True
            model_info['original_error'] = str(e)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
        info_path = os.path.join(save_path, 'model_info.json')
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(model_info, f, indent=2, ensure_ascii=False)
        
        print(f"[Serializer] –ú–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {info_path}")
        return model_info
    
    @staticmethod
    def load_model(save_path: str) -> Tuple[Any, Dict[str, Any]]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è—è —Ñ–æ—Ä–º–∞—Ç
        
        Args:
            save_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
            
        Returns:
            Tuple[–º–æ–¥–µ–ª—å, –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è]
        """
        # –ß–∏—Ç–∞–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        info_path = os.path.join(save_path, 'model_info.json')
        if not os.path.exists(info_path):
            raise FileNotFoundError(f"–§–∞–π–ª model_info.json –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {save_path}")
        
        with open(info_path, 'r', encoding='utf-8') as f:
            model_info = json.load(f)
        
        model_path = os.path.join(save_path, model_info['model_path'])
        serialization_format = model_info['serialization_format']
        
        print(f"[Serializer] –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {model_path} (—Ñ–æ—Ä–º–∞—Ç: {serialization_format})")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –≤–µ—Ä—Å–∏–π
        UniversalModelSerializer._check_version_compatibility(model_info.get('library_versions', {}))
        
        try:
            if serialization_format == 'tensorflow_savedmodel':
                if not TENSORFLOW_AVAILABLE:
                    raise ImportError("TensorFlow –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ Keras –º–æ–¥–µ–ª–∏")
                model = load_model(model_path)
                
            elif serialization_format == 'joblib':
                if not SKLEARN_AVAILABLE:
                    raise ImportError("Scikit-learn –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ joblib –º–æ–¥–µ–ª–∏")
                model = joblib.load(model_path)
                
            elif serialization_format == 'pickle':
                with open(model_path, 'rb') as f:
                    model = pickle.load(f)
                    
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {serialization_format}")
            
            print(f"[Serializer] –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return model, model_info
            
        except Exception as e:
            print(f"[Serializer] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    @staticmethod
    def _detect_model_type(model: Any) -> str:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Keras/TensorFlow –º–æ–¥–µ–ª–∏
        if TENSORFLOW_AVAILABLE:
            try:
                if hasattr(model, 'save') and hasattr(model, 'predict'):
                    model_str = str(type(model))
                    if any(keyword in model_str.lower() for keyword in ['tensorflow', 'keras']):
                        return 'neural_network'
            except:
                pass
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º sklearn –º–æ–¥–µ–ª–∏
        if SKLEARN_AVAILABLE:
            try:
                if isinstance(model, BaseEstimator):
                    model_name = type(model).__name__.lower()
                    if 'logistic' in model_name:
                        return 'logistic_regression'
                    elif 'forest' in model_name:
                        return 'random_forest'
                    elif 'gradient' in model_name or 'gbm' in model_name:
                        return 'gradient_boosting'
                    else:
                        return 'sklearn_unknown'
            except:
                pass
        
        return 'unknown'
    
    @staticmethod
    def _get_library_versions() -> Dict[str, str]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤–µ—Ä—Å–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫"""
        versions = {}
        
        try:
            import sklearn
            versions['sklearn'] = sklearn.__version__
        except ImportError:
            versions['sklearn'] = 'not_available'
        
        try:
            import tensorflow as tf
            versions['tensorflow'] = tf.__version__
        except ImportError:
            versions['tensorflow'] = 'not_available'
        
        try:
            import pandas as pd
            versions['pandas'] = pd.__version__
        except ImportError:
            versions['pandas'] = 'not_available'
        
        try:
            import numpy as np
            versions['numpy'] = np.__version__
        except ImportError:
            versions['numpy'] = 'not_available'
        
        return versions
    
    @staticmethod
    def _check_version_compatibility(saved_versions: Dict[str, str]) -> None:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –≤–µ—Ä—Å–∏–π –±–∏–±–ª–∏–æ—Ç–µ–∫"""
        if not saved_versions:
            return
        
        current_versions = UniversalModelSerializer._get_library_versions()
        
        for lib, saved_version in saved_versions.items():
            if lib in current_versions and current_versions[lib] != 'not_available':
                current_version = current_versions[lib]
                if saved_version != current_version and saved_version != 'not_available':
                    warnings.warn(
                        f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å {lib} {saved_version}, "
                        f"–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Å {lib} {current_version}. "
                        f"–í–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏."
                    )


def test_serializer():
    """–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    print("\n=== –¢–ï–°–¢ UNIVERSAL MODEL SERIALIZER ===")
    
    # –¢–µ—Å—Ç —Å –ø—Ä–æ—Å—Ç–æ–π sklearn –º–æ–¥–µ–ª—å—é (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
    if SKLEARN_AVAILABLE:
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.datasets import make_classification
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        X, y = make_classification(n_samples=100, n_features=4, n_classes=2, random_state=42)
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        model = RandomForestClassifier(n_estimators=10, random_state=42)
        model.fit(X, y)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—é
        test_path = "test_serialization"
        print(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤ –ø–∞–ø–∫—É: {test_path}")
        
        model_info = UniversalModelSerializer.save_model(model, test_path)
        print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {model_info}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É
        loaded_model, loaded_info = UniversalModelSerializer.load_model(test_path)
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {loaded_info['model_type']}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç
        predictions = loaded_model.predict(X[:5])
        print(f"–¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {predictions}")
        
        print("‚úÖ –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    else:
        print("‚ùå Sklearn –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")


if __name__ == "__main__":
    test_serializer() 


========== FILE: src\deployment\production_pipeline.py ==========

import os
import json
import pickle
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, Any, Optional, Union

try:
    from .model_serializer import UniversalModelSerializer
except ImportError:
    from model_serializer import UniversalModelSerializer


class ProductionPipeline:
    
    def __init__(self, preprocessor_states: Dict[str, Any], model: Any, metadata: Dict[str, Any]):
        self.preprocessor_states = preprocessor_states
        self.model = model
        self.metadata = metadata
        
        print(f"[ProductionPipeline] –°–æ–∑–¥–∞–Ω –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –º–æ–¥–µ–ª–∏: {metadata.get('model_type', 'unknown')}")
    
    def preprocess(self, raw_data: pd.DataFrame) -> pd.DataFrame:
        print(f"[ProductionPipeline] –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (shape: {raw_data.shape})")
        
        data = raw_data.copy()
        
        target_col = self.metadata.get('target_column')
        if target_col and target_col in data.columns:
            data = data.drop(columns=[target_col])
            print(f"[ProductionPipeline] –£–¥–∞–ª–µ–Ω–∞ —Ü–µ–ª–µ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ '{target_col}' –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
        
        dropped_columns = self.preprocessor_states.get('dropped_columns', [])
        if dropped_columns:
            cols_to_drop = [col for col in dropped_columns if col in data.columns]
            if cols_to_drop:
                data = data.drop(columns=cols_to_drop)
                print(f"[ProductionPipeline] –£–¥–∞–ª–µ–Ω—ã ID –∫–æ–ª–æ–Ω–∫–∏: {cols_to_drop}")
        
        if 'preprocessor' in self.preprocessor_states:
            data = self._apply_imputation(data)
            print(f"[ProductionPipeline] –ò–º–ø—É—Ç–∞—Ü–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∞")
        
        if 'preprocessor' in self.preprocessor_states:
            data = self._apply_encoding(data)
            print(f"[ProductionPipeline] –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–æ")
        
        if 'scaler' in self.preprocessor_states:
            data = self._apply_scaling(data)
            print(f"[ProductionPipeline] –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–æ")
        
        print(f"[ProductionPipeline] –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (final shape: {data.shape})")
        return data
    
    def predict(self, raw_data: pd.DataFrame) -> pd.DataFrame:
        print(f"[ProductionPipeline] –ù–∞—á–∞–ª–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {len(raw_data)} –∑–∞–ø–∏—Å–µ–π")
        
        processed_data = self.preprocess(raw_data)
        
        try:
            if hasattr(self.model, 'predict_proba'):
                probabilities = self.model.predict_proba(processed_data)
                predictions = self.model.predict(processed_data)
                results = self._format_results(raw_data, predictions, probabilities)
            else:
                predictions = self.model.predict(processed_data)
                results = self._format_results(raw_data, predictions)
            
            print(f"[ProductionPipeline] –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã")
            return results
            
        except Exception as e:
            print(f"[ProductionPipeline] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
            raise
    
    def _apply_imputation(self, data: pd.DataFrame) -> pd.DataFrame:
        if 'preprocessor' not in self.preprocessor_states:
            print(f"[ProductionPipeline] –ü—Ä–æ–ø—É—Å–∫ –∏–º–ø—É—Ç–∞—Ü–∏–∏ - —Å–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return data
            
        from preprocessing.data_preprocessor import DataPreprocessor
        preprocessor = DataPreprocessor()
        preprocessor.set_preprocessor_state(self.preprocessor_states['preprocessor'])
        
        config = self.preprocessor_states.get('processing_config', {})
        method = config.get('imputation_method', 'knn')
        params = config.get('imputation_params', {})
        
        print(f"[ProductionPipeline] –ò–º–ø—É—Ç–∞—Ü–∏—è: {method}")
        return preprocessor.impute(data, method=method, **params)
    
    def _apply_encoding(self, data: pd.DataFrame) -> pd.DataFrame:
        if 'preprocessor' not in self.preprocessor_states:
            print(f"[ProductionPipeline] –ü—Ä–æ–ø—É—Å–∫ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è - —Å–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return data
            
        from preprocessing.data_preprocessor import DataPreprocessor
        preprocessor = DataPreprocessor()
        preprocessor.set_preprocessor_state(self.preprocessor_states['preprocessor'])
        
        config = self.preprocessor_states.get('processing_config', {})
        method = config.get('encoding_method', 'label')
        params = config.get('encoding_params', {})
        target_col = self.metadata.get('target_column')
        
        print(f"[ProductionPipeline] –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: {method}")
        return preprocessor.encode(data, method=method, target_col=target_col, **params)
    
    def _apply_scaling(self, data: pd.DataFrame) -> pd.DataFrame:
        scaler = self.preprocessor_states.get('scaler')
        if scaler is None:
            print(f"[ProductionPipeline] –ü—Ä–æ–ø—É—Å–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è - —Å–∫–µ–π–ª–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return data
            
        target_col = self.metadata.get('target_column')
        
        if target_col in data.columns:
            X_features = data.drop(columns=[target_col])
            y_target = data[target_col]
        else:
            X_features = data
            y_target = None
        
        print(f"[ProductionPipeline] –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ: {self.preprocessor_states.get('scaler_method', 'unknown')}")
        
        try:
            scaled_features = scaler.transform(X_features)
            scaled_df = pd.DataFrame(scaled_features, columns=X_features.columns, index=X_features.index)
            
            if y_target is not None:
                return pd.concat([scaled_df, y_target], axis=1)
            else:
                return scaled_df
                
        except Exception as e:
            print(f"[ProductionPipeline] –û—à–∏–±–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: {e}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return data
    
    def _format_results(self, raw_data: pd.DataFrame, predictions: np.ndarray, 
                       probabilities: Optional[np.ndarray] = None) -> pd.DataFrame:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —É–¥–æ–±–Ω–æ–º –≤–∏–¥–µ
        
        Args:
            raw_data: –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            predictions: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
            probabilities: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
            
        Returns:
            DataFrame —Å –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        results = raw_data.copy()
        results['prediction'] = predictions
        
        if probabilities is not None:
            if len(probabilities.shape) > 1 and probabilities.shape[1] > 1:
                # –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–ª–∏ –±–∏–Ω–∞—Ä–Ω–∞—è —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
                for i in range(probabilities.shape[1]):
                    results[f'probability_class_{i}'] = probabilities[:, i]
                
                if probabilities.shape[1] == 2:  # –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
                    results['credit_score'] = probabilities[:, 1]  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
                    results['risk_level'] = results['credit_score'].apply(
                        lambda x: 'Low' if x > 0.7 else 'Medium' if x > 0.4 else 'High'
                    )
        
        return results
    
    def save(self, save_path: str) -> str:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ—Å—å –ø–∞–π–ø–ª–∞–π–Ω
        
        Args:
            save_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            
        Returns:
            –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É –ø–∞–π–ø–ª–∞–π–Ω—É
        """
        print(f"[ProductionPipeline] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –≤ {save_path}")
        os.makedirs(save_path, exist_ok=True)
        
        try:
            # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å (–∏—Å–ø–æ–ª—å–∑—É—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ç–æ—Ä)
            model_info = UniversalModelSerializer.save_model(
                self.model, 
                save_path, 
                self.metadata.get('model_type')
            )
            
            # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
            preprocessor_path = os.path.join(save_path, 'preprocessor_states.pkl')
            with open(preprocessor_path, 'wb') as f:
                pickle.dump(self.preprocessor_states, f)
            print(f"[ProductionPipeline] –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {preprocessor_path}")
            
            # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–∞–π–ø–ª–∞–π–Ω–∞
            pipeline_metadata = {
                **self.metadata,
                'model_info': model_info,
                'preprocessor_path': 'preprocessor_states.pkl',
                'pipeline_created_at': datetime.now().isoformat(),
                'pipeline_version': '1.0'
            }
            
            metadata_path = os.path.join(save_path, 'pipeline_metadata.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(pipeline_metadata, f, indent=2, ensure_ascii=False, default=str)
            print(f"[ProductionPipeline] –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")
            
            print(f"[ProductionPipeline] ‚úÖ –ü–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}")
            return save_path
            
        except Exception as e:
            print(f"[ProductionPipeline] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
            raise
    
    @classmethod
    def load(cls, save_path: str) -> 'ProductionPipeline':
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ—Å—å –ø–∞–π–ø–ª–∞–π–Ω
        
        Args:
            save_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º –ø–∞–π–ø–ª–∞–π–Ω–æ–º
            
        Returns:
            –≠–∫–∑–µ–º–ø–ª—è—Ä ProductionPipeline
        """
        print(f"[ProductionPipeline] –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏–∑ {save_path}")
        
        try:
            # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata_path = os.path.join(save_path, 'pipeline_metadata.json')
            if not os.path.exists(metadata_path):
                raise FileNotFoundError(f"pipeline_metadata.json –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {save_path}")
            
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            model, model_info = UniversalModelSerializer.load_model(save_path)
            
            # 3. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
            preprocessor_path = os.path.join(save_path, metadata['preprocessor_path'])
            if not os.path.exists(preprocessor_path):
                raise FileNotFoundError(f"–§–∞–π–ª –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {preprocessor_path}")
            
            with open(preprocessor_path, 'rb') as f:
                preprocessor_states = pickle.load(f)
            
            print(f"[ProductionPipeline] ‚úÖ –ü–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
            return cls(preprocessor_states, model, metadata)
            
        except Exception as e:
            print(f"[ProductionPipeline] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            raise
    
    def get_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–π–ø–ª–∞–π–Ω–µ"""
        return {
            'model_type': self.metadata.get('model_type', 'unknown'),
            'dataset': self.metadata.get('dataset_name', 'unknown'),
            'chromosome': self.metadata.get('chromosome', []),
            'performance': self.metadata.get('performance', {}),
            'created_at': self.metadata.get('pipeline_created_at', 'unknown'),
            'preprocessing_steps': list(self.preprocessor_states.keys())
        }


def test_production_pipeline():
    """–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç ProductionPipeline"""
    print("\n=== –¢–ï–°–¢ PRODUCTION PIPELINE ===")
    
    try:
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.datasets import make_classification
        import pandas as pd
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        X, y = make_classification(n_samples=100, n_features=4, n_classes=2, random_state=42)
        df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(4)])
        df['target'] = y
        
        # –û–±—É—á–∞–µ–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å
        model = RandomForestClassifier(n_estimators=10, random_state=42)
        model.fit(df.drop('target', axis=1), df['target'])
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
        preprocessor_states = {
            'imputation': {'method': 'knn', 'params': {'n_neighbors': 5}},
            'encoding': {'method': 'label', 'params': {}},
            'scaling': {'method': 'standard', 'params': {}}
        }
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'model_type': 'random_forest',
            'dataset_name': 'test_dataset',
            'chromosome': [1, 2, 3, 4, 5],
            'performance': {'pr_auc': 0.85}
        }
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
        pipeline = ProductionPipeline(preprocessor_states, model, metadata)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        save_path = "test_pipeline"
        pipeline.save(save_path)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É
        loaded_pipeline = ProductionPipeline.load(save_path)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–Ω–∞ –∑–∞–≥–ª—É—à–∫–µ)
        test_data = df.drop('target', axis=1)[:5]
        results = loaded_pipeline.predict(test_data)
        
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:")
        print(results.head())
        
        print("‚úÖ –¢–µ—Å—Ç ProductionPipeline –ø—Ä–æ–π–¥–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ ProductionPipeline: {e}")


if __name__ == "__main__":
    test_production_pipeline() 


========== FILE: src\ga_optimizer.py ==========

import os
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt 

from pipeline_processor import (
    process_data, get_dataset_name, decode_and_log_chromosome,
    ChromosomeConfig, train_model
)

class GAConfig:
    def __init__(self, 
                 train_path=None,
                 test_path=None,
                 target_column=None,
                 population_size=10,
                 num_generations=8,
                 elitism_percent=0.25,
                 mutation_rate=0.1,
                 tournament_size=3,
                 generate_learning_curves=False):
        
        if train_path is None:
            raise ValueError("train_path is required")
        if target_column is None:
            raise ValueError("target_column is required")
        
        self.train_path = train_path
        self.test_path = test_path
        self.target_column = target_column
        
        self.population_size = population_size
        self.num_generations = num_generations
        self.elitism_percent = elitism_percent
        self.mutation_rate = mutation_rate
        self.tournament_size = tournament_size
        self.generate_learning_curves = generate_learning_curves
        
        self.max_generations = num_generations
    
    def __str__(self):
        return (f"GAConfig(train_path='{self.train_path}', "
                f"target_column='{self.target_column}', "
                f"population_size={self.population_size}, "
                f"num_generations={self.num_generations})")

config = ChromosomeConfig()

IMPUTATION_MAP = config.IMPUTATION_MAP
OUTLIER_MAP = config.OUTLIER_MAP
RESAMPLING_MAP = config.RESAMPLING_MAP
ENCODING_MAP = config.ENCODING_MAP
SCALING_MAP = config.SCALING_MAP
MODEL_MAP = config.MODEL_MAP
HP_IMPUTATION_KNN_N_NEIGHBORS = config.HP_IMPUTATION_KNN_N_NEIGHBORS
HP_IMPUTATION_MISSFOREST_N_ESTIMATORS = config.HP_IMPUTATION_MISSFOREST_N_ESTIMATORS
HP_IMPUTATION_MISSFOREST_MAX_ITER = config.HP_IMPUTATION_MISSFOREST_MAX_ITER

HP_OUTLIER_IF_N_ESTIMATORS = config.HP_OUTLIER_IF_N_ESTIMATORS
HP_OUTLIER_IF_CONTAMINATION = config.HP_OUTLIER_IF_CONTAMINATION
HP_OUTLIER_IQR_MULTIPLIER = config.HP_OUTLIER_IQR_MULTIPLIER

HP_RESAMPLING_ROS_STRATEGY = config.HP_RESAMPLING_ROS_STRATEGY
HP_RESAMPLING_SMOTE_K_NEIGHBORS = config.HP_RESAMPLING_SMOTE_K_NEIGHBORS
HP_RESAMPLING_SMOTE_STRATEGY = config.HP_RESAMPLING_SMOTE_STRATEGY
HP_RESAMPLING_ADASYN_N_NEIGHBORS = config.HP_RESAMPLING_ADASYN_N_NEIGHBORS
HP_RESAMPLING_ADASYN_STRATEGY = config.HP_RESAMPLING_ADASYN_STRATEGY

HP_ENCODING_ONEHOT_MAX_CARDINALITY = config.HP_ENCODING_ONEHOT_MAX_CARDINALITY
HP_ENCODING_ONEHOT_DROP = config.HP_ENCODING_ONEHOT_DROP
HP_ENCODING_LSA_N_COMPONENTS = config.HP_ENCODING_LSA_N_COMPONENTS
HP_ENCODING_LSA_NGRAM_MAX = config.HP_ENCODING_LSA_NGRAM_MAX
HP_ENCODING_W2V_DIM = config.HP_ENCODING_W2V_DIM
HP_ENCODING_W2V_WINDOW = config.HP_ENCODING_W2V_WINDOW

HP_SCALING_STANDARD_WITH_MEAN = config.HP_SCALING_STANDARD_WITH_MEAN
HP_SCALING_STANDARD_WITH_STD = config.HP_SCALING_STANDARD_WITH_STD

HP_MODEL_LOGREG_C = config.HP_MODEL_LOGREG_C
HP_MODEL_LOGREG_PENALTY_SOLVER = config.HP_MODEL_LOGREG_PENALTY_SOLVER
HP_MODEL_LOGREG_CLASS_WEIGHT = config.HP_MODEL_LOGREG_CLASS_WEIGHT
HP_MODEL_LOGREG_MAX_ITER = config.HP_MODEL_LOGREG_MAX_ITER

HP_MODEL_RF_N_ESTIMATORS = config.HP_MODEL_RF_N_ESTIMATORS
HP_MODEL_RF_MAX_DEPTH = config.HP_MODEL_RF_MAX_DEPTH
HP_MODEL_RF_MIN_SAMPLES_SPLIT = config.HP_MODEL_RF_MIN_SAMPLES_SPLIT
HP_MODEL_RF_MIN_SAMPLES_LEAF = config.HP_MODEL_RF_MIN_SAMPLES_LEAF

HP_MODEL_GB_N_ESTIMATORS = config.HP_MODEL_GB_N_ESTIMATORS
HP_MODEL_GB_LEARNING_RATE = config.HP_MODEL_GB_LEARNING_RATE
HP_MODEL_GB_MAX_DEPTH = config.HP_MODEL_GB_MAX_DEPTH
HP_MODEL_GB_SUBSAMPLE = config.HP_MODEL_GB_SUBSAMPLE

HP_MODEL_NN_LAYERS = config.HP_MODEL_NN_LAYERS
HP_MODEL_NN_DROPOUT = config.HP_MODEL_NN_DROPOUT
HP_MODEL_NN_LR = config.HP_MODEL_NN_LR
HP_MODEL_NN_BATCH_SIZE = config.HP_MODEL_NN_BATCH_SIZE

POPULATION_SIZE = 10  
NUM_GENERATIONS = 8
MAX_GENERATIONS = NUM_GENERATIONS
ELITISM_PERCENT = 0.25 
MUTATION_RATE = 0.1   
TOURNAMENT_SIZE = 3

TRAIN_PATH = "../datasets/diabetes.csv"
TARGET_COLUMN = "Outcome"  


GENE_CHOICES_COUNT = [
    len(IMPUTATION_MAP), # Imputation Method (0)
    max(len(HP_IMPUTATION_KNN_N_NEIGHBORS), len(HP_IMPUTATION_MISSFOREST_N_ESTIMATORS)), # Imputation HP1 (1)
    len(HP_IMPUTATION_MISSFOREST_MAX_ITER), # Imputation HP2 (2)
    
    len(OUTLIER_MAP),    # Outlier Method (3)
    max(len(HP_OUTLIER_IF_N_ESTIMATORS), len(HP_OUTLIER_IQR_MULTIPLIER)), # Outlier HP1 (4)
    len(HP_OUTLIER_IF_CONTAMINATION), # Outlier HP2 (5)
    
    len(RESAMPLING_MAP), # Resampling Method (6)
    max(len(HP_RESAMPLING_ROS_STRATEGY), len(HP_RESAMPLING_SMOTE_K_NEIGHBORS), len(HP_RESAMPLING_ADASYN_N_NEIGHBORS)), # Resampling HP1 (7)
    max(len(HP_RESAMPLING_SMOTE_STRATEGY), len(HP_RESAMPLING_ADASYN_STRATEGY)), # Resampling HP2 (8)
    
    len(ENCODING_MAP),   # Encoding Method (9)
    max(len(HP_ENCODING_ONEHOT_MAX_CARDINALITY), len(HP_ENCODING_LSA_N_COMPONENTS), len(HP_ENCODING_W2V_DIM)), # Encoding HP1 (10)
    max(len(HP_ENCODING_ONEHOT_DROP), len(HP_ENCODING_LSA_NGRAM_MAX), len(HP_ENCODING_W2V_WINDOW)), # Encoding HP2 (11)
    
    len(SCALING_MAP),    # Scaling Method (12)
    len(HP_SCALING_STANDARD_WITH_MEAN),      # Scaling HP1 (13)
    len(HP_SCALING_STANDARD_WITH_STD),       # Scaling HP2 (14)
    
    len(MODEL_MAP),      # Model Method (15)
    max(len(HP_MODEL_LOGREG_C), len(HP_MODEL_RF_N_ESTIMATORS), len(HP_MODEL_GB_N_ESTIMATORS), len(HP_MODEL_NN_LAYERS)), # Model HP1 (16)
    max(len(HP_MODEL_LOGREG_PENALTY_SOLVER), len(HP_MODEL_RF_MAX_DEPTH), len(HP_MODEL_GB_LEARNING_RATE), len(HP_MODEL_NN_DROPOUT)), # Model HP2 (17)
    max(len(HP_MODEL_LOGREG_CLASS_WEIGHT), len(HP_MODEL_RF_MIN_SAMPLES_SPLIT), len(HP_MODEL_GB_MAX_DEPTH), len(HP_MODEL_NN_LR)), # Model HP3 (18)
    max(len(HP_MODEL_LOGREG_MAX_ITER), len(HP_MODEL_RF_MIN_SAMPLES_LEAF), len(HP_MODEL_GB_SUBSAMPLE), len(HP_MODEL_NN_BATCH_SIZE)) # Model HP4 (19)
]

if len(GENE_CHOICES_COUNT) != 20:
    raise ValueError(f"GENE_CHOICES_COUNT should have 20 elements, but has {len(GENE_CHOICES_COUNT)}")

def initialize_individual():
    individual = [np.random.randint(0, max_val) if max_val > 0 else 0 for max_val in GENE_CHOICES_COUNT]
    return individual

def initialize_population(pop_size):
    return [initialize_individual() for _ in range(pop_size)]

def select_parent_tournament(population_with_fitness, tournament_size):
    if not population_with_fitness:
        return initialize_individual()

    actual_tournament_size = min(tournament_size, len(population_with_fitness))
    if actual_tournament_size == 0:
        return initialize_individual()

    tournament_indices = np.random.choice(len(population_with_fitness), size=actual_tournament_size, replace=False)
    tournament_contestants = [population_with_fitness[i] for i in tournament_indices]
    
    winner = max(tournament_contestants, key=lambda x: x[1])
    return winner[0]

def crossover(parent1, parent2):
    if len(parent1) != len(parent2):
        raise ValueError("Parents must have the same chromosome length for crossover.")
    if len(parent1) < 2:
        return list(parent1), list(parent2)

    c_point = np.random.randint(1, len(parent1)) 
    
    offspring1 = parent1[:c_point] + parent2[c_point:]
    offspring2 = parent2[:c_point] + parent1[c_point:]
    
    return offspring1, offspring2

def mutate(chromosome, mutation_rate, gene_choices_count):
    """Performs mutation on a chromosome.
    Args:
        chromosome: The chromosome to mutate (list of genes).
        mutation_rate: The probability of each gene mutating.
        gene_choices_count: List of max values for each gene, to ensure valid mutations.
    Returns:
        The mutated chromosome (list of genes).
    """
    mutated_chromosome = list(chromosome) 
    for i in range(len(mutated_chromosome)):
        if np.random.rand() < mutation_rate:
            if gene_choices_count[i] > 0: 
                current_value = mutated_chromosome[i]
                new_value = np.random.randint(0, gene_choices_count[i])

                if gene_choices_count[i] > 1:
                    while new_value == current_value:
                        new_value = np.random.randint(0, gene_choices_count[i])
                mutated_chromosome[i] = new_value
 
    return mutated_chromosome

def evaluate_chromosome(chromosome_genes, train_path_ga, test_path_ga, target_column_ga, base_research_path_ga, gen_learning_curves_ga):
    """Evaluates a single chromosome and returns its fitness (PR AUC) and model type."""
    decoded_pipeline_params = decode_and_log_chromosome(chromosome_genes) 
    
    # Initialize model_type to None in case of early failure
    model_type_for_return = None

    if decoded_pipeline_params is None:
        return -1.0, model_type_for_return # Low fitness for errors

    # Extract parameters from the decoded dictionary
    current_imputation_method = decoded_pipeline_params['imputation_method']
    current_imputation_params = decoded_pipeline_params['imputation_params']
    current_outlier_method = decoded_pipeline_params['outlier_method']
    current_outlier_params = decoded_pipeline_params['outlier_params']
    current_resampling_method = decoded_pipeline_params['resampling_method']
    current_resampling_params = decoded_pipeline_params['resampling_params']
    current_encoding_method = decoded_pipeline_params['encoding_method']
    current_encoding_params = decoded_pipeline_params['encoding_params']
    current_scaling_method = decoded_pipeline_params['scaling_method']
    current_scaling_params = decoded_pipeline_params['scaling_params']
    current_model_type = decoded_pipeline_params['model_type']
    current_model_params = decoded_pipeline_params['model_params']
    
    model_type_for_return = current_model_type # Assign actual model type if decoding was successful

    fitness = -1.0 
    try:
        print(f"\n[GA - Chromosome: {chromosome_genes}] Calling process_data...") # Reduced verbosity
        processed_train_data_df, processed_test_data_df, research_path_for_chromosome_config, preprocessor_states = process_data(
            train_path_ga, test_path_ga, target_column_ga,
            imputation_method=current_imputation_method,
            imputation_params=current_imputation_params,
            outlier_method=current_outlier_method, 
            outlier_params=current_outlier_params,
            encoding_method=current_encoding_method,
            encoding_params=current_encoding_params,
            resampling_method=current_resampling_method, 
            resampling_params=current_resampling_params,
            scaling_method=current_scaling_method,
            scaling_params=current_scaling_params,
            save_processed_data=False,
            save_model_artifacts=False
        )
        
        if processed_train_data_df is None: 
            print(f"Data processing failed for chromosome {chromosome_genes}. Skipping model training.") # Keep for potential error feedback
            return -1.0, model_type_for_return # Return model_type even on processing failure
            
        # print(f"[GA - Chromosome: {chromosome_genes}] Calling train_model for model: {current_model_type}...") # Reduced verbosity
        metrics_output, ft_importance_output, trainer_dropped_cols = train_model(
            processed_train_data_df, 
            processed_test_data_df,  
            target_column_ga,
            research_path=research_path_for_chromosome_config, 
            model_type=current_model_type,
            model_hyperparameters=current_model_params, # Pass model HPs
            plot_learning_curves=gen_learning_curves_ga,
            save_run_results=False # Explicitly set to False for GA runs
        )

        if metrics_output:
            metric_source_dict = None
            eval_set_key_used = None
            fitness_metric_name = "AUPRC" # Default expected metric

            if 'test' in metrics_output and isinstance(metrics_output['test'], dict):
                metric_source_dict = metrics_output['test']
                eval_set_key_used = 'test'
            elif 'validation' in metrics_output and isinstance(metrics_output['validation'], dict):
                metric_source_dict = metrics_output['validation']
                eval_set_key_used = 'validation'
            
            if metric_source_dict: # Primary path: metrics are in a nested dict ('test' or 'validation')
                if 'auprc' in metric_source_dict:
                    fitness = metric_source_dict['auprc']
                    if fitness is None or pd.isna(fitness): 
                        print(f"Warning: AUPRC for chromosome {chromosome_genes} (eval set: {eval_set_key_used}) is None/NaN. Assigning low fitness.")
                        fitness = -0.5
                    else:
                        print(f"Model training completed for {chromosome_genes} (eval set: {eval_set_key_used}). Fitness (AUPRC): {fitness:.4f}") 
                elif 'average_precision_weighted' in metric_source_dict and current_model_type != 'neural_network': 
                    fitness = metric_source_dict['average_precision_weighted']
                    fitness_metric_name = "Avg_Precision_Weighted"
                    if fitness is None or pd.isna(fitness):
                        print(f"Warning: Weighted Average Precision for {chromosome_genes} (eval set: {eval_set_key_used}) is None/NaN. Assigning low fitness.") # Keep for potential error feedback
                        fitness = -0.6
                    else:
                        print(f"Model training completed for {chromosome_genes} (eval set: {eval_set_key_used}). Fitness (Avg Precision Weighted): {fitness:.4f}") # Reduced verbosity
                elif 'accuracy' in metric_source_dict: # Fallback to accuracy if other primary metrics are missing
                    fitness = metric_source_dict['accuracy'] 
                    fitness_metric_name = "Accuracy"
                    print(f"Warning: AUPRC and Avg_Precision_Weighted not found in '{eval_set_key_used}' dict for chromosome {chromosome_genes}. Using Accuracy: {fitness:.4f} as fitness.") # Keep for potential error feedback
                    if fitness is None or pd.isna(fitness):
                        fitness = -0.7 # Low fitness for NaN accuracy
                else:
                    print(f"Core fitness metrics (AUPRC, Avg_Precision_Weighted, Accuracy) not found in '{eval_set_key_used}' dict for chromosome {chromosome_genes}. Keys: {metric_source_dict.keys()}. Assigning low fitness.") # Keep for potential error feedback
                    fitness = -0.85
            else: # Fallback path: metrics might be in a flat structure directly under metrics_output
                if 'auprc' in metrics_output:
                    fitness = metrics_output['auprc']
                    eval_set_key_used = 'top-level'
                    fitness_metric_name = "AUPRC"
                    if fitness is None or pd.isna(fitness):
                        print(f"Warning: AUPRC for chromosome {chromosome_genes} (eval set: {eval_set_key_used}) is None/NaN. Assigning low fitness.") # Keep for potential error feedback
                        fitness = -0.51
                    else:
                        print(f"Model training completed for {chromosome_genes} (eval set: {eval_set_key_used}). Fitness (AUPRC): {fitness:.4f}") # Reduced verbosity
                elif 'average_precision_weighted' in metrics_output and current_model_type != 'neural_network':
                    fitness = metrics_output['average_precision_weighted']
                    fitness_metric_name = "Avg_Precision_Weighted"
                    eval_set_key_used = 'top-level'
                    if fitness is None or pd.isna(fitness):
                        print(f"Warning: Weighted Average Precision for {chromosome_genes} (eval set: {eval_set_key_used}) is None/NaN. Assigning low fitness.") # Keep for potential error feedback
                        fitness = -0.61
                    else:
                        print(f"Model training completed for {chromosome_genes} (eval set: {eval_set_key_used}). Fitness (Avg Precision Weighted): {fitness:.4f}") # Reduced verbosity
                elif 'accuracy' in metrics_output: # Fallback to accuracy in flat structure
                    fitness = metrics_output['accuracy']
                    fitness_metric_name = "Accuracy"
                    eval_set_key_used = 'top-level'
                    print(f"Warning: AUPRC and Avg_Precision_Weighted not found at top-level for chromosome {chromosome_genes}. Using Accuracy: {fitness:.4f} as fitness.") # Keep for potential error feedback
                    if fitness is None or pd.isna(fitness):
                        fitness = -0.71 # Low fitness for NaN accuracy at top-level
                else:
                    print(f"Neither nested eval dict nor top-level AUPRC/Avg_Prec_Weighted/Accuracy found in metrics_output for chromosome {chromosome_genes}. Metric Keys: {metrics_output.keys()}. Assigning low fitness.") # Keep for potential error feedback
                    fitness = -0.8
        else:
            print(f"Model training failed or did not produce metrics for chromosome {chromosome_genes}. Assigning low fitness.") # Keep for potential error feedback
            fitness = -0.9 
        
        print(f"[GA - Chromosome: {chromosome_genes}] Fitness ({fitness_metric_name}) determination complete.") # Reduced verbosity

    except Exception as e:
        print(f"Error evaluating chromosome {chromosome_genes}: {e}")
        fitness = -1.0 # Ensure low fitness on any exception

    print(f"Final Chromosome: {chromosome_genes}, Decoded Model: {model_type_for_return}, Fitness: {fitness:.4f}")
    return fitness, model_type_for_return

def run_genetic_algorithm(ga_config=None):
    """
    Main function to set up and run the genetic algorithm.
    
    Args:
        ga_config: GAConfig instance with parameters, or None for default config
    """
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∞
    if ga_config is None:
        ga_config = GAConfig(
            train_path=TRAIN_PATH,
            test_path=None,
            target_column=TARGET_COLUMN
        )
    
    print(f"\n=== Starting Genetic Algorithm ===")
    print(f"Configuration: {ga_config}")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    train_path = ga_config.train_path
    test_path = ga_config.test_path
    target_column = ga_config.target_column
    population_size = ga_config.population_size
    num_generations = ga_config.num_generations
    elitism_percent = ga_config.elitism_percent
    mutation_rate = ga_config.mutation_rate
    tournament_size = ga_config.tournament_size
    generate_learning_curves = ga_config.generate_learning_curves

    # --- Check for dataset existence ---
    if not os.path.exists(train_path):
        print(f"Error: Training data file not found at: {train_path}")
        return None
    if test_path is not None and not os.path.exists(test_path):
        print(f"Error: Test data file not found at: {test_path}")
        return None
    # ---

    ga_base_research_path = os.path.join("research", get_dataset_name(train_path), "genetic_algorithm_runs")
    os.makedirs(ga_base_research_path, exist_ok=True)

    population = initialize_population(population_size) 
    best_overall_chromosome = None
    best_overall_fitness = -1.0 
    best_fitness_over_generations = [] 
    all_individuals_details_over_generations = [] 
    
    best_fitness_per_model_type_over_generations = {
        model_name: [] for model_name in MODEL_MAP.values()
    }

    try: 
        for gen in range(num_generations):
            print(f"\n--- Generation {gen + 1}/{num_generations} ---") # Keep for progress tracking
            current_generation_details = [] 
            population_eval_results = [] 

            for i, ind_chromosome in enumerate(population):
                print(f"\nEvaluating Individual {i+1}/{population_size} in Generation {gen+1}")
                current_fitness, current_model_type = evaluate_chromosome(ind_chromosome, train_path, test_path, target_column, ga_base_research_path, generate_learning_curves)
                population_eval_results.append((ind_chromosome, current_fitness, current_model_type))
                current_generation_details.append((current_fitness, current_model_type)) 
                
                if current_fitness > best_overall_fitness:
                    best_overall_fitness = current_fitness
                    best_overall_chromosome = ind_chromosome
                    print(f"*** New best: Gen {gen+1}, Ind {i+1}: {best_overall_chromosome} -> Fit: {best_overall_fitness:.4f} (Model: {current_model_type}) ***") # Keep for important events

            population_eval_results.sort(key=lambda x: x[1], reverse=True)
            population_with_fitness = [(item[0], item[1]) for item in population_eval_results]
            
            if not population_eval_results: 
                print("Warning: Population is empty after evaluation (population_eval_results).") # Keep for potential error feedback
                for model_name in MODEL_MAP.values():
                    best_fitness_per_model_type_over_generations[model_name].append(np.nan)
                best_fitness_over_generations.append(np.nan) # Also for overall best
                all_individuals_details_over_generations.append([]) # No individuals
                break 
            
            current_gen_best_overall_fitness = population_eval_results[0][1]
            current_gen_best_overall_model_type = population_eval_results[0][2]
            best_fitness_over_generations.append(current_gen_best_overall_fitness)
            all_individuals_details_over_generations.append(current_generation_details)

            # Find and store best fitness for each model type in this generation
            fitness_by_model_this_gen = {model_name: [] for model_name in MODEL_MAP.values()}
            for _, fitness, model_type in population_eval_results: # Iterate through sorted results
                if model_type in fitness_by_model_this_gen:
                    fitness_by_model_this_gen[model_type].append(fitness)
            
            for model_name in MODEL_MAP.values():
                if fitness_by_model_this_gen[model_name]: # If this model type was present
                    best_fitness_per_model_type_over_generations[model_name].append(max(fitness_by_model_this_gen[model_name]))
                else: # Model type not present in this generation
                    best_fitness_per_model_type_over_generations[model_name].append(np.nan) # Use NaN for missing data points

            print(f"\nEnd of Generation {gen + 1}. Best overall fitness in this generation: {current_gen_best_overall_fitness:.4f}") # Reduced verbosity
            print(f"Best chromosome this generation: {population_eval_results[0][0]} (Model: {current_gen_best_overall_model_type}, Fitness: {population_eval_results[0][1]:.4f})") # Reduced verbosity

            if gen < num_generations - 1:
                next_population = []
                num_elite = int(population_size * elitism_percent)
                
                if num_elite > 0 and population_eval_results:
                    elite_individuals = [item[0] for item in population_eval_results[:num_elite]]
                    next_population.extend(elite_individuals)

                num_offspring_needed = population_size - len(next_population)
                offspring_generated = 0

                if not population_with_fitness:
                    next_population.extend([initialize_individual() for _ in range(num_offspring_needed)])
                else:
                    # Generate offspring through crossover and mutation
                    while offspring_generated < num_offspring_needed:
                        # Select parents using population_with_fitness (chromosome, fitness)
                        parent1 = select_parent_tournament(population_with_fitness, tournament_size)
                        parent2 = select_parent_tournament(population_with_fitness, tournament_size)
                        
                        offspring1, offspring2 = crossover(parent1, parent2)
                        
                        mutated_offspring1 = mutate(offspring1, mutation_rate, GENE_CHOICES_COUNT)
                        if offspring_generated < num_offspring_needed:
                            next_population.append(mutated_offspring1)
                            offspring_generated += 1
                        
                        if offspring_generated < num_offspring_needed:
                            mutated_offspring2 = mutate(offspring2, mutation_rate, GENE_CHOICES_COUNT)
                            next_population.append(mutated_offspring2)
                            offspring_generated += 1
                
                population = next_population[:population_size] 
                if not population and population_size > 0:
                    population = initialize_population(population_size)

    finally: 
        if best_fitness_over_generations:
            plt.figure(figsize=(15, 9)) # Adjusted figure size
            generations_x_axis = np.array(range(1, len(best_fitness_over_generations) + 1))

            model_colors = {
                'logistic_regression': 'cyan',
                'random_forest': 'green',
                'gradient_boosting': 'blue',
                'neural_network': 'purple',
                None: 'lightgrey' 
            }
            model_names_display = {
                'logistic_regression': 'Logistic Regression Best',
                'random_forest': 'Random Forest Best',
                'gradient_boosting': 'Gradient Boosting Best',
                'neural_network': 'Neural Network Best',
                None: 'Undefined/Error'
            }
            model_markers = { # Different markers for lines
                'logistic_regression': 's', # Square
                'random_forest': '^', # Triangle up
                'gradient_boosting': 'D', # Diamond
                'neural_network': 'P', # Plus (filled)
            }
            
            legend_handles_map = {} # Using a map for easier handle management

            for model_name, original_fitness_list in best_fitness_per_model_type_over_generations.items():
                if model_name in model_colors: # Ensure we have a color/marker for it
                    
                    # Create a plotting-specific list by carrying forward last known fitness for NaNs
                    plot_fitness_list_for_line = []
                    last_valid_fitness = np.nan 
                    for f_val in original_fitness_list:
                        if pd.isna(f_val): # Using pd.isna for robust NaN check
                            plot_fitness_list_for_line.append(last_valid_fitness) # Carry forward
                        else:
                            plot_fitness_list_for_line.append(f_val)
                            last_valid_fitness = f_val # Update last_valid_fitness

                    # Only plot if there was at least one actual data point for this model
                    if not all(pd.isna(f) for f in original_fitness_list):
                        # Plot the continuous line (using potentially carried-forward values)
                        line_plot, = plt.plot(generations_x_axis, plot_fitness_list_for_line, 
                                             linestyle='--', 
                                             linewidth=1.5, 
                                             color=model_colors[model_name],
                                             label=model_names_display.get(model_name, model_name)
                                             # No marker on this line plot itself
                                             )
                        legend_handles_map[model_names_display.get(model_name, model_name)] = line_plot

                        # Now, plot markers only at actual data points from original_fitness_list
                        actual_data_x_coords = [generations_x_axis[i] for i, y_val in enumerate(original_fitness_list) if not pd.isna(y_val)]
                        actual_data_y_coords = [y_val for y_val in original_fitness_list if not pd.isna(y_val)]
                        
                        if actual_data_x_coords: 
                            plt.plot(actual_data_x_coords, actual_data_y_coords, 
                                     marker=model_markers.get(model_name, '.'), 
                                     linestyle='None', # Crucial: no line for this plot, just markers
                                     color=model_colors[model_name],
                                     markersize=7
                                     )
            
            # 3. Plot the overall best fitness line (highlighted)
            best_overall_line, = plt.plot(generations_x_axis, best_fitness_over_generations, marker='o', linestyle='-', color='red', 
                                          linewidth=3, markersize=9, label='Overall Best Fitness')
            legend_handles_map['Overall Best Fitness'] = best_overall_line
            
            plt.title('GA Fitness Progression: Best per Model Type, Individuals & Overall Best', fontsize=16)
            plt.xlabel('Generation', fontsize=14)
            plt.ylabel('Fitness (AUPRC)', fontsize=14)
            plt.xticks(generations_x_axis, fontsize=12)
            plt.yticks(fontsize=12)
            
            # Create legend with a specific order
            preferred_order = ['Overall Best Fitness']
            # Add model type best lines to preferred order
            for mt_key in MODEL_MAP.values():
                display_name = model_names_display.get(mt_key)
                if display_name in legend_handles_map and display_name not in preferred_order:
                    preferred_order.append(display_name)

            final_handles = [legend_handles_map[label] for label in preferred_order if label in legend_handles_map]
            final_labels = [label for label in preferred_order if label in legend_handles_map]

            if final_handles: # Only show legend if there are items to show
                plt.legend(handles=final_handles, labels=final_labels, loc='center left', bbox_to_anchor=(1.02, 0.5), fontsize=10, title="Legend", title_fontsize="12")
                plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend
            else:
                plt.tight_layout() # Standard layout if no legend
            
            plt.grid(True, linestyle=':', alpha=0.6)
            plot_save_path = os.path.join(ga_base_research_path, "ga_fitness_progression_detailed_v2.png") # New name
            try:
                plt.savefig(plot_save_path)
                print(f"\nFitness progression plot saved to {plot_save_path}") # Keep for confirmation
            except Exception as e:
                print(f"Error saving fitness progression plot: {e}") # Keep for error feedback
            plt.close()

    print("\n=== Genetic Algorithm Finished ===") # Keep for context
    if best_overall_chromosome:
        print(f"Best overall chromosome found across all generations: {best_overall_chromosome}") # Keep for results
        print("Details of the Best Chromosome:") # Keep for results
        decode_and_log_chromosome(best_overall_chromosome) 
        print(f"Best overall fitness (AUPRC): {best_overall_fitness:.4f}") # Keep for results
    else:
        print("No successful evaluation run or no improvement found in the GA.") # Keep for context
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    return {
        'best_chromosome': best_overall_chromosome,
        'best_fitness': best_overall_fitness,
        'fitness_history': best_fitness_over_generations,
        'config': ga_config
    }

if __name__ == '__main__':
    run_genetic_algorithm() 


========== FILE: src\main.py ==========

import warnings
import ga_optimizer
# Attempt to import SklearnFutureWarning, fallback to built-in FutureWarning
try:
    from sklearn.exceptions import FutureWarning as SklearnFutureWarning
except ImportError:
    # For older scikit-learn versions where sklearn.exceptions.FutureWarning might not exist
    SklearnFutureWarning = FutureWarning 

# –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ FutureWarning –æ—Ç sklearn, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –≤–∏–¥–∏—Ç–µ
warnings.filterwarnings("ignore", category=SklearnFutureWarning, message=".*`BaseEstimator._check_n_features` is deprecated.*")
warnings.filterwarnings("ignore", category=SklearnFutureWarning, message=".*`BaseEstimator._check_feature_names` is deprecated.*")
warnings.filterwarnings("ignore", message=".*'force_all_finite' was renamed to 'ensure_all_finite'.*")
# –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏ –¥—Ä—É–≥–∏–µ, –µ—Å–ª–∏ –ø–æ—è–≤—è—Ç—Å—è, –∏–ª–∏ —Å–¥–µ–ª–∞—Ç—å —Ñ–∏–ª—å—Ç—Ä –±–æ–ª–µ–µ –æ–±—â–∏–º, –Ω–æ —ç—Ç–æ –º–µ–Ω–µ–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è

from preprocessing.data_loader import DataLoader
from preprocessing.data_preprocessor import DataPreprocessor
from preprocessing.outlier_remover import OutlierRemover
from preprocessing.resampler import Resampler
from modeling.model_trainer import ModelTrainer
import os
import pandas as pd
from utils.data_analysis import (
    # print_missing_summary, # Will reduce verbosity
    # print_numeric_stats, # Will reduce verbosity
    # print_target_distribution, # Will reduce verbosity
    # analyze_target_correlations, # Will reduce verbosity
    save_model_results
)
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import numpy as np

# Import the moved functions
from pipeline_processor import process_data, get_dataset_name, decode_and_log_chromosome, train_model

# –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ - —Ç–µ–ø–µ—Ä—å –≤—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –≤ pipeline_processor.py

def main():
    # print("\n=== Processing credit score classification dataset (Single Run Example with HPs) ===")
    # train_path = "datasets/credit-score-classification-manual-cleaned.csv"
    # test_path = None
    # target_column = "Credit_Score"
    train_path = "datasets/Loan_Default.csv"
    test_path = None
    target_column = "Status"
    generate_learning_curves = False 

    # train_path = "datasets/credit-score-classification/train.csv"
    # test_path = "datasets/credit-score-classification/test.csv"
    # target_column = "Credit_Score"
    # generate_learning_curves = False    

    # Chromosomes to test
   # chromosome_1 = [2, 4, 0, 0, 0, 1, 2, 4, 0, 1, 0, 3, 2, 1, 0, 1, 7, 2, 5, 1] 
    chromosome_2 = [1, 2, 2, 1, 3, 1, 1, 4, 4, 0, 1, 0, 2, 1, 0, 1, 6, 1, 0, 2]

    test_chromosomes = {
     #   "Chromosome_1_MissForest": chromosome_1,
        "Chromosome_2_y_1d_array": chromosome_2
    }

    for name, current_chromosome in test_chromosomes.items():
        print(f"\n\n=== TESTING PIPELINE FOR: {name} ===")
        print(f"Chromosome: {current_chromosome}")
        
        decoded_params = decode_and_log_chromosome(current_chromosome)

        if decoded_params is None:
            print(f"Error decoding {name}. Skipping this test run.")
            continue

        print(f"\n--- Running Pipeline with Decoded Chromosome: {name} ---")
        # Parameters are now in decoded_params

        try:
            print(f"\n[Test Run - {name}] Calling process_data...")
            processed_train_path, processed_test_path, research_base_path = process_data(
                train_path, test_path, target_column,
                imputation_method=decoded_params['imputation_method'],
                imputation_params=decoded_params['imputation_params'],
                outlier_method=decoded_params['outlier_method'],
                outlier_params=decoded_params['outlier_params'],
                encoding_method=decoded_params['encoding_method'],
                encoding_params=decoded_params['encoding_params'],
                resampling_method=decoded_params['resampling_method'],
                resampling_params=decoded_params['resampling_params'],
                scaling_method=decoded_params['scaling_method'],
                scaling_params=decoded_params['scaling_params'],
                save_processed_data=True, # For test runs, save the intermediate files
                save_model_artifacts=True # Also save model artifacts
            )
            
            if processed_train_path is None: 
                print(f"Data processing failed for {name}. Skipping model training for this run.")
                continue

            print(f"[Test Run - {name}] Data processing completed. Processed train: {processed_train_path}")
            print(f"\n[Test Run - {name}] Calling train_model for model: {decoded_params['model_type']}...")
            
            metrics_output, ft_importance_output, trainer_dropped_cols = train_model(
                processed_train_path, processed_test_path, target_column,
                research_path=research_base_path, 
                model_type=decoded_params['model_type'],
                model_hyperparameters=decoded_params['model_params'], # Pass model HPs
                plot_learning_curves=generate_learning_curves,
                save_run_results=True 
            )

            if metrics_output:
                print(f"\n--- Test Run Final Metrics for {decoded_params['model_type']} ({name}) ---")
                auprc_score = metrics_output.get('auprc')

                if auprc_score is not None and not pd.isna(auprc_score):
                    print(f"Evaluation AUPRC: {auprc_score:.4f}")
                else:
                    accuracy_score = metrics_output.get('accuracy')
                    if accuracy_score is not None and not pd.isna(accuracy_score):
                        print(f"Evaluation AUPRC not available or is invalid. Accuracy: {accuracy_score:.4f}")
                    else:
                        print("Evaluation AUPRC and Accuracy are not available or are invalid for this test run.")
            else:
                print(f"Test run model training for {name} did not produce metrics.")
            print(f"[Test Run - {name}] Model training and metric display complete.")

        except Exception as e:
            print(f"Error in pipeline run for {name} ({current_chromosome}): {e}")
            import traceback
            traceback.print_exc()
        print(f"=== FINISHED TESTING PIPELINE FOR: {name} ===")


def run_ga_with_config():
    """–ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞ GA —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
    from ga_optimizer import GAConfig, run_genetic_algorithm
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –ì–ê
    ga_config = GAConfig(
        train_path="../datasets/diabetes.csv",
        test_path=None,
        target_column="Outcome",
        population_size=10,
        num_generations=8,
        elitism_percent=0.25,
        mutation_rate=0.1,
        tournament_size=3,
        generate_learning_curves=False
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ì–ê —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    results = run_genetic_algorithm(ga_config)
    
    if results:
        print(f"\n=== GA Results Summary ===")
        print(f"Best fitness: {results['best_fitness']:.4f}")
        print(f"Best chromosome: {results['best_chromosome']}")
    
    return results


if __name__ == "__main__":
    # –ú–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ö—Ä–æ–º–æ—Å–æ–º
    # main()
    
    # –ò–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –ì–ê —Å –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
    run_ga_with_config()


========== FILE: src\modeling\model_trainer.py ==========

import tensorflow as tf # Ensure TensorFlow is imported early
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score, average_precision_score, roc_auc_score, precision_recall_curve, auc, precision_score, recall_score
from sklearn.preprocessing import LabelBinarizer, LabelEncoder
from sklearn.linear_model import LogisticRegression
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l1_l2

class ModelTrainer:
    
    def __init__(self, model_type='random_forest', model_hyperparameters=None, random_state=42, validation_size=0.2):
        self.model_type = model_type
        self.random_state = random_state
        self.validation_size = validation_size
        self.model = None
        self.history = None
        self.model_hyperparameters = model_hyperparameters if model_hyperparameters is not None else {}
        
        if 'tensorflow' in globals() and self.model_type == 'neural_network':
            globals()['tensorflow'].random.set_seed(self.random_state)
        
    def plot_learning_curves(self, X, y, output_path):
        train_sizes, train_scores, test_scores = learning_curve(
            self.model, X, y,
            cv=5,
            n_jobs=-1,
            train_sizes=np.linspace(0.1, 1.0, 10),
            scoring='accuracy'
        )
        
        train_mean = np.mean(train_scores, axis=1)
        train_std = np.std(train_scores, axis=1)
        test_mean = np.mean(test_scores, axis=1)
        test_std = np.std(test_scores, axis=1)
        
        plt.figure(figsize=(10, 6))
        plt.plot(train_sizes, train_mean, label='Training score', color='blue', marker='o')
        plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.15, color='blue')
        plt.plot(train_sizes, test_mean, label='Cross-validation score', color='red', marker='o')
        plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.15, color='red')
        
        plt.xlabel('Training Examples')
        plt.ylabel('Accuracy Score')
        plt.title('Learning Curves (Scikit-learn)')
        plt.legend(loc='lower right')
        plt.grid(True)
        
        curves_path = os.path.join(output_path, 'sklearn_learning_curves.png')
        plt.savefig(curves_path)
        plt.close()
        
        return {
            'train_sizes': train_sizes.tolist(),
            'train_scores': {
                'mean': train_mean.tolist(),
                'std': train_std.tolist()
            },
            'test_scores': {
                'mean': test_mean.tolist(),
                'std': test_std.tolist()
            }
        }
        
    def _plot_keras_learning_curves(self, output_path):
        if self.history and output_path:
            os.makedirs(output_path, exist_ok=True) # Ensure output_path exists
            plt.figure(figsize=(12, 4))
            
            # Plot Loss
            plt.subplot(1, 2, 1)
            if 'loss' in self.history.history:
                 plt.plot(self.history.history['loss'], label='Train Loss')
            if 'val_loss' in self.history.history:
                plt.plot(self.history.history['val_loss'], label='Validation Loss')
            plt.title('Keras Model Loss')
            plt.xlabel('Epochs')
            plt.ylabel('Loss')
            if 'loss' in self.history.history or 'val_loss' in self.history.history: # Only add legend if there's data
                plt.legend()

            # Plot Accuracy (or other primary metric like AUC if available and preferred)
            plt.subplot(1, 2, 2)
            primary_metric_key, val_primary_metric_key = None, None

            # Prioritize auprc or auc for Keras plots if available
            if 'auprc' in self.history.history: # Check for specific AUPRC metric from Keras
                primary_metric_key = 'auprc'
                val_primary_metric_key = 'val_auprc' if 'val_auprc' in self.history.history else None
            elif any(k.startswith('auc') and 'val' not in k for k in self.history.history.keys()): # Generic AUC (could be ROC AUC)
                 try:
                     primary_metric_key = [k for k in self.history.history.keys() if k.startswith('auc') and 'val' not in k][0]
                     val_primary_metric_key = [k for k in self.history.history.keys() if k.startswith('val_auc') and primary_metric_key in k][0] if any(k.startswith('val_auc') and primary_metric_key in k for k in self.history.history.keys()) else None
                 except IndexError:
                     primary_metric_key = None # Fallback if no non-val AUC found
            elif 'accuracy' in self.history.history:
                primary_metric_key = 'accuracy'
                val_primary_metric_key = 'val_accuracy' if 'val_accuracy' in self.history.history else None
            
            metric_name_display = "Metric" # Default display name
            if primary_metric_key and primary_metric_key in self.history.history:
                metric_name_display = primary_metric_key.replace("_", " ").title() # e.g. "Auc" or "Accuracy"
                plt.plot(self.history.history[primary_metric_key], label=f'Train {metric_name_display}')
            if val_primary_metric_key and val_primary_metric_key in self.history.history:
                plt.plot(self.history.history[val_primary_metric_key], label=f'Validation {metric_name_display}')
            
            plt.title(f'Keras Model {metric_name_display}')
            plt.xlabel('Epochs')
            plt.ylabel(metric_name_display)
            if primary_metric_key or (val_primary_metric_key and val_primary_metric_key in self.history.history): # Ensure legend only if data plotted
                plt.legend()
            
            plot_file = os.path.join(output_path, f'{self.model_type}_keras_learning_curves.png')
            try:
                plt.savefig(plot_file)
                # print(f"Keras learning curves saved to {plot_file}")
            except Exception as e:
                print(f"Error saving Keras learning curves: {e}")
            plt.close()

    def train(self, train_data, test_data, target_column, output_path=None, plot_learning_curves=True, save_run_results=True): # Added save_run_results
        if train_data is None or train_data.empty:
            print("Error: Training data is None or empty. Aborting training.")
            return None, None, None

        _train_data = train_data.copy()
        _test_data = test_data.copy() if test_data is not None else pd.DataFrame()


        potential_id_cols = [col for col in _train_data.columns if col.lower() == 'id' and col != target_column]
        if potential_id_cols:
            _train_data = _train_data.drop(columns=potential_id_cols)
            if not _test_data.empty:
                _test_data = _test_data.drop(columns=potential_id_cols, errors='ignore')
        
        has_test_target = target_column in _test_data.columns and not _test_data[_test_data[target_column].notna()].empty

        X_train, y_train, X_test, y_test = None, None, None, None
        eval_set_description = "N/A"
        
        # --- Robust extraction of X and y ---
        # Handle y_train
        if target_column not in _train_data.columns:
            raise ValueError(f"Target column '{target_column}' not found in training data.")
        y_train_raw = _train_data[target_column]
        if isinstance(y_train_raw, pd.DataFrame):
            print(f"Warning: y_train extracted from _train_data['{target_column}'] was a DataFrame (shape: {y_train_raw.shape}). Using its first column as the target variable.")
            y_train = y_train_raw.iloc[:, 0]
        else:
            y_train = y_train_raw
        
        # Handle X_train (ensure no target column, even if duplicated)
        X_train = _train_data.loc[:, _train_data.columns != target_column]
        # ---

        if has_test_target:
            # --- Robust extraction for X_test and y_test ---
            if target_column not in _test_data.columns:
                 # This case should ideally be caught by has_test_target, but as a safeguard:
                print(f"Warning: Target column '{target_column}' not found in test data despite has_test_target being true. Proceeding as if no test target.")
                has_test_target = False # Correct the flag
                X_test = _test_data.copy() # X_test will be all of test_data, y_test will be None
                y_test = None
            else:
                y_test_raw = _test_data[target_column]
                if isinstance(y_test_raw, pd.DataFrame):
                    print(f"Warning: y_test extracted from _test_data['{target_column}'] was a DataFrame (shape: {y_test_raw.shape}). Using its first column as the target variable.")
                    y_test = y_test_raw.iloc[:, 0]
                else:
                    y_test = y_test_raw
                X_test = _test_data.loc[:, _test_data.columns != target_column]
            # ---
            eval_set_description = f"test_set (shape: {X_test.shape})"
        
        if not has_test_target: # This block executes if originally no test target OR if test target was problematic
            if y_train is None or y_train.empty: # y_train should be populated from _train_data above
                 raise ValueError(f"Target column '{target_column}' could not be properly extracted from training data for validation split.")

            # Stratify only if target is suitable
            stratify_on = None
            if y_train.nunique() > 1 and y_train.nunique() < len(y_train): # Check after y_train is confirmed 1D
                stratify_on = y_train

            # X_train was already prepared, so we use it directly
            # y_train was already prepared
            X_temp_train, X_val, y_temp_train, y_val = train_test_split(
                X_train, # Use already prepared X_train
                y_train, # Use already prepared y_train
                test_size=self.validation_size,
                random_state=self.random_state,
                stratify=stratify_on
            )
            X_train, y_train = X_temp_train, y_temp_train # Update X_train, y_train to be the smaller training portion
            X_test, y_test = X_val, y_val # X_test, y_test are now from the validation set
            eval_set_description = f"validation_set_from_train (shape: {X_test.shape})"

        X_train_cols_final = X_train.columns.tolist() # Columns used for training

        # Ensure target is numeric (label encoded)
        def _ensure_numeric_target(y_series, series_name="target"):
            if y_series is None: return None
            if not pd.api.types.is_numeric_dtype(y_series):
                try:
                    # Attempt direct conversion if it looks like numbers stored as strings
                    y_series_numeric = pd.to_numeric(y_series, errors='coerce')
                    if not y_series_numeric.isnull().all(): # If conversion was somewhat successful
                        if not pd.api.types.is_integer_dtype(y_series_numeric):
                             return y_series_numeric.astype(int) # Convert float to int if possible
                        return y_series_numeric
                    else: # Coercion failed, try LabelEncoding as last resort
                        print(f"Warning: Target column '{series_name}' is non-numeric. Attempting LabelEncoding.")
                        le = LabelEncoder()
                        return pd.Series(le.fit_transform(y_series.astype(str)), index=y_series.index, name=y_series.name)
                except Exception as e:
                    raise ValueError(f"Target column '{series_name}' could not be converted to numeric: {e}")
            elif not pd.api.types.is_integer_dtype(y_series):
                return y_series.astype(int) # Ensure integer type if already numeric
            return y_series

        y_train = _ensure_numeric_target(y_train, "y_train")
        y_test = _ensure_numeric_target(y_test, "y_test")

        hps = self.model_hyperparameters.copy() # Work with a copy

        if self.model_type == 'random_forest':
            self.model = RandomForestClassifier(random_state=self.random_state, **hps)
        elif self.model_type == 'logistic_regression':
            solver_penalty_config = hps.pop('solver_penalty_config', None)
            if solver_penalty_config and isinstance(solver_penalty_config, dict):
                hps.update(solver_penalty_config) 

            current_solver = hps.get('solver', 'lbfgs') 
            current_penalty = hps.get('penalty')

            compatible_solvers = {
                'l1': ['liblinear', 'saga'],
                'l2': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],
                'elasticnet': ['saga'],
                None: ['lbfgs', 'newton-cg', 'newton-cholesky', 'sag', 'saga'] 
            }
            
            penalty_key = current_penalty if isinstance(current_penalty, str) else None


            if penalty_key is not None and penalty_key in compatible_solvers:
                if current_solver not in compatible_solvers[penalty_key]:
                    new_solver = compatible_solvers[penalty_key][0] 
                    print(f"Warning: Solver '{current_solver}' for Logistic Regression is not compatible with penalty '{current_penalty}'. Changing solver to '{new_solver}'.")
                    hps['solver'] = new_solver
            elif penalty_key is None and 'penalty' in hps : 
                 if current_solver not in compatible_solvers[None]:
                    new_solver = compatible_solvers[None][0]
                    print(f"Warning: Solver '{current_solver}' for Logistic Regression is not compatible with no penalty. Changing solver to '{new_solver}'.")
                    hps['solver'] = new_solver


            if 'l1_ratio' in hps and hps.get('penalty') != 'elasticnet':
                hps.pop('l1_ratio') 
            if hps.get('penalty') == 'elasticnet' and 'l1_ratio' not in hps:
                 hps['l1_ratio'] = 0.5 

            self.model = LogisticRegression(random_state=self.random_state, **hps)

        elif self.model_type == 'gradient_boosting':
            self.model = GradientBoostingClassifier(random_state=self.random_state, **hps)
        
        elif self.model_type == 'neural_network':
            input_shape = (X_train.shape[1],)
            num_classes = y_train.nunique()

            epochs = hps.get('epochs', 50)
            batch_size = hps.get('batch_size', 32)
            early_stopping_patience = hps.get('early_stopping_patience', 5)
            
            keras_train_hps_keys = ['epochs', 'batch_size', 'early_stopping_patience']
            build_hps = {k: v for k, v in hps.items() if k not in keras_train_hps_keys}

            self.model = self._build_keras_model(input_shape, num_classes, **build_hps)
            
            y_train_keras = y_train.values.ravel()
            y_test_keras = y_test.values.ravel()

            if num_classes > 2:
                y_train_keras = to_categorical(y_train_keras, num_classes=num_classes)
                y_test_keras = to_categorical(y_test_keras, num_classes=num_classes)
            
            callbacks_list = []
            if early_stopping_patience > 0:
                early_stopping = EarlyStopping(monitor='val_loss', patience=early_stopping_patience, restore_best_weights=True, verbose=1)
                callbacks_list.append(early_stopping)

            print(f"Training Keras model for {epochs} epochs, batch size {batch_size}...")
            self.history = self.model.fit(
                X_train, y_train_keras,
                epochs=epochs,
                batch_size=batch_size,
                validation_data=(X_test, y_test_keras),
                callbacks=callbacks_list,
                verbose=0 
            )
            if plot_learning_curves and output_path:
                self._plot_keras_learning_curves(output_path)
        else:
            raise ValueError(f"Unsupported model type: {self.model_type}")

        if self.model_type != 'neural_network':
            print(f"Training {self.model_type}...")
            self.model.fit(X_train, y_train)
            if plot_learning_curves and output_path :
                 os.makedirs(output_path, exist_ok=True) 
                 self.plot_learning_curves(X_train, y_train, output_path)


        metrics = {'evaluation_set_description': eval_set_description}
        feature_importance_df = None

        print(f"Evaluating {self.model_type} on {eval_set_description}...")
        if X_test is not None and y_test is not None and not X_test.empty:
            y_pred = self.model.predict(X_test)
            y_pred_proba = self.model.predict_proba(X_test) if hasattr(self.model, "predict_proba") else None


            if self.model_type == 'neural_network':
                if y_pred.ndim > 1 and y_pred.shape[1] > 1: 
                    y_pred = np.argmax(y_pred, axis=1)
                elif y_pred.ndim == 1 or y_pred.shape[1] == 1: 
                    y_pred = (y_pred > 0.5).astype(int)

                if hasattr(self.model, 'predict'): 
                    keras_probas = self.model.predict(X_test)
                    if y_train.nunique() == 2: 
                        if keras_probas.ndim > 1 and keras_probas.shape[1] == 2:
                            y_pred_proba = keras_probas[:, 1] 
                        elif keras_probas.ndim == 1 or keras_probas.shape[1] == 1: 
                             y_pred_proba = keras_probas.ravel()
                    else: 
                        y_pred_proba = keras_probas


            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            metrics['accuracy'] = accuracy_score(y_test, y_pred)
            
            # F1-Score –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö
            if y_train.nunique() == 2:
                # –î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ - –æ—Å–Ω–æ–≤–Ω–æ–π F1
                metrics['f1_score'] = f1_score(y_test, y_pred, zero_division=0)
            else:
                # –î–ª—è –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–æ–π - macro –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π
                metrics['f1_score'] = f1_score(y_test, y_pred, average='macro', zero_division=0)
            
            metrics['f1_score_weighted'] = f1_score(y_test, y_pred, average='weighted', zero_division=0)
            metrics['f1_score_micro'] = f1_score(y_test, y_pred, average='micro', zero_division=0)
            metrics['f1_score_macro'] = f1_score(y_test, y_pred, average='macro', zero_division=0)
            
            # Precision –∏ Recall
            if y_train.nunique() == 2:
                metrics['precision'] = precision_score(y_test, y_pred, zero_division=0)
                metrics['recall'] = recall_score(y_test, y_pred, zero_division=0)
            else:
                metrics['precision'] = precision_score(y_test, y_pred, average='macro', zero_division=0)
                metrics['recall'] = recall_score(y_test, y_pred, average='macro', zero_division=0)
            
            num_unique_classes = y_train.nunique() 

            # ROC-AUC –∏ AUPRC
            if y_pred_proba is not None:
                try:
                    if num_unique_classes == 2: 
                        # –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
                        if y_pred_proba.ndim == 2 and y_pred_proba.shape[1] == 2:
                            y_proba_pos = y_pred_proba[:, 1]
                        elif y_pred_proba.ndim == 2 and y_pred_proba.shape[1] == 1:
                            y_proba_pos = y_pred_proba.ravel()
                        else:
                            y_proba_pos = y_pred_proba.ravel() if y_pred_proba.ndim > 1 else y_pred_proba

                        metrics['roc_auc'] = roc_auc_score(y_test, y_proba_pos)
                        metrics['auprc'] = average_precision_score(y_test, y_proba_pos)
                        
                    else: 
                        # –ú—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
                        metrics['roc_auc'] = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')
                        
                        # –î–ª—è AUPRC –≤ –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º macro-averaging
                        try:
                            lb = LabelBinarizer()
                            y_test_bin = lb.fit_transform(y_test)
                            if y_test_bin.shape[1] == 1:  # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ 2 –∫–ª–∞—Å—Å–∞ –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                                y_test_bin = np.column_stack([1 - y_test_bin, y_test_bin])
                            
                            if y_test_bin.shape[1] == y_pred_proba.shape[1]:
                                metrics['auprc'] = average_precision_score(y_test_bin, y_pred_proba, average='macro')
                            else:
                                print(f"Warning: Shape mismatch for AUPRC - using weighted average instead")
                                metrics['auprc'] = average_precision_score(y_test, y_pred_proba, average='weighted', multi_class='ovr')
                        except Exception as e_auprc:
                            print(f"Could not compute AUPRC for multi-class: {e_auprc}")
                            metrics['auprc'] = None

                except Exception as e_metrics: 
                    print(f"Could not compute ROC AUC or AUPRC: {e_metrics}")
                    metrics['roc_auc'] = None
                    metrics['auprc'] = None
            else:
                print("Warning: No probability predictions available - ROC AUC and AUPRC will be None")
                metrics['roc_auc'] = None
                metrics['auprc'] = None

            report_dict = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
            metrics['classification_report'] = report_dict

        if hasattr(self.model, 'feature_importances_'):
            feature_importance_df = pd.DataFrame({
                'feature': X_train_cols_final,
            'importance': self.model.feature_importances_
            }).sort_values(by='importance', ascending=False)
        elif hasattr(self.model, 'coef_') and self.model_type == 'logistic_regression':
            if self.model.coef_.ndim == 1 : 
                 coefs = self.model.coef_
            elif self.model.coef_.shape[0] == 1: 
                coefs = self.model.coef_[0]
            else: 
                coefs = np.mean(np.abs(self.model.coef_), axis=0)
            
            feature_importance_df = pd.DataFrame({
                'feature': X_train_cols_final,
                'importance': coefs 
            }).sort_values(by='importance', key=abs, ascending=False) 
        
        if output_path and save_run_results:
            os.makedirs(output_path, exist_ok=True)
            results_summary_path = os.path.join(output_path, f'{self.model_type}_results_summary.txt')
            with open(results_summary_path, 'w') as f:
                f.write(f"Model Type: {self.model_type}\n")
                f.write(f"Hyperparameters: {self.model_hyperparameters}\n")
                f.write(f"Evaluation Set: {eval_set_description}\n\n")
                f.write("Metrics:\n")
                for key, value in metrics.items():
                    if key == 'classification_report':
                        f.write(f"  {key}:\n")
                        for class_label, class_metrics in value.items():
                            if isinstance(class_metrics, dict):
                                f.write(f"    {class_label}:\n")
                                for metric_name, metric_value in class_metrics.items():
                                    f.write(f"      {metric_name}: {metric_value:.4f}\n")
                            else: 
                                f.write(f"    {class_label}: {value[class_label]:.4f}\n") 
                    else:
                        f.write(f"  {key}: {value}\n") 
                
                if feature_importance_df is not None:
                    f.write("\nFeature Importances:\n")
                    f.write(feature_importance_df.to_string())

        return metrics, feature_importance_df, potential_id_cols
        
    def predict(self, X):
        """
        Make predictions using the trained model.
        Args:
            X: pandas DataFrame, data to make predictions on
        Returns:
            numpy array: predictions
        """
        if self.model is None:
            raise ValueError("Model has not been trained yet.")
        
        return self.model.predict(X)

    def predict_proba(self, X):
        """
        Make probability predictions using the trained model.
        Args:
            X: pandas DataFrame, data to make predictions on
        Returns:
            numpy array: probability predictions
        """
        if self.model is None:
            raise ValueError("Model has not been trained yet.")
        if not hasattr(self.model, "predict_proba"):
            raise AttributeError(f"The model {self.model_type} does not support predict_proba.")
        return self.model.predict_proba(X)

    def _build_keras_model(self, input_shape, num_classes, **kwargs):
        """
        Builds a Keras Sequential model.
        Hyperparameters for layers, dropout, learning rate, and regularization are passed via kwargs.
        """
        hidden_layer_sizes = kwargs.get('hidden_layer_sizes', (64, 32)) # Default: (64, 32)
        dropout_rate = kwargs.get('dropout_rate', 0.2) # Default: 0.2
        learning_rate = kwargs.get('learning_rate', 0.001) # Default: 0.001
        l1_reg, l2_reg = kwargs.get('l1_reg', 0.00), kwargs.get('l2_reg', 0.00)
        model = Sequential([Input(shape=input_shape)])
        for units in hidden_layer_sizes:
            model.add(Dense(units, activation='relu', kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))
            if dropout_rate > 0: model.add(Dropout(dropout_rate))
        
        metrics_to_compile = ['accuracy'] # Initialize here

        if num_classes == 2: 
            model.add(Dense(1, activation='sigmoid'))
            loss_function = 'binary_crossentropy'
            metrics_to_compile.extend([tf.keras.metrics.AUC(name='roc_auc', curve='ROC'), tf.keras.metrics.AUC(name='auprc', curve='PR')])
        else: 
            model.add(Dense(num_classes, activation='softmax'))
            loss_function = 'categorical_crossentropy'
            # For multi-class, Keras AUC with multi_label=True and num_labels implies an averaging (e.g., macro) over per-class AUCs.
            metrics_to_compile.extend([
                tf.keras.metrics.AUC(name='roc_auc', curve='ROC', multi_label=True, num_labels=num_classes),
                tf.keras.metrics.AUC(name='auprc', curve='PR', multi_label=True, num_labels=num_classes)
            ])
            
        optimizer = Adam(learning_rate=learning_rate)
        
        model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics_to_compile)
        return model



========== FILE: src\pipeline_processor.py ==========

import os
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder

# Assuming these modules are in the same directory or PYTHONPATH is set up
from preprocessing.data_loader import DataLoader
from preprocessing.data_preprocessor import DataPreprocessor
from preprocessing.outlier_remover import OutlierRemover
from preprocessing.resampler import Resampler
from modeling.model_trainer import ModelTrainer


class ChromosomeConfig:
    
    IMPUTATION_MAP = {0: 'knn', 1: 'median', 2: 'missforest'}
    OUTLIER_MAP = {0: 'none', 1: 'isolation_forest', 2: 'iqr'}
    RESAMPLING_MAP = {0: 'none', 1: 'oversample', 2: 'smote', 3: 'adasyn'}
    ENCODING_MAP = {0: 'onehot', 1: 'label', 2: 'lsa', 3: 'word2vec'}
    SCALING_MAP = {0: 'none', 1: 'standard', 2: 'minmax'}
    MODEL_MAP = {0: 'logistic_regression', 1: 'random_forest', 2: 'gradient_boosting', 3: 'neural_network'}
    
    HP_IMPUTATION_KNN_N_NEIGHBORS = {0: 3, 1: 5, 2: 7, 3: 10, 4: 15}
    HP_IMPUTATION_MISSFOREST_N_ESTIMATORS = {0: 30, 1: 50, 2: 100, 3: 150, 4: 200}
    HP_IMPUTATION_MISSFOREST_MAX_ITER = {0: 5, 1: 10, 2: 15, 3: 20}
    
    HP_OUTLIER_IF_N_ESTIMATORS = {0: 30, 1: 50, 2: 100, 3: 150, 4: 200}
    HP_OUTLIER_IF_CONTAMINATION = {0: 'auto', 1: 0.01, 2: 0.025, 3: 0.05, 4: 0.1, 5: 0.15}
    HP_OUTLIER_IQR_MULTIPLIER = {0: 1.5, 1: 2.0, 2: 2.5, 3: 3.0}
    
    HP_RESAMPLING_ROS_STRATEGY = {0: 'auto', 1: 'minority', 2: 0.5, 3: 0.6, 4: 0.75}
    HP_RESAMPLING_SMOTE_K_NEIGHBORS = {0: 3, 1: 5, 2: 7, 3: 9}
    HP_RESAMPLING_SMOTE_STRATEGY = {0: 'auto', 1: 'minority', 2: 0.5, 3: 0.6, 4: 0.75}
    HP_RESAMPLING_ADASYN_N_NEIGHBORS = {0: 3, 1: 5, 2: 7, 3: 9}
    HP_RESAMPLING_ADASYN_STRATEGY = {0: 'auto', 1: 'minority', 2: 0.5, 3: 0.6, 4: 0.75}
    
    HP_ENCODING_ONEHOT_MAX_CARDINALITY = {0: 10, 1: 20, 2: 50, 3: 100}
    HP_ENCODING_ONEHOT_DROP = {0: None, 1: 'first'}
    HP_ENCODING_LSA_N_COMPONENTS = {0: 5, 1: 10, 2: 25, 3: 50, 4: 75}
    HP_ENCODING_LSA_NGRAM_MAX = {0: 1, 1: 2, 2: 3}
    HP_ENCODING_W2V_DIM = {0: 25, 1: 50, 2: 75, 3: 100, 4: 150}
    HP_ENCODING_W2V_WINDOW = {0: 1, 1: 2, 2: 3, 3: 5, 4: 7}
    
    HP_SCALING_STANDARD_WITH_MEAN = {0: True, 1: False}
    HP_SCALING_STANDARD_WITH_STD = {0: True, 1: False}
    
    HP_MODEL_LOGREG_C = {0: 0.001, 1: 0.01, 2: 0.1, 3: 1.0, 4: 10.0, 5: 100.0}
    HP_MODEL_LOGREG_PENALTY_SOLVER = {
        0: {'penalty': 'l2', 'solver': 'lbfgs', 'l1_ratio': None}, 
        1: {'penalty': 'l1', 'solver': 'liblinear', 'l1_ratio': None},
        2: {'penalty': 'l2', 'solver': 'liblinear', 'l1_ratio': None}, 
        3: {'penalty': 'l1', 'solver': 'saga', 'l1_ratio': None},
        4: {'penalty': 'l2', 'solver': 'saga', 'l1_ratio': None}, 
        5: {'penalty': 'elasticnet', 'solver': 'saga', 'l1_ratio': 0.5}
    }
    HP_MODEL_LOGREG_CLASS_WEIGHT = {0: None, 1: 'balanced'}
    HP_MODEL_LOGREG_MAX_ITER = {0: 100, 1: 200, 2: 300, 3: 500}
    
    HP_MODEL_RF_N_ESTIMATORS = {0: 25, 1: 50, 2: 100, 3: 200, 4: 300}
    HP_MODEL_RF_MAX_DEPTH = {0: 5, 1: 7, 2: 10, 3: 15, 4: 20, 5: None}
    HP_MODEL_RF_MIN_SAMPLES_SPLIT = {0: 2, 1: 5, 2: 10, 3: 15}
    HP_MODEL_RF_MIN_SAMPLES_LEAF = {0: 1, 1: 2, 2: 5, 3: 10}
    
    HP_MODEL_GB_N_ESTIMATORS = {0: 25, 1: 50, 2: 100, 3: 200, 4: 300}
    HP_MODEL_GB_LEARNING_RATE = {0: 0.005, 1: 0.01, 2: 0.05, 3: 0.1, 4: 0.2}
    HP_MODEL_GB_MAX_DEPTH = {0: 2, 1: 3, 2: 4, 3: 5, 4: 7}
    HP_MODEL_GB_SUBSAMPLE = {0: 0.7, 1: 0.8, 2: 0.9, 3: 1.0}
    
    HP_MODEL_NN_LAYERS = {
        0: (32,), 1: (64,), 2: (128,),
        3: (32, 32), 4: (64, 32), 5: (128, 64), 
        6: (64, 64), 7: (128, 64, 32)
    }
    HP_MODEL_NN_DROPOUT = {0: 0.0, 1: 0.1, 2: 0.2, 3: 0.3, 4: 0.4, 5: 0.5}
    HP_MODEL_NN_LR = {0: 0.0001, 1: 0.0005, 2: 0.001, 3: 0.005, 4: 0.01, 5: 0.05}
    HP_MODEL_NN_BATCH_SIZE = {0: 16, 1: 32, 2: 64, 3: 128}
    
    GENE_DESCRIPTIONS = [
        "Imputation Method", "Imputation HP1", "Imputation HP2",
        "Outlier Method", "Outlier HP1", "Outlier HP2",
        "Resampling Method", "Resampling HP1", "Resampling HP2",
        "Encoding Method", "Encoding HP1", "Encoding HP2",
        "Scaling Method", "Scaling HP1", "Scaling HP2",
        "Model Method", "Model HP1", "Model HP2", "Model HP3", "Model HP4"
    ]
    
    @classmethod
    def get_gene_ranges(cls):
        return [
            len(cls.IMPUTATION_MAP),
            max(len(cls.HP_IMPUTATION_KNN_N_NEIGHBORS), len(cls.HP_IMPUTATION_MISSFOREST_N_ESTIMATORS)),
            len(cls.HP_IMPUTATION_MISSFOREST_MAX_ITER),
            
            len(cls.OUTLIER_MAP),
            max(len(cls.HP_OUTLIER_IF_N_ESTIMATORS), len(cls.HP_OUTLIER_IQR_MULTIPLIER)),
            len(cls.HP_OUTLIER_IF_CONTAMINATION),
            
            len(cls.RESAMPLING_MAP),
            max(len(cls.HP_RESAMPLING_ROS_STRATEGY), len(cls.HP_RESAMPLING_SMOTE_K_NEIGHBORS), len(cls.HP_RESAMPLING_ADASYN_N_NEIGHBORS)),
            max(len(cls.HP_RESAMPLING_SMOTE_STRATEGY), len(cls.HP_RESAMPLING_ADASYN_STRATEGY)),
            
            len(cls.ENCODING_MAP),
            max(len(cls.HP_ENCODING_ONEHOT_MAX_CARDINALITY), len(cls.HP_ENCODING_LSA_N_COMPONENTS), len(cls.HP_ENCODING_W2V_DIM)),
            max(len(cls.HP_ENCODING_ONEHOT_DROP), len(cls.HP_ENCODING_LSA_NGRAM_MAX), len(cls.HP_ENCODING_W2V_WINDOW)),
            
            len(cls.SCALING_MAP),
            len(cls.HP_SCALING_STANDARD_WITH_MEAN),
            len(cls.HP_SCALING_STANDARD_WITH_STD),
            
            len(cls.MODEL_MAP),
            max(len(cls.HP_MODEL_LOGREG_C), len(cls.HP_MODEL_RF_N_ESTIMATORS), len(cls.HP_MODEL_GB_N_ESTIMATORS), len(cls.HP_MODEL_NN_LAYERS)),
            max(len(cls.HP_MODEL_LOGREG_PENALTY_SOLVER), len(cls.HP_MODEL_RF_MAX_DEPTH), len(cls.HP_MODEL_GB_LEARNING_RATE), len(cls.HP_MODEL_NN_DROPOUT)),
            max(len(cls.HP_MODEL_LOGREG_CLASS_WEIGHT), len(cls.HP_MODEL_RF_MIN_SAMPLES_SPLIT), len(cls.HP_MODEL_GB_MAX_DEPTH), len(cls.HP_MODEL_NN_LR)),
            max(len(cls.HP_MODEL_LOGREG_MAX_ITER), len(cls.HP_MODEL_RF_MIN_SAMPLES_LEAF), len(cls.HP_MODEL_GB_SUBSAMPLE), len(cls.HP_MODEL_NN_BATCH_SIZE))
        ]


class ChromosomeDecoder:
    
    def __init__(self):
        self.config = ChromosomeConfig()
    
    def decode_chromosome(self, chromosome, verbose=True):
        if len(chromosome) != 20:
            print(f"[ChromosomeDecoder] –û—à–∏–±–∫–∞: —Ö—Ä–æ–º–æ—Å–æ–º–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 20 –≥–µ–Ω–æ–≤, –ø–æ–ª—É—á–µ–Ω–æ {len(chromosome)}")
            return None
        
        if verbose:
            print(f"[ChromosomeDecoder] –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ö—Ä–æ–º–æ—Å–æ–º—ã: {list(chromosome)}")
        
        decoded_info = {
            'chromosome_values': list(chromosome),
            'description': {},
            'pipeline_params': {}
        }
        
        self._decode_imputation(chromosome, decoded_info, verbose)
        self._decode_outlier_removal(chromosome, decoded_info, verbose)
        self._decode_resampling(chromosome, decoded_info, verbose)
        self._decode_encoding(chromosome, decoded_info, verbose)
        self._decode_scaling(chromosome, decoded_info, verbose)
        self._decode_model(chromosome, decoded_info, verbose)
        
        if verbose:
            self._print_summary(decoded_info)
        
        return decoded_info
    
    def _get_hp_value(self, gene_val, hp_map):
        return hp_map.get(gene_val)
    
    def _decode_imputation(self, chromosome, decoded_info, verbose):
        method_idx, hp1_idx, hp2_idx = chromosome[0], chromosome[1], chromosome[2]
        method = self.config.IMPUTATION_MAP.get(method_idx, "unknown")
        
        decoded_info['pipeline_params']['imputation_method'] = method
        
        params = {}
        if method == 'knn':
            params['n_neighbors'] = self._get_hp_value(hp1_idx, self.config.HP_IMPUTATION_KNN_N_NEIGHBORS)
        elif method == 'missforest':
            params['n_estimators'] = self._get_hp_value(hp1_idx, self.config.HP_IMPUTATION_MISSFOREST_N_ESTIMATORS)
            params['max_iter'] = self._get_hp_value(hp2_idx, self.config.HP_IMPUTATION_MISSFOREST_MAX_ITER)
        
        decoded_info['pipeline_params']['imputation_params'] = {k: v for k, v in params.items() if v is not None}
        
        if verbose:
            print(f"  –ò–º–ø—É—Ç–∞—Ü–∏—è: {method}, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {decoded_info['pipeline_params']['imputation_params']}")
    
    def _decode_outlier_removal(self, chromosome, decoded_info, verbose):
        """–î–µ–∫–æ–¥–∏—Ä—É–µ—Ç –≥–µ–Ω—ã —É–¥–∞–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤ (3, 4, 5)"""
        method_idx, hp1_idx, hp2_idx = chromosome[3], chromosome[4], chromosome[5]
        method = self.config.OUTLIER_MAP.get(method_idx, "unknown")
        
        decoded_info['pipeline_params']['outlier_method'] = method
        
        params = {}
        if method == 'isolation_forest':
            params['n_estimators'] = self._get_hp_value(hp1_idx, self.config.HP_OUTLIER_IF_N_ESTIMATORS)
            params['contamination'] = self._get_hp_value(hp2_idx, self.config.HP_OUTLIER_IF_CONTAMINATION)
        elif method == 'iqr':
            params['multiplier'] = self._get_hp_value(hp1_idx, self.config.HP_OUTLIER_IQR_MULTIPLIER)
        
        decoded_info['pipeline_params']['outlier_params'] = {k: v for k, v in params.items() if v is not None}
        
        if verbose:
            print(f"  –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤: {method}, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {decoded_info['pipeline_params']['outlier_params']}")
    
    def _decode_resampling(self, chromosome, decoded_info, verbose):
        """–î–µ–∫–æ–¥–∏—Ä—É–µ—Ç –≥–µ–Ω—ã —Ä–µ—Å–µ–º–ø–ª–∏–Ω–≥–∞ (6, 7, 8)"""
        method_idx, hp1_idx, hp2_idx = chromosome[6], chromosome[7], chromosome[8]
        method = self.config.RESAMPLING_MAP.get(method_idx, "unknown")
        
        decoded_info['pipeline_params']['resampling_method'] = method
        
        params = {}
        if method == 'oversample':
            params['sampling_strategy'] = self._get_hp_value(hp1_idx, self.config.HP_RESAMPLING_ROS_STRATEGY)
        elif method == 'smote':
            params['k_neighbors'] = self._get_hp_value(hp1_idx, self.config.HP_RESAMPLING_SMOTE_K_NEIGHBORS)
            params['sampling_strategy'] = self._get_hp_value(hp2_idx, self.config.HP_RESAMPLING_SMOTE_STRATEGY)
        elif method == 'adasyn':
            params['n_neighbors'] = self._get_hp_value(hp1_idx, self.config.HP_RESAMPLING_ADASYN_N_NEIGHBORS)
            params['sampling_strategy'] = self._get_hp_value(hp2_idx, self.config.HP_RESAMPLING_ADASYN_STRATEGY)
        
        decoded_info['pipeline_params']['resampling_params'] = {k: v for k, v in params.items() if v is not None}
        
        if verbose:
            print(f"  –†–µ—Å–µ–º–ø–ª–∏–Ω–≥: {method}, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {decoded_info['pipeline_params']['resampling_params']}")
    
    def _decode_encoding(self, chromosome, decoded_info, verbose):
        """–î–µ–∫–æ–¥–∏—Ä—É–µ—Ç –≥–µ–Ω—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (9, 10, 11)"""
        method_idx, hp1_idx, hp2_idx = chromosome[9], chromosome[10], chromosome[11]
        method = self.config.ENCODING_MAP.get(method_idx, "unknown")
        
        decoded_info['pipeline_params']['encoding_method'] = method
        
        params = {}
        if method == 'onehot':
            params['max_cardinality_threshold'] = self._get_hp_value(hp1_idx, self.config.HP_ENCODING_ONEHOT_MAX_CARDINALITY)
            params['drop'] = self._get_hp_value(hp2_idx, self.config.HP_ENCODING_ONEHOT_DROP)
        elif method == 'lsa':
            params['n_components'] = self._get_hp_value(hp1_idx, self.config.HP_ENCODING_LSA_N_COMPONENTS)
            ngram_max = self._get_hp_value(hp2_idx, self.config.HP_ENCODING_LSA_NGRAM_MAX)
            if ngram_max is not None:
                params['ngram_range'] = (1, ngram_max)
        elif method == 'word2vec':
            params['embedding_dim'] = self._get_hp_value(hp1_idx, self.config.HP_ENCODING_W2V_DIM)
            params['window'] = self._get_hp_value(hp2_idx, self.config.HP_ENCODING_W2V_WINDOW)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–≤–∫–ª—é—á–∞—è None –¥–ª—è drop –≤ onehot)
        filtered_params = {}
        for k, v in params.items():
            if v is not None or (method == 'onehot' and k == 'drop'):
                filtered_params[k] = v
        
        decoded_info['pipeline_params']['encoding_params'] = filtered_params
        
        if verbose:
            print(f"  –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: {method}, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {decoded_info['pipeline_params']['encoding_params']}")
    
    def _decode_scaling(self, chromosome, decoded_info, verbose):
        """–î–µ–∫–æ–¥–∏—Ä—É–µ—Ç –≥–µ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è (12, 13, 14)"""
        method_idx, hp1_idx, hp2_idx = chromosome[12], chromosome[13], chromosome[14]
        method = self.config.SCALING_MAP.get(method_idx, "unknown")
        
        decoded_info['pipeline_params']['scaling_method'] = method
        
        params = {}
        if method == 'standard':
            params['with_mean'] = self._get_hp_value(hp1_idx, self.config.HP_SCALING_STANDARD_WITH_MEAN)
            params['with_std'] = self._get_hp_value(hp2_idx, self.config.HP_SCALING_STANDARD_WITH_STD)
        
        decoded_info['pipeline_params']['scaling_params'] = {k: v for k, v in params.items() if v is not None}
        
        if verbose:
            print(f"  –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ: {method}, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {decoded_info['pipeline_params']['scaling_params']}")
    
    def _decode_model(self, chromosome, decoded_info, verbose):
        """–î–µ–∫–æ–¥–∏—Ä—É–µ—Ç –≥–µ–Ω—ã –º–æ–¥–µ–ª–∏ (15, 16, 17, 18, 19)"""
        method_idx = chromosome[15]
        hp1_idx, hp2_idx, hp3_idx, hp4_idx = chromosome[16], chromosome[17], chromosome[18], chromosome[19]
        model_type = self.config.MODEL_MAP.get(method_idx, "unknown")
        
        decoded_info['pipeline_params']['model_type'] = model_type
        
        params = {}
        if model_type == 'logistic_regression':
            params['C'] = self._get_hp_value(hp1_idx, self.config.HP_MODEL_LOGREG_C)
            params['solver_penalty_config'] = self._get_hp_value(hp2_idx, self.config.HP_MODEL_LOGREG_PENALTY_SOLVER)
            params['class_weight'] = self._get_hp_value(hp3_idx, self.config.HP_MODEL_LOGREG_CLASS_WEIGHT)
            params['max_iter'] = self._get_hp_value(hp4_idx, self.config.HP_MODEL_LOGREG_MAX_ITER)
        elif model_type == 'random_forest':
            params['n_estimators'] = self._get_hp_value(hp1_idx, self.config.HP_MODEL_RF_N_ESTIMATORS)
            params['max_depth'] = self._get_hp_value(hp2_idx, self.config.HP_MODEL_RF_MAX_DEPTH)
            params['min_samples_split'] = self._get_hp_value(hp3_idx, self.config.HP_MODEL_RF_MIN_SAMPLES_SPLIT)
            params['min_samples_leaf'] = self._get_hp_value(hp4_idx, self.config.HP_MODEL_RF_MIN_SAMPLES_LEAF)
        elif model_type == 'gradient_boosting':
            params['n_estimators'] = self._get_hp_value(hp1_idx, self.config.HP_MODEL_GB_N_ESTIMATORS)
            params['learning_rate'] = self._get_hp_value(hp2_idx, self.config.HP_MODEL_GB_LEARNING_RATE)
            params['max_depth'] = self._get_hp_value(hp3_idx, self.config.HP_MODEL_GB_MAX_DEPTH)
            params['subsample'] = self._get_hp_value(hp4_idx, self.config.HP_MODEL_GB_SUBSAMPLE)
        elif model_type == 'neural_network':
            params['hidden_layer_sizes'] = self._get_hp_value(hp1_idx, self.config.HP_MODEL_NN_LAYERS)
            params['dropout_rate'] = self._get_hp_value(hp2_idx, self.config.HP_MODEL_NN_DROPOUT)
            params['learning_rate'] = self._get_hp_value(hp3_idx, self.config.HP_MODEL_NN_LR)
            params['batch_size'] = self._get_hp_value(hp4_idx, self.config.HP_MODEL_NN_BATCH_SIZE)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –æ—Å—Ç–∞–≤–ª—è—è –≤–∞–∂–Ω—ã–µ None –∑–Ω–∞—á–µ–Ω–∏—è
        final_params = {}
        for k, v in params.items():
            if v is not None:
                final_params[k] = v
            elif k in ['class_weight', 'max_depth'] and v is None:
                final_params[k] = None  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω—ã–µ None –∑–Ω–∞—á–µ–Ω–∏—è
            elif k == 'solver_penalty_config' and isinstance(v, dict):
                final_params[k] = v
        
        decoded_info['pipeline_params']['model_params'] = final_params
        
        if verbose:
            print(f"  –ú–æ–¥–µ–ª—å: {model_type}, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {decoded_info['pipeline_params']['model_params']}")
    
    def _print_summary(self, decoded_info):
        """–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ö—Ä–æ–º–æ—Å–æ–º—ã"""
        params = decoded_info['pipeline_params']
        print(f"\n[ChromosomeDecoder] –†–µ–∑—é–º–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞:")
        print(f"  ‚Ä¢ –ò–º–ø—É—Ç–∞—Ü–∏—è: {params['imputation_method']}")
        print(f"  ‚Ä¢ –í—ã–±—Ä–æ—Å—ã: {params['outlier_method']}")  
        print(f"  ‚Ä¢ –†–µ—Å–µ–º–ø–ª–∏–Ω–≥: {params['resampling_method']}")
        print(f"  ‚Ä¢ –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: {params['encoding_method']}")
        print(f"  ‚Ä¢ –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ: {params['scaling_method']}")
        print(f"  ‚Ä¢ –ú–æ–¥–µ–ª—å: {params['model_type']}")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
_chromosome_decoder = ChromosomeDecoder()

def decode_and_log_chromosome(chromosome, verbose=True):
    """
    –î–µ–∫–æ–¥–∏—Ä—É–µ—Ç —Ö—Ä–æ–º–æ—Å–æ–º—É (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º)
    
    Args:
        chromosome: 20-–≥–µ–Ω–Ω–∞—è —Ö—Ä–æ–º–æ—Å–æ–º–∞
        verbose: –í—ã–≤–æ–¥–∏—Ç—å –¥–µ—Ç–∞–ª–∏
        
    Returns:
        Dict —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞
    """
    result = _chromosome_decoder.decode_chromosome(chromosome, verbose)
    if result is None:
        return None
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ pipeline_params –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ —Å—Ç–∞—Ä—ã–º –∫–æ–¥–æ–º
    return result['pipeline_params']


def decode_chromosome_full(chromosome, verbose=True):
    """
    –î–µ–∫–æ–¥–∏—Ä—É–µ—Ç —Ö—Ä–æ–º–æ—Å–æ–º—É —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π (–¥–ª—è CLI –∏ –Ω–æ–≤—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π)
    
    Args:
        chromosome: 20-–≥–µ–Ω–Ω–∞—è —Ö—Ä–æ–º–æ—Å–æ–º–∞
        verbose: –í—ã–≤–æ–¥–∏—Ç—å –¥–µ—Ç–∞–ª–∏
        
    Returns:
        Dict —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ö—Ä–æ–º–æ—Å–æ–º–µ
    """
    return _chromosome_decoder.decode_chromosome(chromosome, verbose)


# ==========================================
# EXISTING PIPELINE PROCESSING FUNCTIONS  
# ==========================================

def get_dataset_name(path):
    """
    Extract dataset name from file path.
    Example: 'datasets/credit-score-classification/train.csv' -> 'credit-score-classification'
    """
    parts = os.path.normpath(path).split(os.sep)
    if 'datasets' in parts:
        datasets_index = parts.index('datasets')
        if len(parts) > datasets_index + 1:
            return parts[datasets_index + 1]
    return 'unknown_dataset'

def process_data(train_path, test_path, target_column,
                 imputation_method='knn', imputation_params=None,
                 outlier_method='isolation_forest', outlier_params=None,
                 encoding_method='label', encoding_params=None,
                 resampling_method='none', resampling_params=None,
                 scaling_method='none', scaling_params=None,
                 save_processed_data=True,
                 save_model_artifacts=True):
    """
    Process data using specified methods and their hyperparameters.
    
    Args:
        train_path: str, path to training data
        test_path: str, path to test data
        target_column: str, name of target column
        imputation_method: str, e.g., 'knn', 'median', 'missforest'
        imputation_params: dict, HPs for the imputation method
        outlier_method: str, e.g., 'isolation_forest', 'iqr', 'none'
        outlier_params: dict, HPs for the outlier removal method
        encoding_method: str, e.g., 'onehot', 'label', 'lsa', 'word2vec'
        encoding_params: dict, HPs for the encoding method
        resampling_method: str, e.g., 'none', 'oversample', 'smote'
        resampling_params: dict, HPs for the resampling method
        scaling_method: str, e.g., 'none', 'standard', 'minmax'
        scaling_params: dict, HPs for the scaling method
        save_processed_data: bool, whether to save processed data to disk
        save_model_artifacts: bool, whether to create and use research_path for model artifacts (e.g. plots)
        
    Returns:
        tuple: (train_data, test_data, research_path)
               train_data/test_data are DataFrames if save_processed_data is False,
               otherwise they are file paths.
               Returns (None, None, None) if a critical error occurs.
    """
    imputation_params = imputation_params or {}
    outlier_params = outlier_params or {}
    encoding_params = encoding_params or {}
    resampling_params = resampling_params or {}
    scaling_params = scaling_params or {}
    
    dataset_name = get_dataset_name(train_path)
    
    experiment_name_parts = [imputation_method, outlier_method, encoding_method, resampling_method]
    if scaling_method != 'none':
        experiment_name_parts.append(scaling_method)
    experiment_name = "_".join(experiment_name_parts)
    
    results_path = os.path.join('research', dataset_name, experiment_name)
    research_path = os.path.join("research", dataset_name, experiment_name)
    
    if save_processed_data:
        os.makedirs(results_path, exist_ok=True)
    
    if save_model_artifacts:
        os.makedirs(research_path, exist_ok=True)

    output_suffix = f"_processed_{experiment_name}"

    print(f"\n[Pipeline Stage - Config: {experiment_name}] Loading data...")
    loader = DataLoader(train_path=train_path, test_path=test_path)
    train_data, test_data = loader.load_data()

    if train_data is None or train_data.empty:
        print(f"Error: Training data could not be loaded from {train_path} or is empty.")
        return None, None, None

    preprocessor = DataPreprocessor()
    
    print(f"[Pipeline Stage - Config: {experiment_name}] Imputation ({imputation_method}, HPs: {imputation_params})...")
    train_data = preprocessor.impute(train_data, method=imputation_method, **imputation_params)
    if test_data is not None and not test_data.empty:
        test_data = preprocessor.impute(test_data, method=imputation_method, **imputation_params)
    print(f"[Pipeline Stage - Config: {experiment_name}] Imputation completed.")

    potential_id_columns_process = [col for col in ['ID', 'id'] if col in train_data.columns and col != target_column]
    if potential_id_columns_process:
        print(f"Dropping ID column(s) from train_data: {potential_id_columns_process} before outlier removal.")
        train_data = train_data.drop(columns=potential_id_columns_process)
        if test_data is not None and not test_data.empty:
            test_data = test_data.drop(columns=potential_id_columns_process, errors='ignore')

    print(f"[Pipeline Stage - Config: {experiment_name}] Outlier removal ({outlier_method}, HPs: {outlier_params})...")
    if outlier_method != 'none':
        train_data_before_outliers = train_data.copy()
        try:
            remover = OutlierRemover(method=outlier_method, **outlier_params) 
            features_for_removal = train_data.drop(columns=[target_column], errors='ignore')
            
            target_series_for_reconstruction = None
            if target_column in train_data.columns:
                target_series_for_reconstruction = train_data[target_column]

            cleaned_features = remover.remove_outliers(features_for_removal)
            
            if cleaned_features.empty and not features_for_removal.empty :
                print(f"Warning: Outlier removal (method: {outlier_method}) resulted in an empty dataset. Reverting.")
                train_data = train_data_before_outliers
            elif target_series_for_reconstruction is not None:
                cleaned_features_idx = cleaned_features.index
                target_series_aligned = target_series_for_reconstruction.loc[target_series_for_reconstruction.index.intersection(cleaned_features_idx)]
                cleaned_features_aligned = cleaned_features.loc[cleaned_features.index.intersection(target_series_aligned.index)]
                
                train_data = pd.concat([cleaned_features_aligned, target_series_aligned.rename(target_column)], axis=1)

            else:
                train_data = cleaned_features
        except Exception as e:
            print(f"Could not remove outliers (method: {outlier_method}): {e}. Reverting.")
            train_data = train_data_before_outliers
    else:
        print("Outlier removal skipped as method is 'none'.")
    print(f"[Pipeline Stage - Config: {experiment_name}] Outlier removal completed.")

    # --- Diagnostic: Check for duplicate columns before encoding ---
    duplicated_cols_before_encoding = train_data.columns[train_data.columns.duplicated()].tolist()
    if duplicated_cols_before_encoding:
        print(f"WARNING: Duplicate columns found in train_data BEFORE encoding: {duplicated_cols_before_encoding}")
        train_data = train_data.loc[:, ~train_data.columns.duplicated(keep='first')]
        print(f"         Duplicates removed. Columns now: {train_data.columns.tolist()}")
    if test_data is not None and not test_data.empty:
        duplicated_cols_test_before_encoding = test_data.columns[test_data.columns.duplicated()].tolist()
        if duplicated_cols_test_before_encoding:
            print(f"WARNING: Duplicate columns found in test_data BEFORE encoding: {duplicated_cols_test_before_encoding}")
            test_data = test_data.loc[:, ~test_data.columns.duplicated(keep='first')]
            print(f"         Duplicates removed from test_data. Columns now: {test_data.columns.tolist()}")
    # --- End Diagnostic ---

    print(f"[Pipeline Stage - Config: {experiment_name}] Encoding ({encoding_method}, HPs: {encoding_params})...")

    train_data = preprocessor.encode(train_data, method=encoding_method, target_col=target_column, **encoding_params)
    if test_data is not None and not test_data.empty:
        test_data = preprocessor.encode(test_data, method=encoding_method, target_col=target_column, **encoding_params)
    print(f"[Pipeline Stage - Config: {experiment_name}] Encoding completed.")
    
    print(f"[Pipeline Stage - Config: {experiment_name}] Resampling ({resampling_method}, HPs: {resampling_params})...")
    if resampling_method != 'none':
        if target_column in train_data.columns:
            if not pd.api.types.is_numeric_dtype(train_data[target_column]):
                print(f"Warning: Target column '{target_column}' is not numeric. Applying LabelEncoding before resampling.")
                train_data = preprocessor._label_encode_target(train_data, target_column)
            
            X_train_encoded = train_data.drop(columns=[target_column])
            y_train_encoded = train_data[target_column]
            
            resampler = Resampler(method=resampling_method, **resampling_params)
            X_train_resampled, y_train_resampled = resampler.fit_resample(X_train_encoded, y_train_encoded)
            
            train_data = pd.concat([pd.DataFrame(X_train_resampled, columns=X_train_encoded.columns), 
                                    pd.Series(y_train_resampled, name=target_column)], axis=1)
        else:
            print(f"Warning: Target column '{target_column}' not found. Skipping resampling.")
    else:
        print("Resampling method is 'none'. Skipping resampling step.")
    print(f"[Pipeline Stage - Config: {experiment_name}] Resampling completed.")

    print(f"[Pipeline Stage - Config: {experiment_name}] Scaling ({scaling_method}, HPs: {scaling_params})...")
    scaler_instance = None  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é —Å–∫–µ–π–ª–µ—Ä–∞ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    if scaling_method != 'none' and scaling_method in ['standard', 'minmax']:
        if target_column not in train_data.columns:
            print(f"CRITICAL WARNING: Target column '{target_column}' not found in train_data before scaling. Skipping scaling.")
        else:
            X_train_features = train_data.drop(columns=[target_column])
            y_train_target = train_data[target_column]

            if scaling_method == 'standard':
                scaler_instance = StandardScaler(**scaling_params)
            elif scaling_method == 'minmax':
                scaler_instance = MinMaxScaler(**scaling_params)

            if scaler_instance:
                try:
                    scaled_X_train_values = scaler_instance.fit_transform(X_train_features)
                    scaled_X_train_df = pd.DataFrame(scaled_X_train_values, columns=X_train_features.columns, index=X_train_features.index)
                    train_data = pd.concat([scaled_X_train_df, y_train_target], axis=1)

                    if test_data is not None and not test_data.empty:
                        X_test_features_original = test_data.drop(columns=[target_column], errors='ignore')
                        y_test_target_original = test_data[target_column] if target_column in test_data.columns else None
                        
                        cols_to_scale_in_test = [col for col in X_train_features.columns if col in X_test_features_original.columns]
                        
                        if not cols_to_scale_in_test:
                             print("Warning: No common features to scale in test data. Test data unscaled.")
                        elif len(cols_to_scale_in_test) != len(X_train_features.columns):
                            print("Warning: Test data feature set mismatch for scaling. Scaling subset.")
                            X_test_subset_to_scale = X_test_features_original[cols_to_scale_in_test]
                            scaled_X_test_subset_values = scaler_instance.transform(X_test_subset_to_scale)
                            scaled_X_test_subset_df = pd.DataFrame(scaled_X_test_subset_values, columns=cols_to_scale_in_test, index=X_test_subset_to_scale.index)
                            
                            X_test_features_updated = X_test_features_original.copy()
                            for col in cols_to_scale_in_test:
                                X_test_features_updated[col] = scaled_X_test_subset_df[col]

                            if y_test_target_original is not None:
                                test_data = pd.concat([X_test_features_updated, y_test_target_original], axis=1)
                            else:
                                test_data = X_test_features_updated
                        else: 
                            scaled_X_test_values = scaler_instance.transform(X_test_features_original[X_train_features.columns])
                            scaled_X_test_df = pd.DataFrame(scaled_X_test_values, columns=X_train_features.columns, index=X_test_features_original.index)

                            if y_test_target_original is not None:
                                test_data = pd.concat([scaled_X_test_df, y_test_target_original], axis=1)
                            else:
                                test_data = scaled_X_test_df
                except Exception as e:
                    print(f"Error during scaling: {e}. Skipping scaling.")
    elif scaling_method != 'none':
        print(f"Warning: Unknown scaling method '{scaling_method}'. Scaling skipped.")
    print(f"[Pipeline Stage - Config: {experiment_name}] Scaling completed.")
    
    # –°–æ–±–∏—Ä–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤—Å–µ—Ö –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞
    preprocessor_states = {
        'preprocessor': preprocessor.get_preprocessor_state(),
        'scaler': scaler_instance,
        'scaler_method': scaling_method,
        'dropped_columns': potential_id_columns_process,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
        'processing_config': {
            'imputation_method': imputation_method,
            'imputation_params': imputation_params,
            'outlier_method': outlier_method,
            'outlier_params': outlier_params,
            'encoding_method': encoding_method,
            'encoding_params': encoding_params,
            'resampling_method': resampling_method,
            'resampling_params': resampling_params,
            'scaling_method': scaling_method,
            'scaling_params': scaling_params
        }
    }
    
    train_data_path_out = None
    test_data_path_out = None

    if save_processed_data:
        train_data_path_out = os.path.join(results_path, f'train{output_suffix}.csv')
        test_data_path_out = os.path.join(results_path, f'test{output_suffix}.csv')
        
        train_data.to_csv(train_data_path_out, index=False)
        if test_data is not None and not test_data.empty:
            test_data.to_csv(test_data_path_out, index=False)
        else:
            test_data_path_out = None 
        print(f"[Pipeline Stage - Config: {experiment_name}] Data processing complete. Processed files saved to: {results_path}")
        return train_data_path_out, test_data_path_out, research_path, preprocessor_states # Return paths + states
    else:
        print(f"[Pipeline Stage - Config: {experiment_name}] Data processing complete. Processed data NOT saved (GA mode).")
        return train_data, test_data, research_path, preprocessor_states # Return DataFrames + states

def train_model(train_data_input, test_data_input, target_column, research_path, 
                model_type='random_forest', model_hyperparameters=None, 
                plot_learning_curves=True, save_run_results=True):
    """
    Trains a model using the specified data and parameters.
    Handles data splitting if test data is missing a target.
    """
    # print(f"\n--- Training Model: {model_type} ---")
    # print(f"Target column: {target_column}")
    # print(f"Research path for this run: {research_path}")
    if model_hyperparameters:
        # print(f"Model hyperparameters received: {model_hyperparameters}")
        pass

    if (save_run_results or plot_learning_curves) and research_path:
        os.makedirs(research_path, exist_ok=True)
    
    # Load data if paths are provided
    current_train_data = train_data_input
    if isinstance(train_data_input, str):
        try:
            current_train_data = pd.read_csv(train_data_input)
            # print(f"Loaded training data from path: {train_data_input}")
        except Exception as e:
            print(f"Error loading training data from path {train_data_input}: {e}")
            return None, None, None # Cannot proceed without training data
    
    current_test_data = test_data_input
    if isinstance(test_data_input, str):
        try:
            current_test_data = pd.read_csv(test_data_input)
            # print(f"Loaded test data from path: {test_data_input}")
        except FileNotFoundError:
            # print(f"Test data file not found at {test_data_input}. Proceeding without test data (will use validation split from train).")
            current_test_data = pd.DataFrame() # Ensure it's an empty DF, not None
        except Exception as e:
            print(f"Error loading test data from path {test_data_input}: {e}")
            current_test_data = pd.DataFrame() 
    elif current_test_data is None: # If None was passed directly (not a path)
        current_test_data = pd.DataFrame()


    # Create ModelTrainer instance
    trainer = ModelTrainer(model_type=model_type, model_hyperparameters=model_hyperparameters, random_state=42)
    
    metrics, feature_importance, trainer_dropped_cols = trainer.train(
        current_train_data, 
        current_test_data, 
        target_column,
        output_path=research_path, 
        plot_learning_curves=plot_learning_curves,
        save_run_results=save_run_results # Pass the flag through
    )    
    return metrics, feature_importance, trainer_dropped_cols


========== FILE: src\preprocessing\__init__.py ==========

"""
Preprocessing package for data preparation pipeline.
Includes data loading, preprocessing, and outlier removal functionality.
""" 


========== FILE: src\preprocessing\data_loader.py ==========

import pandas as pd
from sklearn.model_selection import train_test_split

class DataLoader:
    """
    Class for loading data.
    If train and test paths are provided, they are loaded separately.
    If only one file is provided, it's automatically split into train and test sets.
    """
    def __init__(self, train_path=None, test_path=None, test_size=0.2, random_state=42):
        """
        Initialize DataLoader.
        
        Args:
            train_path: str, path to training data or full dataset
            test_path: str or None, path to test data
            test_size: float, proportion of data to use for testing when splitting
            random_state: int, random state for reproducibility
        """
        self.train_path = train_path
        self.test_path = test_path
        self.test_size = test_size
        self.random_state = random_state
        self.train_data = None
        self.test_data = None

    def load_data(self):
        """
        Load data from specified paths.
        If only train_path is provided, automatically split into train and test sets.
        
        Returns:
            tuple: (train_data, test_data) - pandas DataFrames
        """
        if self.train_path and self.test_path:
            self.train_data = pd.read_csv(self.train_path)
            self.test_data = pd.read_csv(self.test_path)
            print(f"Loaded train dataset: {self.train_path}")
            print(f"Loaded test dataset: {self.test_path}")
        elif self.train_path:
            full_data = pd.read_csv(self.train_path)
            print(f"Loaded full dataset: {self.train_path}")
            print(f"Splitting into train ({1-self.test_size:.0%}) and test ({self.test_size:.0%}) sets...")
            
            self.train_data, self.test_data = train_test_split(
                full_data,
                test_size=self.test_size,
                random_state=self.random_state,
                shuffle=True
            )
            print(f"Split complete. Train size: {len(self.train_data)}, Test size: {len(self.test_data)}")
        else:
            raise ValueError("No data paths provided for loading.")
            
        return self.train_data, self.test_data 


========== FILE: src\preprocessing\data_preprocessor.py ==========

import pandas as pd
import numpy as np
import sys
import os
from sklearn.impute import KNNImputer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from gensim.models import Word2Vec
from sklearn.ensemble import RandomForestRegressor
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from category_encoders import LeaveOneOutEncoder


GENSIM_AVAILABLE = False
MISSFOREST_AVAILABLE = False
try:
    import gensim
    from gensim.models import FastText
    GENSIM_AVAILABLE = True
except ImportError as e:
    pass

class DataPreprocessor:
    def __init__(self):
        self.encoders = {}
        self.cat_columns = []
        self.numeric_columns = []
        self.target_encoder_le = None
        self.lsa_vectorizer = None
        self.lsa_svd = None
        self.lsa_feature_names = None
        self.medians = {}
        self.lsa_components = {}
        self.word2vec_models = {}
        self.word2vec_dims = 50
        self.imputed_medians = {}
        self.imputed_modes = {}
        
    def get_preprocessor_state(self):
        return {
            'encoders': self.encoders,
            'cat_columns': self.cat_columns,
            'numeric_columns': self.numeric_columns,
            'target_encoder_le': self.target_encoder_le,
            'lsa_vectorizer': self.lsa_vectorizer,
            'lsa_svd': self.lsa_svd,
            'lsa_feature_names': self.lsa_feature_names,
            'medians': self.medians,
            'lsa_components': self.lsa_components,
            'word2vec_models': self.word2vec_models,
            'word2vec_dims': self.word2vec_dims,
            'imputed_medians': self.imputed_medians,
            'imputed_modes': self.imputed_modes
        }
    
    def set_preprocessor_state(self, state):
        self.encoders = state.get('encoders', {})
        self.cat_columns = state.get('cat_columns', [])
        self.numeric_columns = state.get('numeric_columns', [])
        self.target_encoder_le = state.get('target_encoder_le', None)
        self.lsa_vectorizer = state.get('lsa_vectorizer', None)
        self.lsa_svd = state.get('lsa_svd', None)
        self.lsa_feature_names = state.get('lsa_feature_names', None)
        self.medians = state.get('medians', {})
        self.lsa_components = state.get('lsa_components', {})
        self.word2vec_models = state.get('word2vec_models', {})
        self.word2vec_dims = state.get('word2vec_dims', 50)
        self.imputed_medians = state.get('imputed_medians', {})
        self.imputed_modes = state.get('imputed_modes', {})

    def _get_column_types(self, data):
        self.numeric_columns = data.select_dtypes(include=np.number).columns.tolist()
        self.cat_columns = data.select_dtypes(include=['object', 'category', 'string']).columns.tolist()

    def _impute_median(self, data):
        data = data.copy()
        numeric_cols_present = [col for col in self.numeric_columns if col in data.columns]

        if not self.medians:
            for col in numeric_cols_present:
                median_val = data[col].median()
                if pd.isna(median_val):
                    median_val = 0
                self.medians[col] = median_val
                data[col] = data[col].fillna(median_val)
        else:
            for col in numeric_cols_present:
                stored_median = self.medians.get(col)
                if stored_median is not None:
                    data[col] = data[col].fillna(stored_median)
                else:
                    fallback_median = data[col].median()
                    fallback_median = fallback_median if pd.notna(fallback_median) else 0
                    data[col] = data[col].fillna(fallback_median)
        return data

    def impute_knn(self, data, n_neighbors=5):
        numeric_cols = data.select_dtypes(include=["number"]).columns
        if not numeric_cols.empty:
            imputer = KNNImputer(n_neighbors=n_neighbors)
            data_numeric = data[numeric_cols]
            data_imputed = pd.DataFrame(
                imputer.fit_transform(data_numeric),
                columns=numeric_cols,
                index=data.index
            )
            data.loc[:, numeric_cols] = data_imputed
        return data

    def impute_categorical(self, data):
        cat_cols_present = [col for col in self.cat_columns if col in data.columns]
        for col in cat_cols_present:
            data[col] = data[col].astype(str)
            modes = data[col].mode()
            if not modes.empty:
                data[col] = data[col].replace({"nan": np.nan, "None": np.nan, "": np.nan})
                mode_val = modes[0]
                data[col] = data[col].fillna(mode_val)
                self.imputed_modes[col] = mode_val
            else:
                data[col] = data[col].fillna("Unknown")
                self.imputed_modes[col] = "Unknown"
        return data

    def impute_missforest(self, data, max_iter=10, n_estimators=100, random_state=42):
        original_columns = data.columns
        if self.cat_columns:
            categorical_columns = [col for col in self.cat_columns if col in data.columns]
        else:
            categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()

        data_encoded = data.copy()
        temp_label_encoders = {}

        for col in categorical_columns:
            le = LabelEncoder()
            non_missing = data_encoded[col].dropna().astype(str)
            if not non_missing.empty:
                le.fit(non_missing)
                temp_label_encoders[col] = le
                mask = data_encoded[col].notna()
                data_encoded.loc[mask, col] = le.transform(data_encoded.loc[mask, col].astype(str))
                data_encoded[col] = pd.to_numeric(data_encoded[col], errors='coerce')
            else:
                data_encoded[col] = pd.to_numeric(data_encoded[col], errors='coerce')

        if not MISSFOREST_AVAILABLE:
            imputer = IterativeImputer(
                estimator=RandomForestRegressor(n_estimators=n_estimators, random_state=random_state),
                max_iter=max_iter, random_state=random_state
            )
        else:
            imputer = MissForest(max_iter=max_iter, n_estimators=n_estimators, random_state=random_state)

        numeric_cols_for_impute = data_encoded.select_dtypes(include=np.number).columns
        imputed_array = imputer.fit_transform(data_encoded[numeric_cols_for_impute])
        imputed_df_numeric = pd.DataFrame(imputed_array, columns=numeric_cols_for_impute, index=data.index)

        imputed_df = data.copy()
        imputed_df[numeric_cols_for_impute] = imputed_df_numeric

        for col in categorical_columns:
            if col in temp_label_encoders:
                le = temp_label_encoders[col]
                min_label, max_label = 0, len(le.classes_) - 1
                if col in imputed_df.columns:
                    imputed_int = imputed_df[col].fillna(-1).round().astype(int)
                    imputed_clipped = np.clip(imputed_int, min_label, max_label)
                    valid_mask = imputed_clipped >= 0
                    imputed_df.loc[valid_mask, col] = le.inverse_transform(imputed_clipped[valid_mask])

        return imputed_df

    def _label_encode_target(self, data, target_col):
        """Encode the target column specifically using LabelEncoder."""
        if target_col not in data.columns:
            return data

        if self.target_encoder_le is None:
            self.target_encoder_le = LabelEncoder()
            self.target_encoder_le.fit(data[target_col].astype(str))

        data[target_col] = self.target_encoder_le.transform(data[target_col].astype(str))
        self.encoders[target_col] = self.target_encoder_le
        # print(f"Target column '{target_col}' was label encoded.")
        return data

    def _get_active_cat_cols(self, data, target_col, method):
        """Get list of categorical columns for current encoding method, excluding target unless method is 'target'."""
        if not self.cat_columns:
             return []

        cols = [col for col in self.cat_columns if col != target_col and col in data.columns]
        return cols

    def _onehot_encode(self, data, cols_to_encode, max_cardinality_threshold=50, drop=None):
        """Encode specified columns using one-hot encoding with cardinality check and drop strategy."""
        if not cols_to_encode:
            return data

        data = data.copy()
        encoder_key = 'onehot'
        
        low_cardinality_cols = []
        skipped_cols = []
        for col in cols_to_encode:
            if data[col].nunique(dropna=False) <= max_cardinality_threshold:
                low_cardinality_cols.append(col)
            else:
                skipped_cols.append(col)
        
        if not low_cardinality_cols:
            if skipped_cols:
                print(f"Warning: OneHotEncoding skipped for all specified columns due to high cardinality (> {max_cardinality_threshold}): {skipped_cols}.")
            return data
        
        if skipped_cols:
            print(f"Warning: OneHotEncoding skipped for high cardinality columns (> {max_cardinality_threshold}): {skipped_cols}. Applied to: {low_cardinality_cols}")

        if encoder_key not in self.encoders:
            self.encoders[encoder_key] = OneHotEncoder(
                handle_unknown='ignore',
                sparse_output=False,
                drop=drop # Use passed drop strategy
            )
            self.encoders[encoder_key].fit(data[low_cardinality_cols].astype(str))

        encoder = self.encoders[encoder_key]
        encoded = encoder.transform(data[low_cardinality_cols].astype(str))
        new_cols = encoder.get_feature_names_out(low_cardinality_cols)
        encoded_df = pd.DataFrame(encoded, columns=new_cols, index=data.index)
        
        data_remaining = data.drop(columns=low_cardinality_cols)
        return pd.concat([data_remaining, encoded_df], axis=1)

    def _label_encode(self, data, cols_to_encode):
        """Encode specified columns using label encoding."""
        if not cols_to_encode:
            return data

        data = data.copy()
        for col in cols_to_encode:
            col_key = f'label_{col}'
            if col_key not in self.encoders:
                le = LabelEncoder()
                le.fit(data[col].astype(str))
                self.encoders[col_key] = le
            else:
                le = self.encoders[col_key]

            data[col] = data[col].astype(str)
            mask = data[col].isin(le.classes_)
            encoded_values = np.full(len(data), -1)
            encoded_values[mask] = le.transform(data.loc[mask, col])
            data[col] = encoded_values.astype('int32')
        return data

    def _ordinal_encode(self, data, cols_to_encode):
        """Encode specified columns using ordinal encoding."""
        if not cols_to_encode:
             return data

        data = data.copy()
        encoder_key = 'ordinal'
        if encoder_key not in self.encoders:
            self.encoders[encoder_key] = OrdinalEncoder(
                handle_unknown='use_encoded_value',
                unknown_value=-1
            )
            self.encoders[encoder_key].fit(data[cols_to_encode].astype(str))

        data[cols_to_encode] = self.encoders[encoder_key].transform(data[cols_to_encode].astype(str))
        data[cols_to_encode] = data[cols_to_encode].astype('int32')
        return data

    def _categorical_to_texts(self, data, cols):
        """Transform categorical data rows into texts."""
        data = data.copy()
        for col in cols:
            data[col] = data[col].astype(str)
        new_data_list = []
        for i, row in data[cols].iterrows():
            new_line = ' '.join([f'{col}_{val}' for col, val in row.items()])
            new_data_list.append(new_line)
        return new_data_list

    def _lsa_encode(self, data, cols_to_encode, n_components=10, ngram_range=(1,1)):
        """Encode specified columns using LSA with n_components and ngram_range."""
        if not cols_to_encode:
            return data
        data = data.copy()
        if self.lsa_vectorizer is None: 
            texts = self._categorical_to_texts(data, cols_to_encode)
            self.lsa_vectorizer = TfidfVectorizer(ngram_range=ngram_range, min_df=5, max_df=0.5) 
            term_doc_matrix = self.lsa_vectorizer.fit_transform(texts)
            self.lsa_svd = TruncatedSVD(n_components=n_components, random_state=42) 
            lsa_embeddings = self.lsa_svd.fit_transform(term_doc_matrix)
            self.lsa_feature_names = [f'LSA_{i+1}' for i in range(n_components)]
        else: 
            texts = self._categorical_to_texts(data, cols_to_encode)
            term_doc_matrix = self.lsa_vectorizer.transform(texts)
            lsa_embeddings = self.lsa_svd.transform(term_doc_matrix)

        lsa_df = pd.DataFrame(lsa_embeddings, columns=self.lsa_feature_names, index=data.index)

        cols_to_keep = [col for col in data.columns if col not in cols_to_encode]
        result_df = pd.concat([data[cols_to_keep], lsa_df], axis=1)
        return result_df

    def _embedding_encode(self, data, cols_to_encode, n_components=100, embedding_method='word2vec'):
        """
        Encode specified columns using different embedding methods
        
        Parameters:
        -----------
        data : pandas DataFrame
            The input data to encode
        cols_to_encode : list
            List of categorical columns to encode
        n_components : int
            Dimension of embeddings or final dimension after reduction
        embedding_method : str
            Method to use: 'word2vec' or 'fasttext'
        
        Returns:
        --------
        DataFrame with embedded representations replacing categorical columns
        """
        if not cols_to_encode:
            return data
            
        if not GENSIM_AVAILABLE:
            raise ImportError("Gensim library is required for Word2Vec and FastText embeddings")
            
        data = data.copy()
        texts = self._categorical_to_texts(data, cols_to_encode)
        
        tokenized_texts = [text.split() for text in texts]
        
        embed_key = f'{embedding_method}_embedder'
        feature_key = f'{embedding_method}_feature_names'
        
        if embedding_method in ['word2vec', 'fasttext']:
            if embed_key not in self.encoders:
                if embedding_method == 'word2vec':
                    model = Word2Vec(sentences=tokenized_texts, vector_size=n_components, 
                                     window=5, min_count=1, workers=4)
                else:  # fasttext
                    model = gensim.models.FastText(sentences=tokenized_texts, vector_size=n_components,
                                    window=5, min_count=1, workers=4)
                
                self.encoders[embed_key] = model
            else:
                model = self.encoders[embed_key]
                
            doc_vectors = []
            for tokens in tokenized_texts:
                token_vecs = []
                for token in tokens:
                    if token in model.wv:
                        token_vecs.append(model.wv[token])
                if token_vecs:
                    doc_vectors.append(np.mean(token_vecs, axis=0))
                else:
                    doc_vectors.append(np.zeros(n_components))
                    
            embeddings = np.array(doc_vectors)
            embed_feature_names = [f'{embedding_method}_{i+1}' for i in range(n_components)]
        else:
            raise ValueError(f"Unknown embedding method: {embedding_method}. Use 'word2vec' or 'fasttext'.")
            
        setattr(self, feature_key, embed_feature_names)
        
        embed_df = pd.DataFrame(embeddings, columns=embed_feature_names, index=data.index)
        
        cols_to_keep = [col for col in data.columns if col not in cols_to_encode]
        result_df = pd.concat([data[cols_to_keep], embed_df], axis=1)
        
        return result_df

    def _impute_categorical_with_mode(self, data, column):
        mode_val = data[column].mode()
        if not mode_val.empty:
            mode = mode_val[0]
            data[column] = data[column].fillna(mode)
            self.imputed_modes[column] = mode
            # print(f"Imputed categorical column '{column}' with mode '{mode}'.")
        else:
            fallback_mode = "__MISSING__"
            data[column] = data[column].fillna(fallback_mode)
            self.imputed_modes[column] = fallback_mode
            # print(f"Imputed categorical column '{column}' with fallback '{fallback_mode}'.")
        return data

    def _impute_numerical_with_median(self, data, column):
        median = data[column].median()
        data[column] = data[column].fillna(median)
        self.imputed_medians[column] = median
        # print(f"Imputed numerical column '{column}' with median '{median}'.")
        return data

    def impute(self, data, method='knn', **kwargs):
        # print(f"Starting imputation with method: {method}")
        data_imputed = data.copy()
        numeric_cols = data_imputed.select_dtypes(include=np.number).columns.tolist()
        categorical_cols = data_imputed.select_dtypes(exclude=np.number).columns.tolist()

        # Handle categorical columns first with mode imputation (simple and common for non-targetted methods)
        for col in categorical_cols:
            if data_imputed[col].isnull().any():
                data_imputed = self._impute_categorical_with_mode(data_imputed, col)
        
        numeric_cols_with_na = [col for col in numeric_cols if data_imputed[col].isnull().any()]
        
        if not numeric_cols_with_na:
            # print("No numerical NAs to impute.")
            pass # No numerical NAs to impute
        elif method == 'median':
            # print("Imputing numerical columns with median.")
            for col in numeric_cols_with_na:
                data_imputed = self._impute_numerical_with_median(data_imputed, col)
        elif method == 'knn':
            # print(f"Imputing numerical columns with KNN. Params: {kwargs}")
            imputer_knn = KNNImputer(**kwargs) # Pass all relevant HPs for KNN (e.g., n_neighbors)
            data_imputed[numeric_cols_with_na] = imputer_knn.fit_transform(data_imputed[numeric_cols_with_na])
        elif method == 'missforest': # This now uses IterativeImputer
            print(f"Using IterativeImputer with RandomForestRegressor for 'missforest' method. Params: {kwargs}")

            iterative_imputer_hps = {}
            rf_regressor_hps = {}

            if 'max_iter' in kwargs: iterative_imputer_hps['max_iter'] = kwargs['max_iter']

            if 'n_estimators' in kwargs: rf_regressor_hps['n_estimators'] = kwargs['n_estimators']
            if 'max_features' in kwargs: rf_regressor_hps['max_features'] = kwargs['max_features'] # string or int
            if 'min_samples_split' in kwargs: rf_regressor_hps['min_samples_split'] = kwargs['min_samples_split'] # int or float
            if 'min_samples_leaf' in kwargs: rf_regressor_hps['min_samples_leaf'] = kwargs['min_samples_leaf'] # int or float
            
            iterative_imputer_hps.setdefault('random_state', 42)
            rf_regressor_hps.setdefault('random_state', 42)
            rf_regressor_hps.setdefault('n_jobs', -1)

            if rf_regressor_hps.get('n_estimators', 0) <= 0:
                rf_regressor_hps['n_estimators'] = 10 # Default to 10 estimators for RF in IterativeImputer

            try:

                iter_imputer = IterativeImputer(
                    estimator=RandomForestRegressor(**rf_regressor_hps),
                    **iterative_imputer_hps
                )
                
                if numeric_cols_with_na:
                    data_to_impute_subset = data_imputed[numeric_cols_with_na].copy()
                    imputed_values = iter_imputer.fit_transform(data_to_impute_subset)
                    data_imputed[numeric_cols_with_na] = imputed_values

            except Exception as e_iter_imp:
                print(f"Error during IterativeImputer (for 'missforest' method) processing: {e_iter_imp}")
                print("Falling back to median imputation for numeric columns due to IterativeImputer error.")
                for col in numeric_cols_with_na:
                    data_imputed = self._impute_numerical_with_median(data_imputed, col)

        elif method not in ['median', 'knn', 'missforest']:
            raise ValueError(f"Unknown imputation method: {method}")
        
        # Final check for NAs after all imputation attempts
        if data_imputed.isnull().any().any():
            print("Warning: NAs still present after chosen imputation method and fallbacks.")
            for col in data_imputed.columns:
                if data_imputed[col].isnull().any():
                    if pd.api.types.is_numeric_dtype(data_imputed[col]):
                        data_imputed[col].fillna(0, inplace=True) # Fill numeric with 0
                    else:
                        data_imputed[col].fillna('__UNKNOWN_FINAL__', inplace=True) # Fill non-numeric
        return data_imputed

    def encode(self, data, method='label', target_col=None, **kwargs):
        """
        Encode categorical features. HPs passed via **kwargs.
        'onehot' HPs: max_cardinality_threshold, drop
        'lsa' HPs: n_components, ngram_range (tuple, e.g., (1,2))
        'word2vec'/'embedding' HPs: embedding_dim, window (used by word2vec blocks)
        """
        encoded_data = data.copy()

        if not self.cat_columns and not self.numeric_columns:
            self._get_column_types(encoded_data)

        active_cat_cols = self._get_active_cat_cols(encoded_data, target_col, method)
        if not active_cat_cols and method not in ['target', 'leaveoneout']:
             return encoded_data

        if method == 'onehot':
            # Extract OneHot specific HPs from kwargs, provide defaults if not present
            onehot_hps = {
                'max_cardinality_threshold': kwargs.get('max_cardinality_threshold', 50),
                'drop': kwargs.get('drop', None)
            }
            encoded_data = self._onehot_encode(encoded_data, active_cat_cols, **onehot_hps)
        elif method == 'label':
            encoded_data = self._label_encode(encoded_data, active_cat_cols)
        elif method == 'ordinal':
             encoded_data = self._ordinal_encode(encoded_data, active_cat_cols)
        elif method == 'lsa':
            # Extract LSA specific HPs
            lsa_hps = {
                'n_components': kwargs.get('n_components', 10),
                'ngram_range': kwargs.get('ngram_range', (1,1))
            }
            encoded_data = self._lsa_encode(encoded_data, active_cat_cols, **lsa_hps)
        elif method == 'embedding' or method == 'word2vec': # Consolidate word2vec logic
            if not GENSIM_AVAILABLE:
                print(f"Word2Vec/Embedding encoding ('{method}') skipped: Gensim library not available.")
                return encoded_data

            # Extract Word2Vec HPs
            self.word2vec_dims = kwargs.get('embedding_dim', self.word2vec_dims) # Fallback to class default if not in kwargs
            w2v_window = kwargs.get('window', 1) # Default window to 1 if not specified, as per prior logic
            
            if self.word2vec_dims <= 0:
                print(f"Word2Vec/Embedding encoding ('{method}') skipped: embedding_dim must be > 0.")
                return encoded_data
            
            if not active_cat_cols:
                return encoded_data

            print(f"Applying Word2Vec/Embedding ('{method}') with embedding_dim={self.word2vec_dims}, window={w2v_window} for columns: {active_cat_cols}")
            
            processed_cols_w2v = []
            for col in active_cat_cols:
                sentences = encoded_data[col].astype(str).fillna('__NULL_W2V__').apply(lambda x: [x]).tolist()
                
                if not sentences:
                    print(f"Skipping Word2Vec for column '{col}' due to no data/sentences after processing.")
                    continue
                try:
                    w2v_model = Word2Vec(sentences=sentences, vector_size=self.word2vec_dims, 
                                         window=w2v_window, # Use HP
                                         min_count=1, workers=4, sg=1, seed=42)
                    self.word2vec_models[col] = w2v_model
                    embedding_vectors = []
                    for val_list in sentences:
                        word = val_list[0]
                        if word in w2v_model.wv:
                            embedding_vectors.append(w2v_model.wv[word])
                        else:
                            embedding_vectors.append(np.zeros(self.word2vec_dims))
                    
                    embedding_df = pd.DataFrame(embedding_vectors, index=encoded_data.index)
                    embedding_df.columns = [f'{col}_w2v_{i}' for i in range(self.word2vec_dims)]
                    encoded_data = pd.concat([encoded_data, embedding_df], axis=1)
                    processed_cols_w2v.append(col)
                except Exception as e_w2v:
                    print(f"Error training or applying Word2Vec for column '{col}': {e_w2v}. Column will be skipped.")
            
            if processed_cols_w2v:
                encoded_data = encoded_data.drop(columns=processed_cols_w2v)
                print(f"Dropped original Word2Vec processed columns: {processed_cols_w2v}")
        elif method == 'leaveoneout':
            if target_col is None or target_col not in data.columns:
                raise ValueError("LeaveOneOutEncoder requires a target column specified and present in data.")
            
            temp_target_series = encoded_data[target_col]
            if not pd.api.types.is_numeric_dtype(temp_target_series):
                print(f"Target column '{target_col}' for LOOE is categorical. Applying temporary LabelEncoding.")
                le_target = LabelEncoder()
                temp_target_series = le_target.fit_transform(temp_target_series)

            looe_sigma = kwargs.get('sigma', 0.05) # Example HP for LOOE
            looe = LeaveOneOutEncoder(cols=active_cat_cols, sigma=looe_sigma)
            encoded_looe_df = pd.DataFrame(looe.fit_transform(encoded_data[active_cat_cols], temp_target_series), columns=active_cat_cols, index=encoded_data.index)
            for col_name in active_cat_cols: # Corrected variable name
                encoded_data[col_name] = encoded_looe_df[col_name]
            self.encoders['leaveoneout'] = looe
        else:
            raise ValueError(f"Unknown encoding method: {method}. Use 'onehot', 'label', 'ordinal', 'lsa', 'embedding', 'word2vec', or 'leaveoneout'.")

        return encoded_data 


========== FILE: src\preprocessing\outlier_remover.py ==========

import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest

class OutlierRemover:
    """
    A class to remove outliers from a pandas DataFrame.
    Supported methods: 'isolation_forest', 'iqr', 'none'.
    """
    def __init__(self, method='isolation_forest', **kwargs):
        """
        Initialize the OutlierRemover.

        Args:
            method (str): Method to use for outlier detection.
                          'isolation_forest', 'iqr', or 'none'.
            **kwargs: Hyperparameters for the chosen method.
                      For 'isolation_forest': n_estimators, contamination.
                      For 'iqr': multiplier.
        """
        if method not in ['isolation_forest', 'iqr', 'none']:
            raise ValueError(f"Unsupported outlier removal method: {method}")
        
        self.method = method
        self.model = None
        self.kwargs = kwargs # Store all kwargs
        self.outlier_indices_ = None

        if self.method == 'isolation_forest':

            if_n_estimators = self.kwargs.get('n_estimators', 100)
            if_contamination = self.kwargs.get('contamination', 'auto')
            self.model = IsolationForest(n_estimators=if_n_estimators, 
                                         contamination=if_contamination, 
                                         random_state=42)
        elif self.method == 'iqr':
            self.iqr_multiplier = self.kwargs.get('multiplier', 1.5)
            pass # No model to pre-initialize for IQR, logic is in remove_outliers
        elif self.method == 'none':
            # print("Outlier removal method is 'none'. No model initialized.")
            pass
        else:
            raise ValueError(f"Unknown outlier removal method: {self.method}")

    def remove_outliers(self, data):
        """
        Remove outliers from the DataFrame based on the chosen method.
        Only numeric columns are considered for outlier detection.
        
        Args:
            data (pd.DataFrame): The input DataFrame.

        Returns:
            pd.DataFrame: DataFrame with outliers removed if a removal method was chosen,
                          otherwise the original DataFrame.
        """
        if self.method == 'none' or data.empty:
            # print("Skipping outlier removal (method is 'none' or data is empty).")
            return data

        data_cleaned = data.copy()
        numeric_cols = data_cleaned.select_dtypes(include=np.number).columns

        if numeric_cols.empty:
            print("No numeric columns found to perform outlier detection. Returning original data.")
            return data

        print(f"Performing outlier removal using method: {self.method} on {len(numeric_cols)} numeric columns.")

        if self.method == 'isolation_forest':
            if self.model is None: # Should have been initialized in __init__
                print("Error: Isolation Forest model not initialized.")
                return data # or raise error
            
            # Ensure all data is finite for Isolation Forest
            data_numeric_finite = data_cleaned[numeric_cols].replace([np.inf, -np.inf], np.nan).dropna()
            if data_numeric_finite.empty:
                print("Warning: Data became empty after handling non-finite values for Isolation Forest. Returning original data.")
                return data
            
            original_indices = data_numeric_finite.index
            outliers = self.model.fit_predict(data_numeric_finite)
            mask_inliers = outliers != -1
            
            cleaned_numeric_data = data_numeric_finite[mask_inliers]

        elif self.method == 'iqr':
            multiplier = self.kwargs.get('multiplier', 1.5) # Get from stored kwargs or default
            # print(f"Applying IQR with multiplier: {multiplier}")
            Q1 = data_cleaned[numeric_cols].quantile(0.25)
            Q3 = data_cleaned[numeric_cols].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - multiplier * IQR
            upper_bound = Q3 + multiplier * IQR
            
            mask_inliers = ~((data_cleaned[numeric_cols] < lower_bound) | (data_cleaned[numeric_cols] > upper_bound)).any(axis=1)
            cleaned_numeric_data = data_cleaned[mask_inliers]
            # print(f"IQR: {np.sum(~mask_inliers)} outliers removed out of {len(data_cleaned[numeric_cols])} samples.")
        else:
            # Should not happen due to __init__ check, but as a safeguard
            print(f"Unknown or unhandled outlier removal method: {self.method}. Returning original data.")
            return data
        

        if self.method == 'iqr':
            final_data = cleaned_numeric_data # For IQR, cleaned_numeric_data is the full filtered dataframe
        elif self.method == 'isolation_forest':
            if not data_cleaned.drop(columns=numeric_cols, errors='ignore').empty:

                if not data_numeric_finite.index.equals(data_cleaned[numeric_cols].index):

                     final_data = pd.concat([cleaned_numeric_data, data_cleaned.drop(columns=numeric_cols, errors='ignore').loc[cleaned_numeric_data.index]], axis=1)
                else:
     
                     final_data = pd.concat([cleaned_numeric_data, data_cleaned.drop(columns=numeric_cols, errors='ignore').loc[cleaned_numeric_data.index]], axis=1)
            else:

                final_data = cleaned_numeric_data
        else: 
            final_data = data_cleaned 

        original_shape = data.shape
        cleaned_shape = final_data.shape
        print(f"Outlier removal: {original_shape[0]} rows before, {cleaned_shape[0]} rows after.")
        
        return final_data 


========== FILE: src\preprocessing\resampler.py ==========

import pandas as pd
import numpy as np
from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler

class Resampler:
    """
    A class to handle different resampling techniques for imbalanced datasets.
    Supported methods: 'none', 'oversample' (ROS), 'undersample' (RUS), 'smote', 'adasyn'.
    """
    def __init__(self, method='none', random_state=42, **kwargs):
        """
        Initialize the Resampler.
        Args:
            method (str): The resampling method to use.
            random_state (int): Random state for reproducibility.
            **kwargs: Hyperparameters for the chosen resampling method.
                      For SMOTE/ADASYN: k_neighbors, sampling_strategy.
                      For ROS: sampling_strategy.
        """
        if method not in ['none', 'oversample', 'undersample', 'smote', 'adasyn']:
            raise ValueError(f"Unsupported resampling method: {method}")
        
        self.method = method
        self.random_state = random_state
        self.kwargs = kwargs # Store kwargs
        self.resampler = self._get_resampler()

    def _get_resampler(self):
        """Get the resampler object based on the chosen method and HPs."""
        if self.method == 'oversample':
            # ROS HPs: sampling_strategy
            ros_sampling_strategy = self.kwargs.get('sampling_strategy', 'auto')
            # print(f"ROS initialized with sampling_strategy={ros_sampling_strategy}")
            return RandomOverSampler(sampling_strategy=ros_sampling_strategy, random_state=self.random_state)
        
        elif self.method == 'undersample':
            # RUS HPs: sampling_strategy (can also be a float)
            rus_sampling_strategy = self.kwargs.get('sampling_strategy', 'auto')
            # print(f"RUS initialized with sampling_strategy={rus_sampling_strategy}")
            return RandomUnderSampler(sampling_strategy=rus_sampling_strategy, random_state=self.random_state)
        
        elif self.method == 'smote':
            # SMOTE HPs: k_neighbors, sampling_strategy
            smote_k_neighbors = self.kwargs.get('k_neighbors', 5)
            smote_sampling_strategy = self.kwargs.get('sampling_strategy', 'auto')
            # print(f"SMOTE initialized with k_neighbors={smote_k_neighbors}, sampling_strategy={smote_sampling_strategy}")
            return SMOTE(k_neighbors=smote_k_neighbors, sampling_strategy=smote_sampling_strategy, 
                         random_state=self.random_state, n_jobs=-1)
        
        elif self.method == 'adasyn':
            # ADASYN HPs: n_neighbors, sampling_strategy
            adasyn_n_neighbors = self.kwargs.get('n_neighbors', 5)
            adasyn_sampling_strategy = self.kwargs.get('sampling_strategy', 'auto')
            # print(f"ADASYN initialized with n_neighbors={adasyn_n_neighbors}, sampling_strategy={adasyn_sampling_strategy}")
            return ADASYN(n_neighbors=adasyn_n_neighbors, sampling_strategy=adasyn_sampling_strategy, 
                          random_state=self.random_state, n_jobs=-1)
        
        elif self.method == 'none':
            return None
        
        # This case should ideally be caught by __init__ validation
        raise ValueError(f"Resampling method '{self.method}' is not recognized in _get_resampler.")

    def fit_resample(self, X, y):
        """
        Apply the chosen resampling method to the data.
        Args:
            X (pd.DataFrame or np.ndarray): Feature matrix.
            y (pd.Series or np.ndarray): Target vector.
        Returns:
            (pd.DataFrame, pd.Series): Resampled X and y.
        """
        if self.method == 'none' or self.resampler is None:
            # print("Resampling method is 'none'. No changes to data.")
            return X, y
        
        # print(f"Applying resampling method: {self.method} with HPs: {self.kwargs}")
        # Store original column names if X is a DataFrame
        original_columns = X.columns if isinstance(X, pd.DataFrame) else None
        original_index_name = X.index.name if isinstance(X, pd.DataFrame) else None

        if self.method == 'smote' or self.method == 'adasyn':
            # Ensure y is a pandas Series for nunique()
            y_series = pd.Series(y) if not isinstance(y, pd.Series) else y
            if y_series.nunique() > 2:  # Multi-class
                # Check the sampling_strategy of the initialized SMOTE resampler
                current_strategy = self.resampler.sampling_strategy
                if isinstance(current_strategy, float):
                    adapted_strategy = 'auto'
                    print(f"Warning: SMOTE sampling_strategy '{current_strategy}' is a float but target is multi-class ({y_series.nunique()} classes).")
                    print(f"Adapting SMOTE sampling_strategy to '{adapted_strategy}' for this run.")
                    # Modify the strategy of the existing resampler instance
                    self.resampler.sampling_strategy = adapted_strategy
        
        try:
            X_resampled, y_resampled = self.resampler.fit_resample(X, y)
        except Exception as e:
            print(f"Error during {self.method} resampling with HPs {self.kwargs}: {e}")
            print("Returning original data due to resampling error.")
            return X, y
        
        # print(f"Resampling complete. Original shape: X-{X.shape}, y-{y.shape}. Resampled shape: X-{X_resampled.shape}, y-{y_resampled.shape}")

        # If X was a DataFrame, try to convert X_resampled back to DataFrame with original columns
        if original_columns is not None and isinstance(X_resampled, np.ndarray):
            X_resampled = pd.DataFrame(X_resampled, columns=original_columns)
            # Attempting to restore index is tricky as resampling changes row count and order.
            # A simple re-index might not be meaningful. For now, use default integer index.

        # Ensure y_resampled is a pd.Series if y was one
        if isinstance(y, pd.Series) and isinstance(y_resampled, np.ndarray):
            y_name = y.name if y.name else 'target'
            y_resampled = pd.Series(y_resampled, name=y_name)
            # Similarly, index for y_resampled will be the default from imblearn

        return X_resampled, y_resampled 


========== FILE: src\utils\__init__.py ==========

"""
Utility functions package.
Contains data analysis and visualization tools.
""" 


========== FILE: src\utils\data_analysis.py ==========

def print_missing_summary(data, name):
    """
    Print statistics about missing values in the dataset.
    
    Args:
        data: pandas DataFrame to analyze
        name: str, name of the dataset
    """
    total = data.isnull().sum().sum()
    print(f"\nMissing values in {name}:")
    print(f"Total missing values: {total}")
    if total > 0:
        print("Distribution by columns:")
        missing = data.isnull().sum()
        missing = missing[missing > 0]
        for col, count in missing.items():
            print(f"- {col}: {count} missing values")

def print_numeric_stats(data, stage_name):
    """
    Print statistics for numeric columns.
    
    Args:
        data: pandas DataFrame
        stage_name: str, name of the processing stage
    """
    numeric_cols = data.select_dtypes(include=['number']).columns
    print(f"\n=== Numeric features {stage_name} ===")
    print(f"Total numeric columns: {len(numeric_cols)}")
    
    if len(numeric_cols) > 0:
        stats = data[numeric_cols].agg(['mean', 'median', 'std', 'min', 'max'])
        print("Statistics:\n", stats.round(2))

def print_target_distribution(data, target_col):
    """
    Print distribution of values in the target column.
    
    Args:
        data: pandas DataFrame
        target_col: str, name of the target column
    """
    if target_col not in data.columns:
        raise ValueError(f"Column {target_col} not found in data")
    
    target_counts = data[target_col].value_counts(dropna=False)
    target_percent = data[target_col].value_counts(normalize=True, dropna=False) * 100
    
    print(f"\nTarget variable '{target_col}' distribution:")
    print("----------------------------------")
    print(f"Total records: {len(data)}")
    print("----------------------------------")
    print("Value | Count | Percentage")
    print("----------------------------------")
    
    for value, count in target_counts.items():
        percent = target_percent[value]
        print(f"{value!r:8} | {count:5} | {percent:.2f}%")
    
    if target_counts.isna().sum() > 0:
        print("\nWarning: missing values found in target variable!")

def analyze_target_correlations(data, target_col, output_path):
    """
    Calculate and save correlations between features and target variable.
    
    Args:
        data: pandas DataFrame
        target_col: str, name of the target column
        output_path: str, path to save correlation results
    """
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    correlations = data.corr()[target_col].sort_values(ascending=False)
    
    corr_df = pd.DataFrame({
        'Feature': correlations.index,
        'Correlation': correlations.values
    })
    
    corr_df.to_csv(f"{output_path}/feature_correlations.csv", index=False)
    
    plt.figure(figsize=(12, 8))
    sns.barplot(data=corr_df, x='Correlation', y='Feature')
    plt.title(f'Feature Correlations with {target_col}')
    plt.tight_layout()
    plt.savefig(f"{output_path}/feature_correlations.png")
    plt.close()

def save_model_results(metrics, feature_importance, output_path):
    """
    Save model evaluation results and feature importance.
    
    Args:
        metrics: dict, containing model metrics
        feature_importance: pandas DataFrame, feature importance scores
        output_path: str, path to save results
    """
    import os
    
    model_path = os.path.join(output_path, 'model_results')
    os.makedirs(model_path, exist_ok=True)
    
    eval_key = 'test' if 'test' in metrics else 'validation'
    
    with open(os.path.join(model_path, 'metrics.txt'), 'w') as f:
        f.write("=== Training Metrics ===\n")
        f.write(f"Accuracy: {metrics['train']['accuracy']:.4f}\n")
        f.write(f"F1 Score: {metrics['train']['f1']:.4f}\n")
        f.write("\nDetailed Training Report:\n")
        f.write(metrics['train']['detailed_report'])
        
        f.write(f"\n\n=== {eval_key.title()} Metrics ===\n")
        f.write(f"Accuracy: {metrics[eval_key]['accuracy']:.4f}\n")
        f.write(f"F1 Score: {metrics[eval_key]['f1']:.4f}\n")
        f.write(f"\nDetailed {eval_key.title()} Report:\n")
        f.write(metrics[eval_key]['detailed_report'])
    
    feature_importance.to_csv(os.path.join(model_path, 'feature_importance.csv'), index=False) 


========== FILE: src\utils\results_saver.py ==========

import json
import os
import matplotlib.pyplot as plt
import seaborn as sns

def save_model_results(metrics, feature_importance, output_path):
    """
    Save model training and evaluation results.
    
    Args:
        metrics (dict): Dictionary containing training and evaluation metrics
        feature_importance (pd.DataFrame): DataFrame with feature importance scores
        output_path (str): Path to save the results
    """
    os.makedirs(output_path, exist_ok=True)
    
    metrics_path = os.path.join(output_path, 'metrics.json')
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f, indent=4)
    print(f"\nMetrics saved to: {metrics_path}")
    
    plt.figure(figsize=(12, 6))
    sns.barplot(data=feature_importance.sort_values('importance', ascending=False),
                x='importance', y='feature')
    plt.title('Feature Importance')
    plt.tight_layout()
    feature_importance_plot_path = os.path.join(output_path, 'feature_importance.png')
    plt.savefig(feature_importance_plot_path)
    plt.close()
    print(f"Feature importance plot saved to: {feature_importance_plot_path}")
    
    feature_importance_path = os.path.join(output_path, 'feature_importance.csv')
    feature_importance.to_csv(feature_importance_path, index=False)
    print(f"Feature importance data saved to: {feature_importance_path}") 


