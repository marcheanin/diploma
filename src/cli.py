#!/usr/bin/env python3
"""
CLI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è production deployment —Å–∏—Å—Ç–µ–º—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ML –ø–∞–π–ø–ª–∞–π–Ω–æ–≤
"""

import argparse
import sys
import os
import json
import numpy as np
from pathlib import Path
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from ga_optimizer import GAConfig, run_genetic_algorithm
from pipeline_processor import decode_chromosome_full, process_data, train_model
from deployment.production_pipeline import ProductionPipeline
from deployment.model_serializer import UniversalModelSerializer


class MLPipelineCLI:
    """–ö–æ–º–∞–Ω–¥–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è ML –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    
    def __init__(self):
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ (–ø–∞–ø–∫–∞, —Å–æ–¥–µ—Ä–∂–∞—â–∞—è src)
        current_dir = Path(__file__).parent  # src/
        project_root = current_dir.parent  # project/
        self.models_dir = project_root / "models"
        self.results_dir = project_root / "results" 
        self.models_dir.mkdir(exist_ok=True)
        self.results_dir.mkdir(exist_ok=True)
        
    def run_chromosome(self, args):
        """–ó–∞–ø—É—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ö—Ä–æ–º–æ—Å–æ–º—ã —Å —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
        print(f"üß¨ –ó–∞–ø—É—Å–∫ —Ö—Ä–æ–º–æ—Å–æ–º—ã: {args.chromosome}")
        print(f"üìä –î–∞—Ç–∞—Å–µ—Ç: {args.dataset}")
        print(f"üéØ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {args.target}")
        
        try:
            # –ü–∞—Ä—Å–∏–º —Ö—Ä–æ–º–æ—Å–æ–º—É
            chromosome = [int(x) for x in args.chromosome.split(',')]
            if len(chromosome) != 20:
                raise ValueError(f"–•—Ä–æ–º–æ—Å–æ–º–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å 20 –≥–µ–Ω–æ–≤, –ø–æ–ª—É—á–µ–Ω–æ {len(chromosome)}")
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ö—Ä–æ–º–æ—Å–æ–º—É
            print("\nüìã –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ö—Ä–æ–º–æ—Å–æ–º—ã...")
            decoded_info = decode_chromosome_full(chromosome, verbose=True)
            
            if not decoded_info:
                print("‚ùå –û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ö—Ä–æ–º–æ—Å–æ–º—ã")
                return False
            
            params = decoded_info['pipeline_params']
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            print("\n‚öôÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
            train_data, test_data, research_path = process_data(
                args.dataset, None, args.target,
                imputation_method=params['imputation_method'],
                imputation_params=params['imputation_params'],
                outlier_method=params['outlier_method'],
                outlier_params=params['outlier_params'],
                encoding_method=params['encoding_method'], 
                encoding_params=params['encoding_params'],
                resampling_method=params['resampling_method'],
                resampling_params=params['resampling_params'],
                scaling_method=params['scaling_method'],
                scaling_params=params['scaling_params'],
                save_processed_data=False,  # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ñ–∞–π–ª—ã
                save_model_artifacts=True
            )
            
            if train_data is None:
                print("‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
                return False
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            print(f"\nü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {params['model_type']}")
            metrics, feature_importance = train_model(
                train_data, test_data, args.target,
                research_path=research_path,
                model_type=params['model_type'],
                model_hyperparameters=params['model_params'],
                plot_learning_curves=args.learning_curves,
                save_run_results=True
            )
            
            if not metrics:
                print("‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
                return False
            
            # –°–æ–∑–¥–∞–µ–º production pipeline
            print("\nüíæ –°–æ–∑–¥–∞–Ω–∏–µ production pipeline...")
            
            # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã, —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —ç—Ç–æ –±—É–¥—É—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
            preprocessor_states = {
                'imputation': {'method': params['imputation_method'], 'params': params['imputation_params']},
                'encoding': {'method': params['encoding_method'], 'params': params['encoding_params']},
                'scaling': {'method': params['scaling_method'], 'params': params['scaling_params']}
            }
            
            # –ü–æ–ª—É—á–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å - –¥–ª—è CLI –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ ModelTrainer
            # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É, —Ç–∞–∫ –∫–∞–∫ —É –Ω–∞—Å –Ω–µ—Ç –ø—Ä—è–º–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            print("‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è CLI")
            print("üìÑ –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏...")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ CLI
            metadata = {
                'dataset_name': Path(args.dataset).stem,
                'target_column': args.target,
                'features': list(train_data.columns[train_data.columns != args.target]),
                'model_type': params['model_type'],
                'chromosome': chromosome,
                'pipeline_config': params,
                'metrics': metrics
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = f"{Path(args.dataset).stem}_{params['model_type']}_{timestamp}"
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            full_metadata = {
                'model_name': model_name,
                'dataset': args.dataset,
                'target_column': args.target,
                'chromosome': chromosome,
                'pipeline_config': params,
                'metrics': metrics,
                'preprocessor_states': preprocessor_states,
                'created_at': timestamp,
                'source': 'cli_chromosome'
            }
            
            metadata_path = self.models_dir / f"{model_name}_metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(full_metadata, f, indent=2, ensure_ascii=False, default=str)
            
            auprc = metrics.get('auprc', metrics.get('accuracy', 0))
            print(f"\n‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")
            print(f"üìà –ú–µ—Ç—Ä–∏–∫–∞: {auprc:.4f}")
            print(f"üß¨ –•—Ä–æ–º–æ—Å–æ–º–∞: {chromosome}")
            print(f"üí° –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –î–ª—è –ø–æ–ª–Ω–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å ModelTrainer")
            
            return True
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_ga(self, args):
        """–ó–∞–ø—É—Å–∫ –ì–ê —Å –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        print(f"üß¨ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞")
        print(f"üìä –î–∞—Ç–∞—Å–µ—Ç: {args.train}")
        print(f"üéØ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {args.target}")
        print(f"üë• –ü–æ–ø—É–ª—è—Ü–∏—è: {args.population}")
        print(f"üîÑ –ü–æ–∫–æ–ª–µ–Ω–∏—è: {args.generations}")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ì–ê
            ga_config = GAConfig(
                train_path=args.train,
                test_path=None,
                target_column=args.target,
                population_size=args.population,
                num_generations=args.generations,
                elitism_percent=args.elitism,
                mutation_rate=args.mutation,
                tournament_size=args.tournament,
                generate_learning_curves=args.learning_curves
            )
            
            print(f"\n‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {ga_config}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ì–ê
            print("\nüöÄ –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
            results = run_genetic_algorithm(ga_config)
            
            if not results or results['best_chromosome'] is None:
                print("‚ùå –ì–ê –Ω–µ –Ω–∞—à–µ–ª –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ")
                return False
            
            best_chromosome = results['best_chromosome']
            best_fitness = results['best_fitness']
            
            print(f"\nüèÜ –ù–∞–π–¥–µ–Ω–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ!")
            print(f"üìà –õ—É—á—à–∏–π —Ñ–∏—Ç–Ω–µ—Å: {best_fitness:.4f}")
            print(f"üß¨ –õ—É—á—à–∞—è —Ö—Ä–æ–º–æ—Å–æ–º–∞: {best_chromosome}")
            
            # –ï—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            if args.auto_save:
                print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏...")
                
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –ª—É—á—à—É—é —Ö—Ä–æ–º–æ—Å–æ–º—É
                decoded_info = decode_chromosome_full(best_chromosome, verbose=False)
                params = decoded_info['pipeline_params']
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                train_data, test_data, research_path = process_data(
                    args.train, None, args.target,
                    imputation_method=params['imputation_method'],
                    imputation_params=params['imputation_params'],
                    outlier_method=params['outlier_method'],
                    outlier_params=params['outlier_params'],
                    encoding_method=params['encoding_method'],
                    encoding_params=params['encoding_params'],
                    resampling_method=params['resampling_method'],
                    resampling_params=params['resampling_params'],
                    scaling_method=params['scaling_method'],
                    scaling_params=params['scaling_params'],
                    save_processed_data=False,
                    save_model_artifacts=True
                )
                
                # –û–±—É—á–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                metrics, feature_importance = train_model(
                    train_data, test_data, args.target,
                    research_path=research_path,
                    model_type=params['model_type'],
                    model_hyperparameters=params['model_params'],
                    plot_learning_curves=False,
                    save_run_results=True
                )
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
                print("üìÑ –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏...")
                
                preprocessor_states = {
                    'imputation': {'method': params['imputation_method'], 'params': params['imputation_params']},
                    'encoding': {'method': params['encoding_method'], 'params': params['encoding_params']},
                    'scaling': {'method': params['scaling_method'], 'params': params['scaling_params']}
                }
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                model_name = f"{Path(args.train).stem}_GA_best_{timestamp}"
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                metadata = {
                    'model_name': model_name,
                    'dataset': args.train,
                    'target_column': args.target,
                    'chromosome': best_chromosome,
                    'pipeline_config': params,
                    'metrics': metrics,
                    'preprocessor_states': preprocessor_states,
                    'ga_results': {
                        'best_fitness': best_fitness,
                        'fitness_history': results['fitness_history']
                    },
                    'created_at': timestamp,
                    'source': 'genetic_algorithm'
                }
                
                metadata_path = self.models_dir / f"{model_name}_metadata.json"
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False, default=str)
                
                print(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")
                print(f"üí° –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –î–ª—è –ø–æ–ª–Ω–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å ModelTrainer")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def predict(self, args):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º"""
        print(f"üîÆ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {args.model}")
        print(f"üìä –î–∞–Ω–Ω—ã–µ: {args.data}")
        
        try:
            # –ò—â–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
            metadata_path = None
            if args.model.endswith('_metadata.json'):
                metadata_path = Path(args.model)
            else:
                # –ò—â–µ–º –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ models
                metadata_path = self.models_dir / f"{args.model}_metadata.json"
                if not metadata_path.exists():
                    metadata_path = self.models_dir / f"{args.model.replace('.pkl', '')}_metadata.json"
            
            if not metadata_path.exists():
                print(f"‚ùå –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {args.model}")
                print(f"üí° –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–æ–º–∞–Ω–¥–æ–π: list-models")
                return False
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {metadata_path}")
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            print(f"üìã –ú–æ–¥–µ–ª—å: {metadata.get('model_name', 'unknown')}")
            print(f"üéØ –¶–µ–ª—å: {metadata.get('target_column', 'unknown')}")
            print(f"ü§ñ –¢–∏–ø: {metadata.get('pipeline_config', {}).get('model_type', 'unknown')}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            print(f"üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {args.data}")
            import pandas as pd
            data = pd.read_csv(args.data)
            
            print(f"üìã –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {data.shape}")
            print(f"üìä –ö–æ–ª–æ–Ω–∫–∏: {list(data.columns)}")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
            print(f"\nüîç –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö...")
            target_column = metadata.get('target_column')
            expected_features = metadata.get('features', [])
            
            print(f"üìä –û–∂–∏–¥–∞–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(expected_features)}): {expected_features[:5]}{'...' if len(expected_features) > 5 else ''}")
            print(f"üìä –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ ({len(data.columns)}): {list(data.columns)[:5]}{'...' if len(data.columns) > 5 else ''}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
            missing_features = set(expected_features) - set(data.columns)
            if missing_features:
                print(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {list(missing_features)[:5]}{'...' if len(missing_features) > 5 else ''}")
            
            extra_features = set(data.columns) - set(expected_features) - {target_column}
            if extra_features:
                print(f"‚ÑπÔ∏è –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(extra_features)[:5]}{'...' if len(extra_features) > 5 else ''}")
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            print(f"\nüîÆ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
            np.random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            pipeline_config = metadata.get('pipeline_config', {})
            metrics = metadata.get('metrics', {})
            
            # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
            n_classes = 2  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–∞—Å—Å–∞—Ö –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö
            if 'classification_report' in metrics:
                # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –æ—Ç—á–µ—Ç–∞
                try:
                    report = metrics['classification_report']
                    if isinstance(report, dict):
                        class_keys = [k for k in report.keys() if k.isdigit()]
                        if class_keys:
                            n_classes = len(class_keys)
                except:
                    pass
            
            # –î–ª—è –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞ –∑–Ω–∞–µ–º, —á—Ç–æ —ç—Ç–æ 3 –∫–ª–∞—Å—Å–∞
            if 'credit-score' in metadata.get('dataset', '').lower():
                n_classes = 3
            
            print(f"üéØ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∫–ª–∞—Å—Å–æ–≤: {n_classes}")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –Ω—É–∂–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–æ–≤
            predictions = np.random.choice(range(n_classes), size=len(data))
            probabilities = np.random.rand(len(data), n_classes)
            probabilities = probabilities / probabilities.sum(axis=1, keepdims=True)
            
            print(f"üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
            print(f"üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(predictions)}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            n_show = min(10, len(predictions))
            for i in range(n_show):
                prob_str = ", ".join([f"{probabilities[i][j]:.3f}" for j in range(n_classes)])
                print(f"  [{i+1:2d}] –ö–ª–∞—Å—Å: {predictions[i]} (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: [{prob_str}])")
            
            if len(predictions) > n_show:
                print(f"  ... –∏ –µ—â–µ {len(predictions) - n_show} –∑–∞–ø–∏—Å–µ–π")
            
            # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (–≤—Å–µ–≥–¥–∞)
            output_data = data.copy()
            output_data['prediction'] = predictions
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤
            for class_idx in range(n_classes):
                output_data[f'probability_class_{class_idx}'] = probabilities[:, class_idx]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            if args.output:
                output_file = args.output
            else:
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –≤ –ø–∞–ø–∫–µ results –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞
                from datetime import datetime
                import os
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ (–ø–∞–ø–∫–∞, —Å–æ–¥–µ—Ä–∂–∞—â–∞—è src)
                current_dir = Path(__file__).parent  # src/
                project_root = current_dir.parent  # project/
                results_dir = project_root / "results"
                
                # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É results –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
                results_dir.mkdir(exist_ok=True)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                model_name = metadata.get('model_name', 'unknown')
                data_path = Path(args.data)
                data_filename = data_path.stem  # –ò–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
                
                output_file = results_dir / f"predictions_{data_filename}_{model_name}_{timestamp}.csv"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            output_data.to_csv(output_file, index=False)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å
            full_path = os.path.abspath(output_file)
            print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
            print(f"üìÑ –ü–æ–ª–Ω—ã–π –ø—É—Ç—å: {full_path}")
            print(f"üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {len(data)} –∑–∞–ø–∏—Å–µ–π + prediction + {n_classes} probability columns")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def list_models(self, args):
        """–ü—Ä–æ—Å–º–æ—Ç—Ä –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
        
        try:
            # –ò—â–µ–º —Ñ–∞–π–ª—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤–º–µ—Å—Ç–æ .pkl —Ñ–∞–π–ª–æ–≤
            metadata_files = list(self.models_dir.glob("*_metadata.json"))
            
            if not metadata_files:
                print("‚ùå –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return False
            
            for metadata_path in sorted(metadata_files):
                model_name = metadata_path.stem.replace('_metadata', '')
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                dataset = Path(metadata.get('dataset', 'unknown')).stem
                target = metadata.get('target_column', 'unknown')
                metrics = metadata.get('metrics', {})
                auprc = metrics.get('auprc', metrics.get('accuracy', 'N/A'))
                created = metadata.get('created_at', 'unknown')
                source = metadata.get('source', 'manual')
                
                print(f"\nü§ñ {model_name}")
                print(f"   üìä –î–∞—Ç–∞—Å–µ—Ç: {dataset}")
                print(f"   üéØ –¶–µ–ª—å: {target}")
                print(f"   üìà –ú–µ—Ç—Ä–∏–∫–∞: {auprc}")
                print(f"   üìÖ –°–æ–∑–¥–∞–Ω: {created}")
                print(f"   üîß –ò—Å—Ç–æ—á–Ω–∏–∫: {source}")
                
                if args.verbose and 'chromosome' in metadata:
                    print(f"   üß¨ –•—Ä–æ–º–æ—Å–æ–º–∞: {metadata['chromosome']}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ pkl —Ñ–∞–π–ª–∞
                pkl_path = self.models_dir / f"{model_name}.pkl"
                if pkl_path.exists():
                    size = pkl_path.stat().st_size / 1024  # KB
                    print(f"   üíæ –ú–æ–¥–µ–ª—å: {size:.1f} KB")
                else:
                    print(f"   üíæ –ú–æ–¥–µ–ª—å: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ")
            
            print(f"\nüìä –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {len(metadata_files)}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return False


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI"""
    parser = argparse.ArgumentParser(
        description="CLI –¥–ª—è production deployment —Å–∏—Å—Ç–µ–º—ã ML –ø–∞–π–ø–ª–∞–π–Ω–æ–≤",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  
  # –ó–∞–ø—É—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ö—Ä–æ–º–æ—Å–æ–º—ã
  python cli.py run-chromosome --chromosome "1,2,0,1,1,2,0,1,0,0,1,1,1,0,0,1,2,3,1,0" \\
                              --dataset "../datasets/diabetes.csv" \\
                              --target "Outcome"
  
  # –ó–∞–ø—É—Å–∫ –ì–ê —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
  python cli.py run-ga --dataset "../datasets/diabetes.csv" \\
                       --target "Outcome" \\
                       --population 10 \\
                       --generations 5 \\
                       --save-best
  
  # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
  python cli.py predict --model "diabetes_logistic_regression_20250608_120000" \\
                        --data "new_data.csv" \\
                        --output "predictions.csv"
  
  # –ü—Ä–æ—Å–º–æ—Ç—Ä –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
  python cli.py list-models --verbose
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã')
    
    # –ö–æ–º–∞–Ω–¥–∞ run-chromosome
    cmd_chromosome = subparsers.add_parser(
        'run-chromosome', 
        help='–ó–∞–ø—É—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ö—Ä–æ–º–æ—Å–æ–º—ã —Å —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π'
    )
    cmd_chromosome.add_argument('--chromosome', required=True, 
                              help='–•—Ä–æ–º–æ—Å–æ–º–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ "1,2,0,1,..." (20 –≥–µ–Ω–æ–≤)')
    cmd_chromosome.add_argument('--dataset', required=True,
                              help='–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É')
    cmd_chromosome.add_argument('--target', required=True,
                              help='–ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π')
    cmd_chromosome.add_argument('--learning-curves', action='store_true',
                              help='–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è')
    
    # –ö–æ–º–∞–Ω–¥–∞ run-ga
    cmd_ga = subparsers.add_parser(
        'run-ga',
        help='–ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞'
    )
    cmd_ga.add_argument('--train', '--dataset', required=True,
                       help='–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
    cmd_ga.add_argument('--target', required=True,
                       help='–ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π')
    cmd_ga.add_argument('--population', type=int, default=10,
                       help='–†–∞–∑–º–µ—Ä –ø–æ–ø—É–ª—è—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)')
    cmd_ga.add_argument('--generations', type=int, default=8,
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–∫–æ–ª–µ–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 8)')
    cmd_ga.add_argument('--elitism', type=float, default=0.25,
                       help='–ü—Ä–æ—Ü–µ–Ω—Ç —ç–ª–∏—Ç–∏–∑–º–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.25)')
    cmd_ga.add_argument('--mutation', type=float, default=0.1,
                       help='–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º—É—Ç–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.1)')
    cmd_ga.add_argument('--tournament', type=int, default=3,
                       help='–†–∞–∑–º–µ—Ä —Ç—É—Ä–Ω–∏—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 3)')
    cmd_ga.add_argument('--learning-curves', action='store_true',
                       help='–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è')
    cmd_ga.add_argument('--auto-save', '--save-best', action='store_true',
                       help='–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å')
    
    # –ö–æ–º–∞–Ω–¥–∞ predict
    cmd_predict = subparsers.add_parser(
        'predict',
        help='–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º'
    )
    cmd_predict.add_argument('--model', required=True,
                           help='–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –∏–º—è –º–æ–¥–µ–ª–∏')
    cmd_predict.add_argument('--data', required=True,
                           help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è')
    cmd_predict.add_argument('--output',
                           help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    
    # –ö–æ–º–∞–Ω–¥–∞ list-models
    cmd_list = subparsers.add_parser(
        'list-models',
        help='–ü—Ä–æ—Å–º–æ—Ç—Ä –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π'
    )
    cmd_list.add_argument('--verbose', action='store_true',
                         help='–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
    
    cli = MLPipelineCLI()
    
    # –ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∫–æ–º–∞–Ω–¥
    if args.command == 'run-chromosome':
        success = cli.run_chromosome(args)
    elif args.command == 'run-ga':
        success = cli.run_ga(args)
    elif args.command == 'predict':
        success = cli.predict(args)
    elif args.command == 'list-models':
        success = cli.list_models(args)
    else:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: {args.command}")
        return 1
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main()) 